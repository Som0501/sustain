{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: AI Modelling (Improved Feature Engineering)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Loads data from Notebook 1, adds ENHANCED features, trains RF and LSTM for horizons 1,3,6,12,24h.\n",
    "\n",
    "Justification: Enhanced temporal features including trends, differences, and better lag patterns to capture PM2.5 dynamics properly and avoid straight-line predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress SettingWithCopyWarning for cleaner output"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the full path for the processed data generated by Notebook 1\n",
    "input_data_path = '/content/drive/MyDrive/AI_Sustainability_Project_lsa/sensor_12178556_Singapore_pm25_weather_hourly_data_processed_final.csv'\n",
    "\n",
    "print(f\"--- Starting AI Modelling (Notebook 3) ---\")\n",
    "print(f\"Loading pre-processed data from: {input_data_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the processed data from Google Drive\n",
    "    df = pd.read_csv(input_data_path, index_col='timestamp', parse_dates=True)\n",
    "    print(f\"Data loaded successfully. Initial shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from {input_data_path}: {e}\")\n",
    "    print(\"Please ensure Notebook 1 has run successfully and the file exists at the specified path in Google Drive.\")\n",
    "    raise SystemExit(\"Failed to load pre-processed data. Aborting Notebook 3 execution.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- IMPROVED FEATURE ENGINEERING ---\n",
    "def add_enhanced_features(data_df):\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering to capture temporal patterns and avoid flat predictions.\n",
    "    This version includes trends, differences, better lag patterns, and temporal statistics.\n",
    "    \"\"\"\n",
    "    df_featured = data_df.copy()\n",
    "    \n",
    "    print(\"Adding enhanced temporal features...\")\n",
    "    \n",
    "    # 1. Basic lag features with better selection\n",
    "    lags = [1, 2, 3, 6, 12, 24, 48, 72]  # More comprehensive lag structure\n",
    "    features_to_lag = ['pm25_value', 'temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "    \n",
    "    for feature in features_to_lag:\n",
    "        for lag in lags:\n",
    "            df_featured[f'{feature}_lag_{lag}'] = df_featured[feature].shift(lag)\n",
    "    \n",
    "    # 2. Trend and difference features (CRITICAL for temporal modeling)\n",
    "    # Short-term trends\n",
    "    df_featured['pm25_trend_3h'] = df_featured['pm25_value'] - df_featured['pm25_value'].shift(3)\n",
    "    df_featured['pm25_trend_6h'] = df_featured['pm25_value'] - df_featured['pm25_value'].shift(6)\n",
    "    df_featured['pm25_trend_12h'] = df_featured['pm25_value'] - df_featured['pm25_value'].shift(12)\n",
    "    df_featured['pm25_trend_24h'] = df_featured['pm25_value'] - df_featured['pm25_value'].shift(24)\n",
    "    \n",
    "    # Weather trends\n",
    "    df_featured['temp_trend_6h'] = df_featured['temp'] - df_featured['temp'].shift(6)\n",
    "    df_featured['humidity_trend_6h'] = df_featured['humidity'] - df_featured['humidity'].shift(6)\n",
    "    df_featured['wind_speed_trend_6h'] = df_featured['wind_speed'] - df_featured['wind_speed'].shift(6)\n",
    "    \n",
    "    # 3. Enhanced rolling statistics with multiple windows\n",
    "    windows = [3, 6, 12, 24, 48]\n",
    "    \n",
    "    for window in windows:\n",
    "        # PM2.5 rolling features\n",
    "        df_featured[f'pm25_mean_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=max(1, window//2)).mean()\n",
    "        df_featured[f'pm25_std_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=max(1, window//2)).std()\n",
    "        df_featured[f'pm25_min_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=max(1, window//2)).min()\n",
    "        df_featured[f'pm25_max_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=max(1, window//2)).max()\n",
    "        \n",
    "        # Weather rolling features\n",
    "        df_featured[f'temp_mean_{window}h'] = df_featured['temp'].rolling(window=window, min_periods=max(1, window//2)).mean()\n",
    "        df_featured[f'humidity_mean_{window}h'] = df_featured['humidity'].rolling(window=window, min_periods=max(1, window//2)).mean()\n",
    "        df_featured[f'wind_speed_mean_{window}h'] = df_featured['wind_speed'].rolling(window=window, min_periods=max(1, window//2)).mean()\n",
    "    \n",
    "    # 4. Enhanced cyclical encoding\n",
    "    df_featured['hour_sin'] = np.sin(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['hour_cos'] = np.cos(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['day_of_week_sin'] = np.sin(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "    df_featured['day_of_week_cos'] = np.cos(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "    df_featured['month_sin'] = np.sin(2 * np.pi * df_featured.index.month / 12)\n",
    "    df_featured['month_cos'] = np.cos(2 * np.pi * df_featured.index.month / 12)\n",
    "    \n",
    "    return df_featured"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "    # 5. Interaction features\n",
    "    df_featured['wind_humidity_interaction'] = df_featured['wind_speed'] * df_featured['humidity']\n",
    "    df_featured['temp_humidity_interaction'] = df_featured['temp'] * df_featured['humidity']\n",
    "    df_featured['wind_temp_interaction'] = df_featured['wind_speed'] * df_featured['temp']\n",
    "    \n",
    "    # 6. Volatility and rate of change features\n",
    "    df_featured['pm25_volatility_24h'] = df_featured['pm25_value'].rolling(window=24, min_periods=12).std()\n",
    "    df_featured['pm25_roc_1h'] = df_featured['pm25_value'].pct_change(periods=1)\n",
    "    df_featured['pm25_roc_6h'] = df_featured['pm25_value'].pct_change(periods=6)\n",
    "    df_featured['pm25_roc_24h'] = df_featured['pm25_value'].pct_change(periods=24)\n",
    "    \n",
    "    # 7. Weather variability\n",
    "    df_featured['temp_variability_12h'] = df_featured['temp'].rolling(window=12, min_periods=6).std()\n",
    "    df_featured['humidity_variability_12h'] = df_featured['humidity'].rolling(window=12, min_periods=6).std()\n",
    "    df_featured['wind_variability_12h'] = df_featured['wind_speed'].rolling(window=12, min_periods=6).std()\n",
    "    \n",
    "    # 8. Categorical time features\n",
    "    df_featured['hour_category'] = pd.cut(df_featured.index.hour, \n",
    "                                         bins=[0, 6, 12, 18, 24], \n",
    "                                         labels=['night', 'morning', 'afternoon', 'evening'],\n",
    "                                         include_lowest=True)\n",
    "    \n",
    "    # One-hot encode hour categories\n",
    "    hour_dummies = pd.get_dummies(df_featured['hour_category'], prefix='hour_cat')\n",
    "    df_featured = pd.concat([df_featured, hour_dummies], axis=1)\n",
    "    df_featured.drop('hour_category', axis=1, inplace=True)\n",
    "    \n",
    "    # 9. Peak detection features\n",
    "    df_featured['is_pm25_local_peak'] = ((df_featured['pm25_value'] > df_featured['pm25_value'].shift(1)) & \n",
    "                                        (df_featured['pm25_value'] > df_featured['pm25_value'].shift(-1))).astype(int)\n",
    "    \n",
    "    # 10. Exponential moving averages (better for trend following)\n",
    "    df_featured['pm25_ema_6h'] = df_featured['pm25_value'].ewm(span=6).mean()\n",
    "    df_featured['pm25_ema_24h'] = df_featured['pm25_value'].ewm(span=24).mean()\n",
    "    df_featured['temp_ema_12h'] = df_featured['temp'].ewm(span=12).mean()\n",
    "    \n",
    "    # Fill infinite values and replace with NaN\n",
    "    df_featured = df_featured.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Drop rows with NaNs created by feature engineering\n",
    "    initial_shape = df_featured.shape[0]\n",
    "    df_featured.dropna(inplace=True)\n",
    "    final_shape = df_featured.shape[0]\n",
    "    \n",
    "    print(f\"Feature engineering complete. Dropped {initial_shape - final_shape} rows with NaN values.\")\n",
    "    print(f\"Total features created: {len(df_featured.columns) - len(data_df.columns)}\")\n",
    "    \n",
    "    return df_featured"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n--- Adding Enhanced Features to the entire dataset ---\")\n",
    "df_featured = add_enhanced_features(df)\n",
    "print(f\"Shape after enhanced features and cleaning: {df_featured.shape}\")\n",
    "print(f\"New feature count: {len(df_featured.columns)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- Chronological Train/Test Split on the *featured* DataFrame ---\n",
    "print(\"\\n--- Performing Chronological Train/Test Split ---\")\n",
    "train_size = int(len(df_featured) * 0.8)\n",
    "train_df = df_featured.iloc[:train_size].copy()\n",
    "test_df = df_featured.iloc[train_size:].copy()\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "# Define features list (exclude target and original pm25_value)\n",
    "features_for_scaling = [col for col in train_df.columns if col != 'pm25_value' and 'target' not in col]\n",
    "print(f\"Number of features for modeling: {len(features_for_scaling)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- Feature Scaling ---\n",
    "print(\"\\n--- Scaling Features (MinMaxScaler on Train) ---\")\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df[features_for_scaling] = scaler_x.fit_transform(train_df[features_for_scaling])\n",
    "test_df[features_for_scaling] = scaler_x.transform(test_df[features_for_scaling])\n",
    "joblib.dump(scaler_x, '/content/drive/MyDrive/AI_Sustainability_Project_lsa/scaler_x.pkl')\n",
    "print(f\"Features scaled. Scaler saved.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- Model Training Loop for Multiple Horizons ---\n",
    "horizons = [1, 3, 6, 12, 24]\n",
    "print(f\"\\n--- Training Models for Horizons: {horizons} ---\")\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n--- Processing Horizon: {h} hours ---\")\n",
    "    \n",
    "    # Create target variable by shifting the original pm25_value\n",
    "    train_df['target_h'] = train_df['pm25_value'].shift(-h)\n",
    "    test_df['target_h'] = test_df['pm25_value'].shift(-h)\n",
    "    \n",
    "    # Drop rows where the shifted target is now NaN\n",
    "    train_h = train_df.dropna(subset=['target_h'])\n",
    "    test_h = test_df.dropna(subset=['target_h'])\n",
    "    \n",
    "    X_train = train_h[features_for_scaling]\n",
    "    y_train = train_h['target_h']\n",
    "    X_test = test_h[features_for_scaling]\n",
    "    y_test = test_h['target_h']\n",
    "    \n",
    "    print(f\"Training data shape: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "    print(f\"Target variance: {y_train.var():.4f} (should be > 0.1 for meaningful predictions)\")\n",
    "    \n",
    "    # Scale target for LSTM only\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    joblib.dump(scaler_y, f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/scaler_y_h{h}.pkl')\n",
    "    print(f\"Target scaled for LSTM (horizon {h}). Scaler saved.\")\n",
    "    \n",
    "    # Time Series Split for Cross-Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Random Forest Regressor\n",
    "    print(f\"Training RandomForestRegressor for horizon {h}...\")\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    param_dist_rf = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    rf_search = RandomizedSearchCV(rf, param_dist_rf, cv=tscv, scoring='neg_mean_squared_error', n_iter=10, verbose=0, random_state=42)\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    joblib.dump(rf_search.best_estimator_, f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/rf_model_h{h}.pkl')\n",
    "    print(f\"RF trained and saved. Best params: {rf_search.best_params_}\")\n",
    "    \n",
    "    # LSTM with improved architecture\n",
    "    print(f\"Training LSTM for horizon {h}...\")\n",
    "    X_train_lstm = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    \n",
    "    # Enhanced LSTM architecture\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(128, activation='relu', input_shape=(1, X_train.shape[1]), return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, activation='relu', return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    \n",
    "    history = model_lstm.fit(\n",
    "        X_train_lstm, y_train_scaled,\n",
    "        epochs=150,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_lstm.save(f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/lstm_model_h{h}.keras')\n",
    "    print(f\"LSTM trained and saved. Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "    \n",
    "    # Clean up target column for next iteration\n",
    "    train_df.drop('target_h', axis=1, inplace=True, errors='ignore')\n",
    "    test_df.drop('target_h', axis=1, inplace=True, errors='ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the final featured dataframes for evaluation in the next notebook\n",
    "train_df.to_csv('/content/drive/MyDrive/AI_Sustainability_Project_lsa/train_featured_data.csv')\n",
    "test_df.to_csv('/content/drive/MyDrive/AI_Sustainability_Project_lsa/test_featured_data.csv')\n",
    "print(\"\\nTrain and Test featured data saved for evaluation/compression in later notebooks.\")\n",
    "\n",
    "print(\"\\n--- AI Modelling Complete ---\")\n",
    "print(\"Enhanced models with improved temporal features trained and saved.\")\n",
    "print(\"This should resolve the straight-line prediction issue in Notebook 4.\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}