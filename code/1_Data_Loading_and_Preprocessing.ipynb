{
    "cells": [
     {
      "cell_type": "markdown",
      "source": [
       "# Notebook 1: Data Loading and Preprocessing\n",
       "## Introduction\n",
       "# This notebook loads PM2.5 data from the OpenAQ API and weather data from Open-Meteo. It merges the datasets, cleans them by handling missing values and outliers, and saves the processed data as a CSV file.\n",
       "# Justification: Using official APIs ensures reliable and up-to-date data. Merging on timestamp allows for hourly alignment. PM2.5 values are capped at 150.5 µg/m³ (Unhealthy AQI threshold) to manage outliers without introducing bias, preserving data integrity for SDG 11 (urban air quality)."
      ],
      "metadata": {}
     },
     {
      "cell_type": "code",
      "source": [
       "# Mount Google Drive\n",
       "from google.colab import drive\n",
       "import os\n",
       "\n",
       "# Mount your Google Drive\n",
       "drive.mount('/content/drive')\n",
       "\n",
       "# Define your project folder in Google Drive\n",
       "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
       "\n",
       "# Create the project directory if it doesn't exist\n",
       "os.makedirs(your_project_path, exist_ok=True)\n",
       "print(f\"Project path set to: {your_project_path}\")\n",
       "\n",
       "# Change current working directory to your project path\n",
       "%cd \"{your_project_path}\"\n",
       "\n",
       "# Verify current working directory\n",
       "!pwd\n",
       "!ls"
      ],
      "metadata": {}
     },
     {
      "cell_type": "markdown",
      "source": [
       "## Setup and Configuration"
      ],
      "metadata": {}
     },
     {
      "cell_type": "code",
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "import requests\n",
       "import time\n",
       "import os\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "# Configuration\n",
       "sensors_id = \"12178556\"  # Singapore PM2.5 Sensor ID\n",
       "latitude = 1.3521\n",
       "longitude = 103.8198\n",
       "location_name = \"Singapore\"\n",
       "openaq_api_key = \"4f1e60991c483fd961169d77137baa593d9568f4fa71585725860294b370bc43\"  # Replace with actual key\n",
       "\n",
       "# Define the filename for the processed data\n",
       "base_processed_file = f'sensor_{sensors_id}_{location_name}_pm25_weather_hourly_data_processed_final.csv'\n",
       "\n",
       "# CRITICAL FIX: Create the 'images' directory if it doesn't exist\n",
       "images_dir = 'images'\n",
       "os.makedirs(images_dir, exist_ok=True)\n",
       "print(f\"Ensured '{images_dir}' directory exists at {os.getcwd()}/{images_dir}\")\n",
       "\n",
       "\n",
       "print(\"--- Starting Data Loading and Preprocessing ---\")\n"
      ],
      "metadata": {}
     },
     {
      "cell_type": "markdown",
      "source": [
       "## Load Existing Processed Data (if available)"
      ],
      "metadata": {}
     },
     {
      "cell_type": "code",
      "source": [
       "# Variable to hold the DataFrame, initialized to None\n",
       "merged_df = None\n",
       "\n",
       "# --- Attempt to Load Existing Processed Data ---\n",
       "if os.path.exists(base_processed_file):\n",
       "    print(f\"Attempting to load existing processed data from {base_processed_file}\")\n",
       "    try:\n",
       "        merged_df = pd.read_csv(base_processed_file, index_col='timestamp', parse_dates=True)\n",
       "        print(f\"Successfully loaded existing data with 'timestamp' as index. Shape: {merged_df.shape}\")\n",
       "        print(f\"Loaded DataFrame index name: {merged_df.index.name}\")\n",
       "    except Exception as e:\n",
       "        print(f\"Error loading existing file: {e}\")\n",
       "        print(\"Proceeding to re-fetch and re-process data.\")\n",
       "        merged_df = None\n"
      ],
      "metadata": {}
     },
     {
      "cell_type": "markdown",
      "source": [
       "## Fetch and Process Data (if not loaded)"
      ],
      "metadata": {}
     },
     {
      "cell_type": "code",
      "source": [
       "# --- Data Fetching and Initial Processing (if not loaded from file) ---\n",
       "if merged_df is None:\n",
       "    print(\"Processed data not loaded (either not found or previous load failed). Initiating API data fetch and processing.\")\n",
       "\n",
       "    # --- Fetch PM2.5 Data from OpenAQ API ---\n",
       "    all_pm25_records = []\n",
       "    page = 1\n",
       "    limit_per_page = 1000\n",
       "    date_from_str = '2022-05-01'\n",
       "    date_to_str = '2024-04-30'\n",
       "\n",
       "    print(\"Fetching PM2.5 data...\")\n",
       "    while True:\n",
       "        base_url = f\"https://api.openaq.org/v3/sensors/{sensors_id}/hours\"\n",
       "        params = {\"date_from\": date_from_str, \"date_to\": date_to_str, \"limit\": limit_per_page, \"page\": page}\n",
       "        headers = {\"X-API-Key\": openaq_api_key}\n",
       "        try:\n",
       "            response = requests.get(base_url, params=params, headers=headers, timeout=30)\n",
       "            response.raise_for_status()\n",
       "            data = response.json()\n",
       "            results = data.get('results', [])\n",
       "            if not results:\n",
       "                print(f\"No more PM2.5 data found or end of data for sensor {sensors_id}.\")\n",
       "                break\n",
       "            for result in results:\n",
       "                all_pm25_records.append({'timestamp': result['period']['datetimeFrom']['utc'], 'pm25_value': result['value']})\n",
       "            print(f\"Fetched page {page} of PM2.5 data (total records: {len(all_pm25_records)})\")\n",
       "            page += 1\n",
       "            time.sleep(1)\n",
       "        except requests.exceptions.RequestException as e:\n",
       "            print(f\"API request error for PM2.5 data: {e}\")\n",
       "            break\n",
       "        except Exception as e:\n",
       "            print(f\"An unexpected error occurred during PM2.5 fetch: {e}\")\n",
       "            break\n",
       "\n",
       "    aq_df_raw = pd.DataFrame(all_pm25_records)\n",
       "    if not aq_df_raw.empty:\n",
       "        aq_df_raw['timestamp'] = pd.to_datetime(aq_df_raw['timestamp'], utc=True)\n",
       "        aq_df_raw.sort_values('timestamp', inplace=True)\n",
       "        aq_df_raw['pm25_value'] = pd.to_numeric(aq_df_raw['pm25_value'], errors='coerce')\n",
       "\n",
       "        print(\"\\n--- PM2.5 Data Initial Cleaning & Outlier Capping ---\")\n",
       "        initial_pm25_rows = aq_df_raw.shape[0]\n",
       "        aq_df_raw.dropna(subset=['pm25_value'], inplace=True)\n",
       "        print(f\"Removed {initial_pm25_rows - aq_df_raw.shape[0]} rows with non-numeric PM2.5 values.\")\n",
       "\n",
       "        initial_pm25_rows = aq_df_raw.shape[0]\n",
       "        aq_df_raw = aq_df_raw[aq_df_raw['pm25_value'] >= 0]\n",
       "        print(f\"Removed {initial_pm25_rows - aq_df_raw.shape[0]} rows with negative PM2.5 values.\")\n",
       "\n",
       "        PM25_CAP = 150.5\n",
       "        aq_df_raw['pm25_value'] = np.where(aq_df_raw['pm25_value'] > PM25_CAP, PM25_CAP, aq_df_raw['pm25_value'])\n",
       "        print(f\"Capped PM2.5 values above {PM25_CAP} µg/m³ to {PM25_CAP} µg/m³.\")\n",
       "    else:\n",
       "        print(\"No PM2.5 data fetched. Skipping PM2.5 cleaning.\")\n",
       "        merged_df = pd.DataFrame() # Set to empty to skip subsequent steps\n",
       "\n",
       "    if aq_df_raw.empty:\n",
       "        print(\"PM2.5 data is empty after initial processing. Skipping weather fetch and merge.\")\n",
       "        merged_df = pd.DataFrame()\n",
       "\n",
       "\n",
       "    # --- Fetch Weather Data from Open-Meteo Archive API ---\n",
       "    weather_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
       "    weather_params = {\n",
       "        \"latitude\": latitude, \"longitude\": longitude,\n",
       "        \"start_date\": aq_df_raw['timestamp'].min().strftime('%Y-%m-%d') if not aq_df_raw.empty else date_from_str,\n",
       "        \"end_date\": aq_df_raw['timestamp'].max().strftime('%Y-%m-%d') if not aq_df_raw.empty else date_to_str,\n",
       "        \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m,wind_direction_10m,precipitation\",\n",
       "        \"timezone\": \"UTC\"\n",
       "    }\n",
       "    print(\"\\nFetching weather data...\")\n",
       "    try:\n",
       "        response_weather = requests.get(weather_url, params=weather_params, timeout=30)\n",
       "        response_weather.raise_for_status()\n",
       "        weather_data = response_weather.json()['hourly']\n",
       "        weather_df = pd.DataFrame(weather_data)\n",
       "        weather_df['timestamp'] = pd.to_datetime(weather_df['time'], utc=True)\n",
       "        weather_df.drop(columns=['time'], inplace=True)\n",
       "        weather_df.sort_values('timestamp', inplace=True)\n",
       "        weather_df.rename(columns={\n",
       "            'temperature_2m': 'temp',\n",
       "            'relative_humidity_2m': 'humidity',\n",
       "            'wind_speed_10m': 'wind_speed',\n",
       "            'wind_direction_10m': 'wind_dir',\n",
       "            'precipitation': 'precipitation'\n",
       "        }, inplace=True)\n",
       "        print(f\"Weather data fetched. Shape: {weather_df.shape}\")\n",
       "    except requests.exceptions.RequestException as e:\n",
       "        print(f\"API request error for weather data: {e}\")\n",
       "        weather_df = pd.DataFrame()\n",
       "    except Exception as e:\n",
       "        print(f\"An unexpected error occurred during weather fetch: {e}\")\n",
       "        weather_df = pd.DataFrame()\n",
       "\n",
       "    # --- Merge PM2.5 and Weather Data ---\n",
       "    if not aq_df_raw.empty and not weather_df.empty:\n",
       "        merged_df = pd.merge(aq_df_raw, weather_df, on='timestamp', how='inner')\n",
       "        print(f\"\\nMerged DataFrame shape after inner join: {merged_df.shape}\")\n",
       "\n",
       "        merged_df.set_index('timestamp', inplace=True)\n",
       "        merged_df.sort_index(inplace=True)\n",
       "        if merged_df.index.name is None:\n",
       "             merged_df.index.name = 'timestamp'\n",
       "        print(f\"Timestamp set as index. Current index name: {merged_df.index.name}\")\n",
       "\n",
       "        # --- Handle Missing Values (Interpolation and Final Dropna) ---\n",
       "        print(\"\\n--- Handling Missing Values in Merged Data ---\")\n",
       "        missing_before_interp = merged_df.isnull().sum()\n",
       "        print(\"Missing values before interpolation:\")\n",
       "        print(missing_before_interp[missing_before_interp > 0])\n",
       "\n",
       "        numerical_cols_to_interp = merged_df.select_dtypes(include=np.number).columns\n",
       "        merged_df[numerical_cols_to_interp] = merged_df[numerical_cols_to_interp].interpolate(method='linear', limit_direction='both')\n",
       "        print(\"Applied linear interpolation to numerical features.\")\n",
       "\n",
       "        initial_rows_after_interp = merged_df.shape[0]\n",
       "        merged_df.dropna(inplace=True)\n",
       "        rows_dropped_after_interp = initial_rows_after_interp - merged_df.shape[0]\n",
       "        if rows_dropped_after_interp > 0:\n",
       "            print(f\"Dropped {rows_dropped_after_interp} rows with remaining NaN values after interpolation.\")\n",
       "        else:\n",
       "            print(\"No additional rows dropped after interpolation.\")\n",
       "\n",
       "        print(\"Missing values after interpolation and final dropna:\")\n",
       "        print(merged_df.isnull().sum().sum())\n",
       "\n",
       "        # --- Feature Engineering: Time-based Features ---\n",
       "        print(\"\\n--- Engineering Time-based Features ---\")\n",
       "        merged_df['hour_of_day'] = merged_df.index.hour\n",
       "        merged_df['day_of_week'] = merged_df.index.dayofweek\n",
       "        merged_df['month'] = merged_df.index.month\n",
       "        merged_df['is_weekend'] = ((merged_df.index.dayofweek == 5) | (merged_df.index.dayofweek == 6)).astype(int)\n",
       "        print(\"Added 'hour_of_day', 'day_of_week', 'month', 'is_weekend' features.\")\n",
       "    else:\n",
       "        print(\"Could not merge dataframes due to empty PM2.5 or weather data. Skipping subsequent steps.\")\n",
       "        merged_df = pd.DataFrame()\n"
      ],
      "metadata": {}
     },
     {
      "cell_type": "markdown",
      "source": [
       "## Initial Visualizations"
      ],
      "metadata": {}
     },
     {
      "cell_type": "code",
      "source": [
       "# Proceed with visualizations and saving only if merged_df is not empty\n",
       "if not merged_df.empty:\n",
       "    # --- Quick Initial Data Exploration (Visualizations for context) ---\n",
       "    print(\"\\n--- Performing Quick Initial Data Visualizations ---\")\n",
       "\n",
       "    # Plot 1: Distribution of PM2.5 Value\n",
       "    plt.figure(figsize=(15, 5))\n",
       "    plt.subplot(1, 3, 1)\n",
       "    sns.histplot(merged_df['pm25_value'], bins=30, kde=True)\n",
       "    plt.title('Distribution of PM2.5 Value')\n",
       "    plt.xlabel('PM2.5 (µg/m³)')\n",
       "    plt.ylabel('Frequency')\n",
       "    plt.savefig('images/plot_0_0.png') # Saving as plot_0_0.png\n",
       "    plt.show()\n",
       "\n",
       "    # Plot 2: Distribution of Temperature\n",
       "    # Ensure this part is consistent if you use subplots, or create new figures\n",
       "    plt.figure(figsize=(15, 5))\n",
       "    plt.subplot(1, 3, 2)\n",
       "    sns.histplot(merged_df['temp'], bins=30, kde=True)\n",
       "    plt.title('Distribution of Temperature')\n",
       "    plt.xlabel('Temperature (°C)')\n",
       "    plt.ylabel('Frequency')\n",
       "    plt.savefig('images/plot_0_1.png') # Saving as plot_0_1.png\n",
       "    plt.show()\n",
       "\n",
       "    # Plot 3: Distribution of Relative Humidity (not explicitly included in LaTeX but good to generate)\n",
       "    plt.figure(figsize=(15, 5))\n",
       "    plt.subplot(1, 3, 3)\n",
       "    sns.histplot(merged_df['humidity'], bins=30, kde=True)\n",
       "    plt.title('Distribution of Relative Humidity')\n",
       "    plt.xlabel('Humidity (%)')\n",
       "    plt.ylabel('Frequency')\n",
       "    plt.savefig('images/plot_0_2.png') # Saving as plot_0_2.png\n",
       "    plt.show()\n",
       "\n",
       "    # Plot 4: Daily Average PM2.5 over Time\n",
       "    plt.figure(figsize=(10, 6))\n",
       "    daily_avg_pm25 = merged_df['pm25_value'].resample('D').mean()\n",
       "    sns.lineplot(data=daily_avg_pm25)\n",
       "    plt.title('Daily Average PM2.5 over Time')\n",
       "    plt.xlabel('Date')\n",
       "    plt.ylabel('PM2.5 (µg/m³)')\n",
       "    plt.grid(True)\n",
       "    plt.savefig('images/plot_0_3.png') # Saving as plot_0_3.png\n",
       "    plt.show()\n",
       "\n",
       "    # --- Save Processed Data for Next Steps ---\n",
       "    print(\"\\n--- Saving Processed Data ---\")\n",
       "    merged_df.to_csv(base_processed_file, index=True)\n",
       "    print(f\"Processed and cleaned data saved to {base_processed_file}.\")\n",
       "else:\n",
       "    print(\"No data processed or merged. Skipping visualizations and saving.\")\n",
       "\n",
       "print(\"--- Data Loading and Preprocessing Complete ---\")\n"
      ],
      "metadata": {}
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    }
   }