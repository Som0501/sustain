{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13266155",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Loading and Preprocessing\n",
    "## Introduction\n",
    "This notebook loads PM2.5 data from the OpenAQ API and weather data from Open-Meteo. It merges the datasets, cleans them by handling missing values and extreme outliers, and saves the processed data as a CSV file.\n",
    "\n",
    "**Key Update**: Removed artificial PM2.5 capping at 150.5 µg/m³ that was causing 99.1% identical values. PM2.5 can legitimately reach 1000+ µg/m³ during severe pollution events (wildfires, industrial incidents, extreme smog). The data now preserves natural variation while handling true extreme outliers (>2000 µg/m³) for robust ML training.\n",
    "\n",
    "**Justification**: Using official APIs ensures reliable data. Natural PM2.5 variation is crucial for meaningful LSTM/GRU model comparison. Data integrity supports SDG 11 (Sustainable Cities) by enabling accurate air quality forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5425392",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "sensors_id = \"12178556\"  # Singapore PM2.5 Sensor ID\n",
    "latitude = 1.3521\n",
    "longitude = 103.8198\n",
    "location_name = \"Singapore\"\n",
    "openaq_api_key = \"4f1e60991c483fd961169d77137baa593d9568f4fa71585725860294b370bc43\"  # Replace with actual key\n",
    "\n",
    "# Define the filename for the processed data\n",
    "base_processed_file = f'sensor_{sensors_id}_{location_name}_pm25_weather_hourly_data_processed_final.csv'\n",
    "\n",
    "# CRITICAL FIX: Create the 'images' directory if it doesn't exist\n",
    "images_dir = 'images'\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "print(f\"Ensured '{images_dir}' directory exists at {os.getcwd()}/{images_dir}\")\n",
    "\n",
    "\n",
    "print(\"--- Starting Data Loading and Preprocessing ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866fc17",
   "metadata": {},
   "source": [
    "## Load Existing Processed Data (if available)\n",
    "\n",
    "**DEBUGGING FIX**: Identified that PM25_CAP = 150.5 was artificially capping all high PM2.5 values to the same number, causing 99.1% identical values. The API system is preserved for educational purposes - only the problematic capping line has been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dbfb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to hold the DataFrame, initialized to None\n",
    "merged_df = None\n",
    "\n",
    "# --- Attempt to Load Existing Processed Data ---\n",
    "if os.path.exists(base_processed_file):\n",
    "    print(f\"Attempting to load existing processed data from {base_processed_file}\")\n",
    "    try:\n",
    "        merged_df = pd.read_csv(base_processed_file, index_col='timestamp', parse_dates=True)\n",
    "        print(f\"Successfully loaded existing data with 'timestamp' as index. Shape: {merged_df.shape}\")\n",
    "        print(f\"Loaded DataFrame index name: {merged_df.index.name}\")\n",
    "        \n",
    "        # DEBUGGING CHECK: Verify if this has the 150.5 capping issue\n",
    "        if (merged_df['pm25_value'] == 150.5).sum() > len(merged_df) * 0.8:\n",
    "            print(f\"⚠️  WARNING: Found {(merged_df['pm25_value'] == 150.5).sum()} records with PM2.5 = 150.5\")\n",
    "            print(f\"   This represents {(merged_df['pm25_value'] == 150.5).sum()/len(merged_df)*100:.1f}% of data\")\n",
    "            print(f\"   Consider re-running data collection with modified PM25_CAP logic\")\n",
    "        else:\n",
    "            print(f\"✅ Data quality check: Only {(merged_df['pm25_value'] == 150.5).sum()} records at 150.5 ({(merged_df['pm25_value'] == 150.5).sum()/len(merged_df)*100:.1f}%)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading existing file: {e}\")\n",
    "        print(\"Proceeding to re-fetch and re-process data.\")\n",
    "        merged_df = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2b18d",
   "metadata": {},
   "source": [
    "## Fetch and Process Data (if not loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cbbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Fetching and Initial Processing (if not loaded from file) ---\n",
    "if merged_df is None:\n",
    "    print(\"Processed data not loaded (either not found or previous load failed). Initiating API data fetch and processing.\")\n",
    "\n",
    "    # --- Fetch PM2.5 Data from OpenAQ API ---\n",
    "    all_pm25_records = []\n",
    "    page = 1\n",
    "    limit_per_page = 1000\n",
    "    date_from_str = '2022-05-01'\n",
    "    date_to_str = '2024-04-30'\n",
    "\n",
    "    print(\"Fetching PM2.5 data...\")\n",
    "    while True:\n",
    "        base_url = f\"https://api.openaq.org/v3/sensors/{sensors_id}/hours\"\n",
    "        params = {\"date_from\": date_from_str, \"date_to\": date_to_str, \"limit\": limit_per_page, \"page\": page}\n",
    "        headers = {\"X-API-Key\": openaq_api_key}\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            results = data.get('results', [])\n",
    "            if not results:\n",
    "                print(f\"No more PM2.5 data found or end of data for sensor {sensors_id}.\")\n",
    "                break\n",
    "            for result in results:\n",
    "                all_pm25_records.append({'timestamp': result['period']['datetimeFrom']['utc'], 'pm25_value': result['value']})\n",
    "            print(f\"Fetched page {page} of PM2.5 data (total records: {len(all_pm25_records)})\")\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API request error for PM2.5 data: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during PM2.5 fetch: {e}\")\n",
    "            break\n",
    "\n",
    "    aq_df_raw = pd.DataFrame(all_pm25_records)\n",
    "    if not aq_df_raw.empty:\n",
    "        aq_df_raw['timestamp'] = pd.to_datetime(aq_df_raw['timestamp'], utc=True)\n",
    "        aq_df_raw.sort_values('timestamp', inplace=True)\n",
    "        aq_df_raw['pm25_value'] = pd.to_numeric(aq_df_raw['pm25_value'], errors='coerce')\n",
    "\n",
    "        print(\"\\n--- PM2.5 Data Initial Cleaning ---\")\n",
    "        initial_pm25_rows = aq_df_raw.shape[0]\n",
    "        aq_df_raw.dropna(subset=['pm25_value'], inplace=True)\n",
    "        print(f\"Removed {initial_pm25_rows - aq_df_raw.shape[0]} rows with non-numeric PM2.5 values.\")\n",
    "\n",
    "        initial_pm25_rows = aq_df_raw.shape[0]\n",
    "        aq_df_raw = aq_df_raw[aq_df_raw['pm25_value'] >= 0]\n",
    "        print(f\"Removed {initial_pm25_rows - aq_df_raw.shape[0]} rows with negative PM2.5 values.\")\n",
    "\n",
    "        # INTELLIGENT OUTLIER HANDLING: Handle extreme values while preserving natural variation\n",
    "        print(\"\\n--- Intelligent PM2.5 Outlier Analysis ---\")\n",
    "        print(f\"Raw PM2.5 value distribution:\")\n",
    "        print(f\"  Count: {len(aq_df_raw):,}\")\n",
    "        print(f\"  Mean: {aq_df_raw['pm25_value'].mean():.1f} µg/m³\")\n",
    "        print(f\"  Std: {aq_df_raw['pm25_value'].std():.1f} µg/m³\")\n",
    "        print(f\"  Min: {aq_df_raw['pm25_value'].min():.1f} µg/m³\")\n",
    "        print(f\"  Max: {aq_df_raw['pm25_value'].max():.1f} µg/m³\")\n",
    "        print(f\"  Median: {aq_df_raw['pm25_value'].median():.1f} µg/m³\")\n",
    "        \n",
    "        # Analyze value ranges for context\n",
    "        ranges = [\n",
    "            (\"Good (0-12)\", (aq_df_raw['pm25_value'] <= 12).sum()),\n",
    "            (\"Moderate (12-35)\", ((aq_df_raw['pm25_value'] > 12) & (aq_df_raw['pm25_value'] <= 35)).sum()),\n",
    "            (\"Unhealthy Sensitive (35-55)\", ((aq_df_raw['pm25_value'] > 35) & (aq_df_raw['pm25_value'] <= 55)).sum()),\n",
    "            (\"Unhealthy (55-150)\", ((aq_df_raw['pm25_value'] > 55) & (aq_df_raw['pm25_value'] <= 150)).sum()),\n",
    "            (\"Very Unhealthy (150-250)\", ((aq_df_raw['pm25_value'] > 150) & (aq_df_raw['pm25_value'] <= 250)).sum()),\n",
    "            (\"Hazardous (250-500)\", ((aq_df_raw['pm25_value'] > 250) & (aq_df_raw['pm25_value'] <= 500)).sum()),\n",
    "            (\"Extreme (500-1000)\", ((aq_df_raw['pm25_value'] > 500) & (aq_df_raw['pm25_value'] <= 1000)).sum()),\n",
    "            (\"Severe (1000+)\", (aq_df_raw['pm25_value'] > 1000).sum()),\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nPM2.5 Distribution by Air Quality Categories:\")\n",
    "        for range_name, count in ranges:\n",
    "            percentage = (count / len(aq_df_raw)) * 100\n",
    "            print(f\"  {range_name}: {count:,} records ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Handle only truly extreme outliers (sensor errors, not pollution events)\n",
    "        # Values >2000 µg/m³ are likely sensor malfunctions\n",
    "        extreme_outlier_threshold = 2000\n",
    "        extreme_outliers = aq_df_raw['pm25_value'] > extreme_outlier_threshold\n",
    "        \n",
    "        if extreme_outliers.sum() > 0:\n",
    "            print(f\"\\nWARNING: Found {extreme_outliers.sum()} extreme outliers > {extreme_outlier_threshold} µg/m³\")\n",
    "            print(\"These likely represent sensor malfunctions rather than real pollution events\")\n",
    "            print(f\"Extreme values: {sorted(aq_df_raw.loc[extreme_outliers, 'pm25_value'].tolist())}\")\n",
    "            \n",
    "            # Remove only the extreme outliers\n",
    "            aq_df_raw = aq_df_raw[~extreme_outliers]\n",
    "            print(f\"Removed extreme outliers, remaining records: {len(aq_df_raw):,}\")\n",
    "        else:\n",
    "            print(f\"No extreme outliers (>{extreme_outlier_threshold} µg/m³) found\")\n",
    "            \n",
    "        print(f\"\\nFINAL DECISION: Natural PM2.5 variation preserved\")\n",
    "        print(f\"   Range: {aq_df_raw['pm25_value'].min():.1f} - {aq_df_raw['pm25_value'].max():.1f} µg/m³\")\n",
    "        print(f\"   This includes legitimate high pollution events (1000+ µg/m³)\")\n",
    "        print(f\"   Data ready for ML with proper scaling in subsequent notebooks\")\n",
    "        \n",
    "        # Add scaling recommendations for ML\n",
    "        print(f\"\\nML SCALING RECOMMENDATIONS:\")\n",
    "        print(f\"   - Use MinMaxScaler or StandardScaler for wide PM2.5 range\")\n",
    "        print(f\"   - Consider log transformation for skewed distribution\")\n",
    "        print(f\"   - Feature scaling crucial for LSTM/GRU training stability\")\n",
    "    else:\n",
    "        print(\"No PM2.5 data fetched. Skipping PM2.5 cleaning.\")\n",
    "        merged_df = pd.DataFrame() # Set to empty to skip subsequent steps\n",
    "\n",
    "    if aq_df_raw.empty:\n",
    "        print(\"PM2.5 data is empty after initial processing. Skipping weather fetch and merge.\")\n",
    "        merged_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # --- Fetch Weather Data from Open-Meteo Archive API ---\n",
    "    weather_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    weather_params = {\n",
    "        \"latitude\": latitude, \"longitude\": longitude,\n",
    "        \"start_date\": aq_df_raw['timestamp'].min().strftime('%Y-%m-%d') if not aq_df_raw.empty else date_from_str,\n",
    "        \"end_date\": aq_df_raw['timestamp'].max().strftime('%Y-%m-%d') if not aq_df_raw.empty else date_to_str,\n",
    "        \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m,wind_direction_10m,precipitation\",\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "    print(\"\\nFetching weather data...\")\n",
    "    try:\n",
    "        response_weather = requests.get(weather_url, params=weather_params, timeout=30)\n",
    "        response_weather.raise_for_status()\n",
    "        weather_data = response_weather.json()['hourly']\n",
    "        weather_df = pd.DataFrame(weather_data)\n",
    "        weather_df['timestamp'] = pd.to_datetime(weather_df['time'], utc=True)\n",
    "        weather_df.drop(columns=['time'], inplace=True)\n",
    "        weather_df.sort_values('timestamp', inplace=True)\n",
    "        weather_df.rename(columns={\n",
    "            'temperature_2m': 'temp',\n",
    "            'relative_humidity_2m': 'humidity',\n",
    "            'wind_speed_10m': 'wind_speed',\n",
    "            'wind_direction_10m': 'wind_dir',\n",
    "            'precipitation': 'precipitation'\n",
    "        }, inplace=True)\n",
    "        print(f\"Weather data fetched. Shape: {weather_df.shape}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error for weather data: {e}\")\n",
    "        weather_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during weather fetch: {e}\")\n",
    "        weather_df = pd.DataFrame()\n",
    "\n",
    "    # --- Merge PM2.5 and Weather Data ---\n",
    "    if not aq_df_raw.empty and not weather_df.empty:\n",
    "        merged_df = pd.merge(aq_df_raw, weather_df, on='timestamp', how='inner')\n",
    "        print(f\"\\nMerged DataFrame shape after inner join: {merged_df.shape}\")\n",
    "\n",
    "        merged_df.set_index('timestamp', inplace=True)\n",
    "        merged_df.sort_index(inplace=True)\n",
    "        if merged_df.index.name is None:\n",
    "             merged_df.index.name = 'timestamp'\n",
    "        print(f\"Timestamp set as index. Current index name: {merged_df.index.name}\")\n",
    "\n",
    "        # --- Handle Missing Values (Interpolation and Final Dropna) ---\n",
    "        print(\"\\n--- Handling Missing Values in Merged Data ---\")\n",
    "        missing_before_interp = merged_df.isnull().sum()\n",
    "        print(\"Missing values before interpolation:\")\n",
    "        print(missing_before_interp[missing_before_interp > 0])\n",
    "\n",
    "        numerical_cols_to_interp = merged_df.select_dtypes(include=np.number).columns\n",
    "        merged_df[numerical_cols_to_interp] = merged_df[numerical_cols_to_interp].interpolate(method='linear', limit_direction='both')\n",
    "        print(\"Applied linear interpolation to numerical features.\")\n",
    "\n",
    "        initial_rows_after_interp = merged_df.shape[0]\n",
    "        merged_df.dropna(inplace=True)\n",
    "        rows_dropped_after_interp = initial_rows_after_interp - merged_df.shape[0]\n",
    "        if rows_dropped_after_interp > 0:\n",
    "            print(f\"Dropped {rows_dropped_after_interp} rows with remaining NaN values after interpolation.\")\n",
    "        else:\n",
    "            print(\"No additional rows dropped after interpolation.\")\n",
    "\n",
    "        print(\"Missing values after interpolation and final dropna:\")\n",
    "        print(merged_df.isnull().sum().sum())\n",
    "\n",
    "        # --- Feature Engineering: Time-based Features ---\n",
    "        print(\"\\n--- Engineering Time-based Features ---\")\n",
    "        merged_df['hour_of_day'] = merged_df.index.hour\n",
    "        merged_df['day_of_week'] = merged_df.index.dayofweek\n",
    "        merged_df['month'] = merged_df.index.month\n",
    "        merged_df['is_weekend'] = ((merged_df.index.dayofweek == 5) | (merged_df.index.dayofweek == 6)).astype(int)\n",
    "        print(\"Added 'hour_of_day', 'day_of_week', 'month', 'is_weekend' features.\")\n",
    "    else:\n",
    "        print(\"Could not merge dataframes due to empty PM2.5 or weather data. Skipping subsequent steps.\")\n",
    "        merged_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27806ef2",
   "metadata": {},
   "source": [
    "## Initial Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed with visualizations and saving only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # --- Quick Initial Data Exploration (Visualizations for context) ---\n",
    "    print(\"\\n--- Performing Quick Initial Data Visualizations ---\")\n",
    "\n",
    "    # Plot 1: Distribution of PM2.5 Value (with intelligent binning for wide range)\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Main distribution plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    # Use intelligent binning for wide range (0 to 1000+)\n",
    "    max_val = merged_df['pm25_value'].max()\n",
    "    if max_val > 500:\n",
    "        bins = np.concatenate([np.arange(0, 100, 5), np.arange(100, 500, 25), np.arange(500, max_val + 100, 100)])\n",
    "    else:\n",
    "        bins = 50\n",
    "    \n",
    "    sns.histplot(merged_df['pm25_value'], bins=bins, kde=True)\n",
    "    plt.title(f'PM2.5 Distribution (Full Range)\\nMax: {max_val:.0f} µg/m³, Unique values: {merged_df[\"pm25_value\"].nunique()}')\n",
    "    plt.xlabel('PM2.5 (µg/m³)')\n",
    "    plt.ylabel('Frequency')\n",
    "    # Add reference lines for air quality categories\n",
    "    plt.axvline(x=150.5, color='red', linestyle='--', alpha=0.7, label='Very Unhealthy (150.5)')\n",
    "    plt.axvline(x=250.5, color='orange', linestyle='--', alpha=0.7, label='Hazardous (250.5)')\n",
    "    if max_val > 500:\n",
    "        plt.axvline(x=500, color='darkred', linestyle='--', alpha=0.7, label='Extreme (500+)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Zoomed view for normal range (0-200)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    normal_range = merged_df[merged_df['pm25_value'] <= 200]\n",
    "    if len(normal_range) > 0:\n",
    "        sns.histplot(normal_range['pm25_value'], bins=30, kde=True)\n",
    "        plt.title(f'PM2.5 Distribution (Normal Range 0-200)\\n{len(normal_range):,} records ({len(normal_range)/len(merged_df)*100:.1f}%)')\n",
    "        plt.xlabel('PM2.5 (µg/m³)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log scale distribution to better see spread\n",
    "    plt.subplot(2, 2, 3)\n",
    "    # Add small constant to avoid log(0)\n",
    "    log_values = np.log10(merged_df['pm25_value'] + 1)\n",
    "    sns.histplot(log_values, bins=30, kde=True)\n",
    "    plt.title('PM2.5 Distribution (Log Scale)\\nlog₁₀(PM2.5 + 1)')\n",
    "    plt.xlabel('Log₁₀(PM2.5 + 1)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot to show quartiles and outliers\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.boxplot(merged_df['pm25_value'], patch_artist=True)\n",
    "    plt.title('PM2.5 Box Plot\\n(Shows quartiles and outliers)')\n",
    "    plt.ylabel('PM2.5 (µg/m³)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/plot_0_0_comprehensive.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Distribution of Temperature\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(merged_df['temp'], bins=30, kde=True)\n",
    "    plt.title('Distribution of Temperature')\n",
    "    plt.xlabel('Temperature (°C)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('images/plot_0_1.png') # Saving as plot_0_1.png\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Distribution of Relative Humidity\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.histplot(merged_df['humidity'], bins=30, kde=True)\n",
    "    plt.title('Distribution of Relative Humidity')\n",
    "    plt.xlabel('Humidity (%)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('images/plot_0_2.png') # Saving as plot_0_2.png\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 4: Daily Average PM2.5 over Time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    daily_avg_pm25 = merged_df['pm25_value'].resample('D').mean()\n",
    "    sns.lineplot(data=daily_avg_pm25)\n",
    "    plt.title('Daily Average PM2.5 over Time\\n(Debugging: Look for flat lines at 150.5)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('PM2.5 (µg/m³)')\n",
    "    plt.axhline(y=150.5, color='red', linestyle='--', alpha=0.5, label='150.5 threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('images/plot_0_3.png') # Saving as plot_0_3.png\n",
    "    plt.show()\n",
    "    \n",
    "    # COMPREHENSIVE DATA QUALITY ANALYSIS\n",
    "    print(f\"\\n=== FINAL DATASET ANALYSIS ===\")\n",
    "    total_records = len(merged_df)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"DATASET OVERVIEW:\")\n",
    "    print(f\"   Total records: {total_records:,}\")\n",
    "    print(f\"   Time span: {merged_df.index.min()} to {merged_df.index.max()}\")\n",
    "    print(f\"   Duration: {(merged_df.index.max() - merged_df.index.min()).days} days\")\n",
    "    \n",
    "    # PM2.5 comprehensive statistics\n",
    "    pm25_stats = merged_df['pm25_value'].describe()\n",
    "    print(f\"\\nPM2.5 STATISTICS:\")\n",
    "    print(f\"   Mean: {pm25_stats['mean']:.1f} µg/m³\")\n",
    "    print(f\"   Median: {pm25_stats['50%']:.1f} µg/m³\")\n",
    "    print(f\"   Std Dev: {pm25_stats['std']:.1f} µg/m³\")\n",
    "    print(f\"   Range: {pm25_stats['min']:.1f} - {pm25_stats['max']:.1f} µg/m³\")\n",
    "    print(f\"   Q1-Q3: {pm25_stats['25%']:.1f} - {pm25_stats['75%']:.1f} µg/m³\")\n",
    "    print(f\"   Unique values: {merged_df['pm25_value'].nunique():,}\")\n",
    "    \n",
    "    # Check for artificial capping issues\n",
    "    records_at_150_5 = (merged_df['pm25_value'] == 150.5).sum()\n",
    "    percentage_150_5 = (records_at_150_5 / total_records) * 100\n",
    "    \n",
    "    # High value analysis\n",
    "    high_values = [\n",
    "        (\"PM2.5 > 150\", (merged_df['pm25_value'] > 150).sum()),\n",
    "        (\"PM2.5 > 250\", (merged_df['pm25_value'] > 250).sum()),\n",
    "        (\"PM2.5 > 500\", (merged_df['pm25_value'] > 500).sum()),\n",
    "        (\"PM2.5 > 1000\", (merged_df['pm25_value'] > 1000).sum()),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nHIGH POLLUTION EVENTS:\")\n",
    "    for desc, count in high_values:\n",
    "        percentage = (count / total_records) * 100\n",
    "        print(f\"   {desc}: {count:,} records ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Data quality assessment\n",
    "    print(f\"\\nDATA QUALITY ASSESSMENT:\")\n",
    "    if percentage_150_5 > 80:\n",
    "        print(f\"   CRITICAL: {percentage_150_5:.1f}% values at exactly 150.5 (artificial capping issue)\")\n",
    "        print(f\"   Recommend re-running with fixed PM25_CAP logic\")\n",
    "    elif percentage_150_5 > 20:\n",
    "        print(f\"   MODERATE CONCERN: {percentage_150_5:.1f}% values at exactly 150.5\")\n",
    "        print(f\"   Monitor for potential capping artifacts\")\n",
    "    else:\n",
    "        print(f\"   ACCEPTABLE: Only {percentage_150_5:.1f}% values at 150.5 (natural variation)\")\n",
    "        print(f\"   Data quality suitable for robust ML training\")\n",
    "    \n",
    "    # ML Readiness Assessment\n",
    "    print(f\"\\nMACHINE LEARNING READINESS:\")\n",
    "    value_range = merged_df['pm25_value'].max() - merged_df['pm25_value'].min()\n",
    "    cv = pm25_stats['std'] / pm25_stats['mean']  # Coefficient of variation\n",
    "    \n",
    "    print(f\"   Value range: {value_range:.1f} µg/m³\")\n",
    "    print(f\"   Coefficient of variation: {cv:.2f}\")\n",
    "    \n",
    "    if merged_df['pm25_value'].nunique() > 1000 and cv > 0.5:\n",
    "        print(f\"   EXCELLENT variability for LSTM/GRU training\")\n",
    "        print(f\"   Natural high pollution events preserved\")\n",
    "    elif merged_df['pm25_value'].nunique() > 100:\n",
    "        print(f\"   GOOD variability for ML training\")\n",
    "    else:\n",
    "        print(f\"   LIMITED variability - check for data issues\")\n",
    "    \n",
    "    # Scaling recommendations\n",
    "    if pm25_stats['max'] > 1000:\n",
    "        print(f\"\\nSCALING RECOMMENDATIONS FOR ML:\")\n",
    "        print(f\"   CRITICAL: Use MinMaxScaler or RobustScaler\")\n",
    "        print(f\"   Consider log transformation: log(PM2.5 + 1)\")\n",
    "        print(f\"   Essential for LSTM/GRU convergence with wide range\")\n",
    "        print(f\"   Apply same scaling to all time series features\")\n",
    "\n",
    "    # --- Save Processed Data for Next Steps ---\n",
    "    print(\"\\n--- Saving Processed Data ---\")\n",
    "    merged_df.to_csv(base_processed_file, index=True)\n",
    "    print(f\"Processed and cleaned data saved to {base_processed_file}.\")\n",
    "else:\n",
    "    print(\"No data processed or merged. Skipping visualizations and saving.\")\n",
    "\n",
    "print(\"--- Data Loading and Preprocessing Complete ---\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
