{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: AI Modelling (FIXED Feature Engineering)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Loads data from Notebook 1, adds PROPERLY IMPLEMENTED enhanced features, trains RF and LSTM for horizons 1,3,6,12,24h.\n",
    "\n",
    "**CRITICAL FIX**: The previous version had incomplete feature engineering that would still cause straight-line predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "input_data_path = '/content/drive/MyDrive/AI_Sustainability_Project_lsa/sensor_12178556_Singapore_pm25_weather_hourly_data_processed_final.csv'\n",
    "\n",
    "print(f\"--- Starting AI Modelling (Notebook 3 - FIXED) ---\")\n",
    "print(f\"Loading pre-processed data from: {input_data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_data_path, index_col='timestamp', parse_dates=True)\n",
    "    print(f\"Data loaded successfully. Initial shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"PM2.5 variance in original data: {df['pm25_value'].var():.4f}\")\n",
    "    print(f\"PM2.5 range: {df['pm25_value'].min():.2f} to {df['pm25_value'].max():.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise SystemExit(\"Failed to load data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED FEATURE ENGINEERING - COMPLETE IMPLEMENTATION\n",
    "def create_comprehensive_features(data_df):\n",
    "    \"\"\"\n",
    "    FIXED: Complete feature engineering to capture temporal patterns and prevent flat predictions.\n",
    "    This addresses the root cause of straight-line predictions by creating meaningful temporal features.\n",
    "    \"\"\"\n",
    "    print(\"Creating comprehensive temporal features...\")\n",
    "    df_featured = data_df.copy()\n",
    "    \n",
    "    # Ensure all numeric columns are float64 to prevent dtype issues\n",
    "    numeric_cols = ['pm25_value', 'temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_featured.columns:\n",
    "            df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce')\n",
    "    \n",
    "    # 1. CRITICAL: Lag features with diverse time horizons\n",
    "    print(\"Adding lag features...\")\n",
    "    lags = [1, 2, 3, 6, 12, 24, 48, 72]  \n",
    "    features_to_lag = ['pm25_value', 'temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "    \n",
    "    for feature in features_to_lag:\n",
    "        if feature in df_featured.columns:\n",
    "            for lag in lags:\n",
    "                df_featured[f'{feature}_lag_{lag}'] = df_featured[feature].shift(lag)\n",
    "    \n",
    "    # 2. CRITICAL: Difference and trend features (captures change patterns)\n",
    "    print(\"Adding trend and difference features...\")\n",
    "    # PM2.5 trends - these are ESSENTIAL for temporal prediction\n",
    "    df_featured['pm25_diff_1h'] = df_featured['pm25_value'].diff(1)\n",
    "    df_featured['pm25_diff_3h'] = df_featured['pm25_value'].diff(3)\n",
    "    df_featured['pm25_diff_6h'] = df_featured['pm25_value'].diff(6)\n",
    "    df_featured['pm25_diff_12h'] = df_featured['pm25_value'].diff(12)\n",
    "    df_featured['pm25_diff_24h'] = df_featured['pm25_value'].diff(24)\n",
    "    \n",
    "    # Rate of change (percentage change)\n",
    "    df_featured['pm25_pct_change_1h'] = df_featured['pm25_value'].pct_change(1)\n",
    "    df_featured['pm25_pct_change_6h'] = df_featured['pm25_value'].pct_change(6)\n",
    "    df_featured['pm25_pct_change_24h'] = df_featured['pm25_value'].pct_change(24)\n",
    "    \n",
    "    # Weather trends (using same naming convention as evaluation)\n",
    "    if 'temp' in df_featured.columns:\n",
    "        df_featured['temp_diff_6h'] = df_featured['temp'].diff(6)\n",
    "    if 'humidity' in df_featured.columns:\n",
    "        df_featured['humidity_diff_6h'] = df_featured['humidity'].diff(6)\n",
    "    if 'wind_speed' in df_featured.columns:\n",
    "        df_featured['wind_speed_diff_6h'] = df_featured['wind_speed'].diff(6)\n",
    "    \n",
    "    # 3. Rolling statistics with proper min_periods\n",
    "    print(\"Adding rolling statistics...\")\n",
    "    windows = [3, 6, 12, 24, 48]\n",
    "    \n",
    "    for window in windows:\n",
    "        min_periods = max(2, window // 3)  # Better min_periods\n",
    "        \n",
    "        # PM2.5 rolling features\n",
    "        df_featured[f'pm25_mean_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).mean()\n",
    "        df_featured[f'pm25_std_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).std()\n",
    "        df_featured[f'pm25_min_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).min()\n",
    "        df_featured[f'pm25_max_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).max()\n",
    "        \n",
    "        # Weather rolling features  \n",
    "        if 'temp' in df_featured.columns:\n",
    "            df_featured[f'temp_mean_{window}h'] = df_featured['temp'].rolling(window=window, min_periods=min_periods).mean()\n",
    "        if 'humidity' in df_featured.columns:\n",
    "            df_featured[f'humidity_mean_{window}h'] = df_featured['humidity'].rolling(window=window, min_periods=min_periods).mean()\n",
    "        if 'wind_speed' in df_featured.columns:\n",
    "            df_featured[f'wind_speed_mean_{window}h'] = df_featured['wind_speed'].rolling(window=window, min_periods=min_periods).mean()\n",
    "    \n",
    "    # 4. Volatility and variability measures\n",
    "    print(\"Adding volatility features...\")\n",
    "    df_featured['pm25_volatility_12h'] = df_featured['pm25_value'].rolling(window=12, min_periods=6).std()\n",
    "    df_featured['pm25_volatility_24h'] = df_featured['pm25_value'].rolling(window=24, min_periods=12).std()\n",
    "    if 'temp' in df_featured.columns:\n",
    "        df_featured['temp_volatility_12h'] = df_featured['temp'].rolling(window=12, min_periods=6).std()\n",
    "    if 'humidity' in df_featured.columns:\n",
    "        df_featured['humidity_volatility_12h'] = df_featured['humidity'].rolling(window=12, min_periods=6).std()\n",
    "    \n",
    "    # 5. Exponential moving averages (trend following)\n",
    "    print(\"Adding exponential moving averages...\")\n",
    "    df_featured['pm25_ema_6h'] = df_featured['pm25_value'].ewm(span=6, adjust=False).mean()\n",
    "    df_featured['pm25_ema_24h'] = df_featured['pm25_value'].ewm(span=24, adjust=False).mean()\n",
    "    if 'temp' in df_featured.columns:\n",
    "        df_featured['temp_ema_12h'] = df_featured['temp'].ewm(span=12, adjust=False).mean()\n",
    "    \n",
    "    # 6. Enhanced cyclical encoding\n",
    "    print(\"Adding cyclical time features...\")\n",
    "    df_featured['hour_sin'] = np.sin(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['hour_cos'] = np.cos(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['day_of_week_sin'] = np.sin(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "    df_featured['day_of_week_cos'] = np.cos(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "    df_featured['month_sin'] = np.sin(2 * np.pi * df_featured.index.month / 12)\n",
    "    df_featured['month_cos'] = np.cos(2 * np.pi * df_featured.index.month / 12)\n",
    "    \n",
    "    # 7. Interaction features\n",
    "    print(\"Adding interaction features...\")\n",
    "    if 'wind_speed' in df_featured.columns and 'humidity' in df_featured.columns:\n",
    "        df_featured['wind_humidity_interaction'] = df_featured['wind_speed'] * df_featured['humidity']\n",
    "    if 'temp' in df_featured.columns and 'humidity' in df_featured.columns:\n",
    "        df_featured['temp_humidity_interaction'] = df_featured['temp'] * df_featured['humidity']\n",
    "    if 'wind_speed' in df_featured.columns and 'temp' in df_featured.columns:\n",
    "        df_featured['wind_temp_interaction'] = df_featured['wind_speed'] * df_featured['temp']\n",
    "    \n",
    "    # 8. Peak and valley detection\n",
    "    print(\"Adding peak detection features...\")\n",
    "    df_featured['is_pm25_peak'] = ((df_featured['pm25_value'] > df_featured['pm25_value'].shift(1)) & \n",
    "                                   (df_featured['pm25_value'] > df_featured['pm25_value'].shift(-1))).astype(int)\n",
    "    df_featured['is_pm25_valley'] = ((df_featured['pm25_value'] < df_featured['pm25_value'].shift(1)) & \n",
    "                                     (df_featured['pm25_value'] < df_featured['pm25_value'].shift(-1))).astype(int)\n",
    "    \n",
    "    # 9. Relative position features\n",
    "    print(\"Adding relative position features...\")\n",
    "    # Position relative to recent min/max\n",
    "    pm25_24h_min = df_featured['pm25_value'].rolling(window=24, min_periods=12).min()\n",
    "    pm25_24h_max = df_featured['pm25_value'].rolling(window=24, min_periods=12).max()\n",
    "    df_featured['pm25_relative_position'] = (df_featured['pm25_value'] - pm25_24h_min) / (pm25_24h_max - pm25_24h_min + 1e-8)\n",
    "    \n",
    "    # 10. Hour category encoding (consistent naming)\n",
    "    print(\"Adding categorical time features...\")\n",
    "    hour_bins = [0, 6, 12, 18, 24]\n",
    "    hour_labels = ['night', 'morning', 'afternoon', 'evening']\n",
    "    df_featured['hour_category'] = pd.cut(df_featured.index.hour, bins=hour_bins, labels=hour_labels, include_lowest=True)\n",
    "    \n",
    "    # One-hot encode with consistent naming\n",
    "    hour_dummies = pd.get_dummies(df_featured['hour_category'], prefix='hour_cat', dtype=float)\n",
    "    df_featured = pd.concat([df_featured, hour_dummies], axis=1)\n",
    "    df_featured.drop('hour_category', axis=1, inplace=True)\n",
    "    \n",
    "    # 11. Clean up infinite and missing values\n",
    "    print(\"Cleaning data...\")\n",
    "    # Replace infinite values\n",
    "    df_featured = df_featured.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Count initial NaNs\n",
    "    initial_shape = df_featured.shape[0]\n",
    "    initial_nans = df_featured.isnull().sum().sum()\n",
    "    \n",
    "    # Drop rows with NaNs\n",
    "    df_featured.dropna(inplace=True)\n",
    "    final_shape = df_featured.shape[0]\n",
    "    \n",
    "    # 12. CRITICAL: Ensure all columns are numeric\n",
    "    print(\"Ensuring all features are numeric...\")\n",
    "    for col in df_featured.columns:\n",
    "        if col != 'pm25_value':  # Keep target as is\n",
    "            df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce')\n",
    "    \n",
    "    # Final cleanup of any remaining NaNs introduced by conversion\n",
    "    df_featured.dropna(inplace=True)\n",
    "    final_final_shape = df_featured.shape[0]\n",
    "    \n",
    "    print(f\"Feature engineering complete:\")\n",
    "    print(f\"- Initial rows: {initial_shape}, Final rows: {final_final_shape}\")\n",
    "    print(f\"- Rows dropped: {initial_shape - final_final_shape}\")\n",
    "    print(f\"- Initial NaNs: {initial_nans}\")\n",
    "    print(f\"- Features created: {len(df_featured.columns) - len(data_df.columns)}\")\n",
    "    print(f\"- PM2.5 variance after features: {df_featured['pm25_value'].var():.4f}\")\n",
    "    \n",
    "    # Verify all columns are numeric\n",
    "    non_numeric = df_featured.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(f\"⚠️  WARNING: Non-numeric columns detected: {non_numeric}\")\n",
    "        for col in non_numeric:\n",
    "            if col != 'pm25_value':\n",
    "                df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce')\n",
    "        df_featured.dropna(inplace=True)\n",
    "        print(f\"✅ Converted to numeric. Final shape: {df_featured.shape}\")\n",
    "    else:\n",
    "        print(\"✅ All features are numeric\")\n",
    "    \n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"\\n--- Creating Comprehensive Features ---\")\n",
    "df_featured = create_comprehensive_features(df)\n",
    "print(f\"\\nFinal shape after feature engineering: {df_featured.shape}\")\n",
    "print(f\"Total features: {len(df_featured.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological train/test split\n",
    "print(\"\\n--- Performing Chronological Train/Test Split ---\")\n",
    "train_size = int(len(df_featured) * 0.8)\n",
    "train_df = df_featured.iloc[:train_size].copy()\n",
    "test_df = df_featured.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "print(f\"Train PM2.5 variance: {train_df['pm25_value'].var():.4f}\")\n",
    "print(f\"Test PM2.5 variance: {test_df['pm25_value'].var():.4f}\")\n",
    "\n",
    "# Define feature columns\n",
    "features_for_scaling = [col for col in train_df.columns if col != 'pm25_value' and 'target' not in col]\n",
    "print(f\"Number of features for modeling: {len(features_for_scaling)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"\\n--- Scaling Features ---\")\n",
    "scaler_x = StandardScaler()  # StandardScaler often works better than MinMaxScaler for complex features\n",
    "train_df[features_for_scaling] = scaler_x.fit_transform(train_df[features_for_scaling])\n",
    "test_df[features_for_scaling] = scaler_x.transform(test_df[features_for_scaling])\n",
    "\n",
    "joblib.dump(scaler_x, '/content/drive/MyDrive/AI_Sustainability_Project_lsa/scaler_x.pkl')\n",
    "print(\"Features scaled and scaler saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training loop\n",
    "horizons = [1, 3, 6, 12, 24]\n",
    "print(f\"\\n--- Training Models for Horizons: {horizons} ---\")\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n=== Processing Horizon: {h} hours ===\")\n",
    "    \n",
    "    # Create target\n",
    "    train_df['target_h'] = train_df['pm25_value'].shift(-h)\n",
    "    test_df['target_h'] = test_df['pm25_value'].shift(-h)\n",
    "    \n",
    "    # Drop NaN targets\n",
    "    train_h = train_df.dropna(subset=['target_h'])\n",
    "    test_h = test_df.dropna(subset=['target_h'])\n",
    "    \n",
    "    X_train = train_h[features_for_scaling]\n",
    "    y_train = train_h['target_h']\n",
    "    X_test = test_h[features_for_scaling]\n",
    "    y_test = test_h['target_h']\n",
    "    \n",
    "    print(f\"Training shapes: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "    print(f\"Target variance: {y_train.var():.4f} (good if > 1.0)\")\n",
    "    print(f\"Target range: {y_train.min():.2f} to {y_train.max():.2f}\")\n",
    "    \n",
    "    # CRITICAL: Verify data types before training\n",
    "    print(\"Verifying data types...\")\n",
    "    non_numeric_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric_features:\n",
    "        print(f\"⚠️  Converting non-numeric features: {non_numeric_features}\")\n",
    "        for col in non_numeric_features:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "        # Drop any rows with NaN after conversion\n",
    "        X_train.dropna(inplace=True)\n",
    "        y_train = y_train.loc[X_train.index]\n",
    "        X_test.dropna(inplace=True)\n",
    "        y_test = y_test.loc[X_test.index]\n",
    "        print(f\"✅ After cleanup: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "    \n",
    "    # Ensure all data is float32 for TensorFlow\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    # Target scaling for LSTM\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    joblib.dump(scaler_y, f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/scaler_y_h{h}.pkl')\n",
    "    \n",
    "    # Save feature names for evaluation consistency\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    joblib.dump(feature_names, f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/feature_names_h{h}.pkl')\n",
    "    print(f\"✅ Saved {len(feature_names)} feature names for evaluation consistency\")\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Random Forest\n",
    "    print(f\"\\nTraining Random Forest...\")\n",
    "    rf_params = {\n",
    "        'n_estimators': [150, 200, 300],\n",
    "        'max_depth': [15, 20, 25, None],\n",
    "        'min_samples_leaf': [1, 2, 3],\n",
    "        'max_features': ['sqrt', 'log2', 0.8]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf, rf_params, cv=tscv, scoring='neg_mean_squared_error', \n",
    "        n_iter=12, verbose=0, random_state=42\n",
    "    )\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    \n",
    "    joblib.dump(rf_search.best_estimator_, f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/rf_model_h{h}.pkl')\n",
    "    print(f\"RF Best RMSE: {np.sqrt(-rf_search.best_score_):.4f}\")\n",
    "    print(f\"RF Best params: {rf_search.best_params_}\")\n",
    "    \n",
    "    # LSTM with improved architecture and error handling\n",
    "    print(f\"\\nTraining LSTM...\")\n",
    "    try:\n",
    "        # Ensure data is properly shaped and typed for LSTM\n",
    "        X_train_lstm = X_train.values.astype(np.float32).reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "        \n",
    "        print(f\"LSTM input shape: {X_train_lstm.shape}\")\n",
    "        print(f\"LSTM target shape: {y_train_scaled.shape}\")\n",
    "        print(f\"Data types: X={X_train_lstm.dtype}, y={y_train_scaled.dtype}\")\n",
    "        \n",
    "        # Clear any previous models\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model_lstm = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=(1, X_train.shape[1])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            BatchNormalization(), \n",
    "            Dropout(0.3),\n",
    "            LSTM(32),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model_lstm.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        history = model_lstm.fit(\n",
    "            X_train_lstm, y_train_scaled,\n",
    "            epochs=200,\n",
    "            batch_size=64,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        model_lstm.save(f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/lstm_model_h{h}.keras')\n",
    "        print(f\"✅ LSTM trained successfully. Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "        \n",
    "        # Clean up for next iteration\n",
    "        del X_train_lstm, model_lstm, history\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LSTM training failed for horizon {h}h: {e}\")\n",
    "        print(f\"Skipping LSTM for this horizon and continuing...\")\n",
    "    \n",
    "    # Clean up target column for next iteration\n",
    "    train_df.drop('target_h', axis=1, inplace=True, errors='ignore')\n",
    "    test_df.drop('target_h', axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save featured data\n",
    "train_df.to_csv('/content/drive/MyDrive/AI_Sustainability_Project_lsa/train_featured_data.csv')\n",
    "test_df.to_csv('/content/drive/MyDrive/AI_Sustainability_Project_lsa/test_featured_data.csv')\n",
    "\n",
    "print(\"\\n=== AI Modelling Complete ===\")\n",
    "print(\"✅ Enhanced models with comprehensive temporal features trained\")\n",
    "print(\"✅ This SHOULD resolve the straight-line prediction issue\")\n",
    "print(\"✅ Models now have rich temporal context and variability\")\n",
    "print(\"\\nNext: Run Notebook 4 for evaluation - expect realistic PM2.5 predictions!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
