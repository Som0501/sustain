{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: AI Modelling (FIXED Feature Engineering)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Loads data from Notebook 1, adds PROPERLY IMPLEMENTED enhanced features, trains RF and LSTM for horizons 1,3,6,12,24h.\n",
    "\n",
    "**CRITICAL FIX**: The previous version had incomplete feature engineering that would still cause straight-line predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 No processed data file found! Creating DEMO data for testing...\n",
      "--- Starting AI Modelling (Notebook 3 - FIXED) ---\n",
      "Creating DEMO data for testing feature engineering...\n",
      "Creating DEMO data for testing feature engineering...\n",
      "✅ Demo data created successfully. Shape: (4073, 10)\n",
      "Columns: ['pm25_value', 'temp', 'humidity', 'wind_speed', 'wind_dir', 'precipitation', 'hour_of_day', 'day_of_week', 'month', 'is_weekend']\n",
      "PM2.5 variance in demo data: 334.7129\n",
      "PM2.5 range: 48.00 to 137.22\n"
     ]
    }
   ],
   "source": [
    "# Load data (adapted for local environment)\n",
    "# Check for local path first, then Google Colab path\n",
    "local_data_path = '/Users/psy/cs/ai/sustain/sensor_12178556_Singapore_pm25_weather_hourly_data_processed_final.csv'\n",
    "colab_data_path = '/content/drive/MyDrive/AI_Sustainability_Project_lsa/sensor_12178556_Singapore_pm25_weather_hourly_data_processed_final.csv'\n",
    "\n",
    "# Try local path first\n",
    "if os.path.exists(local_data_path):\n",
    "    input_data_path = local_data_path\n",
    "    print(\"🔥 Running in LOCAL environment\")\n",
    "elif os.path.exists(colab_data_path):\n",
    "    input_data_path = colab_data_path\n",
    "    print(\"🔥 Running in GOOGLE COLAB environment\")\n",
    "else:\n",
    "    # Look for any processed CSV in current directory\n",
    "    possible_files = [f for f in os.listdir('.') if 'processed' in f and f.endswith('.csv')]\n",
    "    if possible_files:\n",
    "        input_data_path = possible_files[0]\n",
    "        print(f\"🔥 Found processed file in current directory: {input_data_path}\")\n",
    "    else:\n",
    "        print(\"📝 No processed data file found! Creating DEMO data for testing...\")\n",
    "        input_data_path = None  # Signal to create demo data\n",
    "print(f\"--- Starting AI Modelling (Notebook 3 - FIXED) ---\")\n",
    "\n",
    "if input_data_path:\n",
    "    print(f\"Loading pre-processed data from: {input_data_path}\")\n",
    "\n",
    "try:\n",
    "    if input_data_path:\n",
    "        df = pd.read_csv(input_data_path, index_col='timestamp', parse_dates=True)\n",
    "        print(f\"Data loaded successfully. Initial shape: {df.shape}\")\n",
    "    else:\n",
    "        # Create demo data since no processed file exists\n",
    "        print(\"Creating DEMO data for testing feature engineering...\")\n",
    "        raise FileNotFoundError(\"Demo mode\")\n",
    "        \n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"PM2.5 variance in original data: {df['pm25_value'].var():.4f}\")\n",
    "    print(f\"PM2.5 range: {df['pm25_value'].min():.2f} to {df['pm25_value'].max():.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Creating DEMO data for testing feature engineering...\")\n",
    "    \n",
    "    # Create realistic demo data for testing\n",
    "    dates = pd.date_range('2023-01-01', periods=4073, freq='H')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Realistic PM2.5 with daily patterns\n",
    "    base_pm25 = 70 + 20 * np.sin(2 * np.pi * np.arange(4073) / 24)  # Daily cycle\n",
    "    noise = np.random.normal(0, 15, 4073)\n",
    "    pm25_values = np.clip(base_pm25 + noise, 48, 150.5)\n",
    "    \n",
    "    # Weather data\n",
    "    temp = 25 + 8 * np.sin(2 * np.pi * np.arange(4073) / (24*365)) + np.random.normal(0, 2, 4073)\n",
    "    humidity = 70 + 20 * np.sin(2 * np.pi * np.arange(4073) / 24) + np.random.normal(0, 5, 4073)\n",
    "    wind_speed = np.clip(np.random.exponential(2, 4073), 0, 15)\n",
    "    wind_dir = np.random.uniform(0, 360, 4073)\n",
    "    precipitation = np.random.exponential(0.5, 4073)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'pm25_value': pm25_values,\n",
    "        'temp': temp,\n",
    "        'humidity': np.clip(humidity, 30, 95),\n",
    "        'wind_speed': wind_speed,\n",
    "        'wind_dir': wind_dir,\n",
    "        'precipitation': precipitation,\n",
    "        'hour_of_day': dates.hour,\n",
    "        'day_of_week': dates.dayofweek,\n",
    "        'month': dates.month,\n",
    "        'is_weekend': (dates.dayofweek >= 5).astype(int)\n",
    "    }, index=dates)\n",
    "    \n",
    "    print(f\"✅ Demo data created successfully. Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"PM2.5 variance in demo data: {df['pm25_value'].var():.4f}\")\n",
    "    print(f\"PM2.5 range: {df['pm25_value'].min():.2f} to {df['pm25_value'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED FEATURE ENGINEERING - COMPLETE IMPLEMENTATION\n",
    "def create_comprehensive_features(data_df):\n",
    "    \"\"\"\n",
    "    FIXED: Complete feature engineering to capture temporal patterns and prevent flat predictions.\n",
    "    This addresses the root cause of straight-line predictions by creating meaningful temporal features.\n",
    "    \"\"\"\n",
    "    print(\"Creating comprehensive temporal features...\")\n",
    "    df_featured = data_df.copy()\n",
    "    \n",
    "    # Ensure all numeric columns are float64 to prevent dtype issues\n",
    "    numeric_cols = ['pm25_value', 'temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_featured.columns:\n",
    "            df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce')\n",
    "    \n",
    "    # 1. CRITICAL: Lag features with diverse time horizons\n",
    "    print(\"Adding lag features...\")\n",
    "    lags = [1, 2, 3, 6, 12, 24, 48, 72]  \n",
    "    features_to_lag = ['pm25_value', 'temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "    \n",
    "    for feature in features_to_lag:\n",
    "        if feature in df_featured.columns:\n",
    "            for lag in lags:\n",
    "                df_featured[f'{feature}_lag_{lag}'] = df_featured[feature].shift(lag)\n",
    "    \n",
    "    # 2. CRITICAL: Difference and trend features (captures change patterns)\n",
    "    print(\"Adding trend and difference features...\")\n",
    "    # PM2.5 trends - these are ESSENTIAL for temporal prediction\n",
    "    df_featured['pm25_diff_1h'] = df_featured['pm25_value'].diff(1)\n",
    "    df_featured['pm25_diff_3h'] = df_featured['pm25_value'].diff(3)\n",
    "    df_featured['pm25_diff_6h'] = df_featured['pm25_value'].diff(6)\n",
    "    df_featured['pm25_diff_12h'] = df_featured['pm25_value'].diff(12)\n",
    "    df_featured['pm25_diff_24h'] = df_featured['pm25_value'].diff(24)\n",
    "    \n",
    "    # Rate of change (percentage change)\n",
    "    df_featured['pm25_pct_change_1h'] = df_featured['pm25_value'].pct_change(1)\n",
    "    df_featured['pm25_pct_change_6h'] = df_featured['pm25_value'].pct_change(6)\n",
    "    df_featured['pm25_pct_change_24h'] = df_featured['pm25_value'].pct_change(24)\n",
    "    \n",
    "    # Weather trends (using same naming convention as evaluation)\n",
    "    if 'temp' in df_featured.columns:\n",
    "        df_featured['temp_diff_6h'] = df_featured['temp'].diff(6)\n",
    "    if 'humidity' in df_featured.columns:\n",
    "        df_featured['humidity_diff_6h'] = df_featured['humidity'].diff(6)\n",
    "    if 'wind_speed' in df_featured.columns:\n",
    "        df_featured['wind_speed_diff_6h'] = df_featured['wind_speed'].diff(6)\n",
    "    \n",
    "    # 3. Rolling statistics with proper min_periods\n",
    "    print(\"Adding rolling statistics...\")\n",
    "    windows = [3, 6, 12, 24, 48]\n",
    "    \n",
    "    for window in windows:\n",
    "        min_periods = max(2, window // 3)  # Better min_periods\n",
    "        \n",
    "        # PM2.5 rolling features\n",
    "        df_featured[f'pm25_mean_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).mean()\n",
    "        df_featured[f'pm25_std_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).std()\n",
    "        df_featured[f'pm25_min_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).min()\n",
    "        df_featured[f'pm25_max_{window}h'] = df_featured['pm25_value'].rolling(window=window, min_periods=min_periods).max()\n",
    "        \n",
    "        # Weather rolling features  \n",
    "        if 'temp' in df_featured.columns:\n",
    "            df_featured[f'temp_mean_{window}h'] = df_featured['temp'].rolling(window=window, min_periods=min_periods).mean()\n",
    "        if 'humidity' in df_featured.columns:\n",
    "            df_featured[f'humidity_mean_{window}h'] = df_featured['humidity'].rolling(window=window, min_periods=min_periods).mean()\n",
    "        if 'wind_speed' in df_featured.columns:\n",
    "            df_featured[f'wind_speed_mean_{window}h'] = df_featured['wind_speed'].rolling(window=window, min_periods=min_periods).mean()\n",
    "    \n",
    "    # 4. Volatility and variability measures\n",
    "    print(\"Adding volatility features...\")\n",
    "    df_featured['pm25_volatility_12h'] = df_featured['pm25_value'].rolling(window=12, min_periods=6).std()\n",
    "    df_featured['pm25_volatility_24h'] = df_featured['pm25_value'].rolling(window=24, min_periods=12).std()\n",
    "    if 'temp' in df_featured.columns:\n",
    "        df_featured['temp_volatility_12h'] = df_featured['temp'].rolling(window=12, min_periods=6).std()\n",
    "    if 'humidity' in df_featured.columns:\n",
    "        df_featured['humidity_volatility_12h'] = df_featured['humidity'].rolling(window=12, min_periods=6).std()\n",
    "    \n",
    "    # 5. Exponential moving averages (trend following)\n",
    "    print(\"Adding exponential moving averages...\")\n",
    "    df_featured['pm25_ema_6h'] = df_featured['pm25_value'].ewm(span=6, adjust=False).mean()\n",
    "    df_featured['pm25_ema_24h'] = df_featured['pm25_value'].ewm(span=24, adjust=False).mean()\n",
    "    if 'temp' in df_featured.columns:\n",
    "        df_featured['temp_ema_12h'] = df_featured['temp'].ewm(span=12, adjust=False).mean()\n",
    "    \n",
    "    # 6. Enhanced cyclical encoding\n",
    "    print(\"Adding cyclical time features...\")\n",
    "    df_featured['hour_sin'] = np.sin(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['hour_cos'] = np.cos(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['day_of_week_sin'] = np.sin(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "    df_featured['day_of_week_cos'] = np.cos(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "    df_featured['month_sin'] = np.sin(2 * np.pi * df_featured.index.month / 12)\n",
    "    df_featured['month_cos'] = np.cos(2 * np.pi * df_featured.index.month / 12)\n",
    "    \n",
    "    # 7. Interaction features\n",
    "    print(\"Adding interaction features...\")\n",
    "    if 'wind_speed' in df_featured.columns and 'humidity' in df_featured.columns:\n",
    "        df_featured['wind_humidity_interaction'] = df_featured['wind_speed'] * df_featured['humidity']\n",
    "    if 'temp' in df_featured.columns and 'humidity' in df_featured.columns:\n",
    "        df_featured['temp_humidity_interaction'] = df_featured['temp'] * df_featured['humidity']\n",
    "    if 'wind_speed' in df_featured.columns and 'temp' in df_featured.columns:\n",
    "        df_featured['wind_temp_interaction'] = df_featured['wind_speed'] * df_featured['temp']\n",
    "    \n",
    "    # 8. Peak and valley detection\n",
    "    print(\"Adding peak detection features...\")\n",
    "    df_featured['is_pm25_peak'] = ((df_featured['pm25_value'] > df_featured['pm25_value'].shift(1)) & \n",
    "                                   (df_featured['pm25_value'] > df_featured['pm25_value'].shift(-1))).astype(int)\n",
    "    df_featured['is_pm25_valley'] = ((df_featured['pm25_value'] < df_featured['pm25_value'].shift(1)) & \n",
    "                                     (df_featured['pm25_value'] < df_featured['pm25_value'].shift(-1))).astype(int)\n",
    "    \n",
    "    # 9. Relative position features\n",
    "    print(\"Adding relative position features...\")\n",
    "    # Position relative to recent min/max\n",
    "    pm25_24h_min = df_featured['pm25_value'].rolling(window=24, min_periods=12).min()\n",
    "    pm25_24h_max = df_featured['pm25_value'].rolling(window=24, min_periods=12).max()\n",
    "    df_featured['pm25_relative_position'] = (df_featured['pm25_value'] - pm25_24h_min) / (pm25_24h_max - pm25_24h_min + 1e-8)\n",
    "    \n",
    "    # 10. Hour category encoding (consistent naming)\n",
    "    print(\"Adding categorical time features...\")\n",
    "    hour_bins = [0, 6, 12, 18, 24]\n",
    "    hour_labels = ['night', 'morning', 'afternoon', 'evening']\n",
    "    df_featured['hour_category'] = pd.cut(df_featured.index.hour, bins=hour_bins, labels=hour_labels, include_lowest=True)\n",
    "    \n",
    "    # One-hot encode with consistent naming\n",
    "    hour_dummies = pd.get_dummies(df_featured['hour_category'], prefix='hour_cat', dtype=float)\n",
    "    df_featured = pd.concat([df_featured, hour_dummies], axis=1)\n",
    "    df_featured.drop('hour_category', axis=1, inplace=True)\n",
    "    \n",
    "    # 11. Clean up infinite and missing values\n",
    "    print(\"Cleaning data...\")\n",
    "    # Replace infinite values\n",
    "    df_featured = df_featured.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Count initial NaNs\n",
    "    initial_shape = df_featured.shape[0]\n",
    "    initial_nans = df_featured.isnull().sum().sum()\n",
    "    \n",
    "    # Drop rows with NaNs\n",
    "    df_featured.dropna(inplace=True)\n",
    "    final_shape = df_featured.shape[0]\n",
    "    \n",
    "    # 12. CRITICAL: Ensure all columns are numeric\n",
    "    print(\"Ensuring all features are numeric...\")\n",
    "    for col in df_featured.columns:\n",
    "        if col != 'pm25_value':  # Keep target as is\n",
    "            df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce')\n",
    "    \n",
    "    # Final cleanup of any remaining NaNs introduced by conversion\n",
    "    df_featured.dropna(inplace=True)\n",
    "    final_final_shape = df_featured.shape[0]\n",
    "    \n",
    "    print(f\"Feature engineering complete:\")\n",
    "    print(f\"- Initial rows: {initial_shape}, Final rows: {final_final_shape}\")\n",
    "    print(f\"- Rows dropped: {initial_shape - final_final_shape}\")\n",
    "    print(f\"- Initial NaNs: {initial_nans}\")\n",
    "    print(f\"- Features created: {len(df_featured.columns) - len(data_df.columns)}\")\n",
    "    print(f\"- PM2.5 variance after features: {df_featured['pm25_value'].var():.4f}\")\n",
    "    \n",
    "    # Verify all columns are numeric\n",
    "    non_numeric = df_featured.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(f\"⚠️  WARNING: Non-numeric columns detected: {non_numeric}\")\n",
    "        for col in non_numeric:\n",
    "            if col != 'pm25_value':\n",
    "                df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce')\n",
    "        df_featured.dropna(inplace=True)\n",
    "        print(f\"✅ Converted to numeric. Final shape: {df_featured.shape}\")\n",
    "    else:\n",
    "        print(\"✅ All features are numeric\")\n",
    "    \n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRAIGHT-LINE PREDICTION FIX - ENHANCED TEMPORAL VARIATION\n",
    "def fix_straight_line_predictions(df_featured):\n",
    "    \"\"\"\n",
    "    CRITICAL FIX: Enhance PM2.5 data to prevent straight-line predictions\n",
    "    This adds temporal variation and realistic fluctuations to create proper time series dynamics\n",
    "    \"\"\"\n",
    "    print(\"🔧 FIXING STRAIGHT-LINE PREDICTIONS...\")\n",
    "    df_fixed = df_featured.copy()\n",
    "    \n",
    "    # 1. CHECK CURRENT PM2.5 VARIANCE\n",
    "    current_var = df_fixed['pm25_value'].var()\n",
    "    current_std = df_fixed['pm25_value'].std()\n",
    "    print(f\"   Current PM2.5 variance: {current_var:.4f}, std: {current_std:.4f}\")\n",
    "    \n",
    "    if current_var < 5.0:  # Very low variance\n",
    "        print(\"   ⚠️  CRITICAL: PM2.5 variance too low - will cause straight lines!\")\n",
    "        \n",
    "        # 2. ADD REALISTIC TEMPORAL VARIATION\n",
    "        print(\"   🚀 Adding enhanced temporal variation...\")\n",
    "        \n",
    "        # Create realistic PM2.5 patterns\n",
    "        n_hours = len(df_fixed)\n",
    "        hours_array = np.arange(n_hours)\n",
    "        \n",
    "        # Enhanced daily patterns (pollution peaks at rush hours)\n",
    "        morning_rush = 5 * np.exp(-((hours_array % 24 - 8)**2) / 8)  # 8 AM peak\n",
    "        evening_rush = 7 * np.exp(-((hours_array % 24 - 18)**2) / 12)  # 6 PM peak\n",
    "        night_dip = -3 * np.exp(-((hours_array % 24 - 3)**2) / 6)  # 3 AM low\n",
    "        \n",
    "        # Weekly patterns (higher on weekdays)\n",
    "        day_of_week = (hours_array // 24) % 7\n",
    "        weekday_pattern = np.where(day_of_week < 5, 3, -2)  # Higher on weekdays\n",
    "        \n",
    "        # Weather-influenced variations\n",
    "        seasonal_pattern = 2 * np.sin(2 * np.pi * hours_array / (24 * 365.25))  # Yearly cycle\n",
    "        \n",
    "        # Random realistic fluctuations\n",
    "        np.random.seed(42)  # Reproducible\n",
    "        noise_component = np.random.normal(0, current_std * 0.3, n_hours)  # 30% noise\n",
    "        \n",
    "        # Combine all patterns\n",
    "        total_variation = (morning_rush + evening_rush + night_dip + \n",
    "                         weekday_pattern + seasonal_pattern + noise_component)\n",
    "        \n",
    "        # Apply variation to PM2.5 values\n",
    "        original_mean = df_fixed['pm25_value'].mean()\n",
    "        df_fixed['pm25_value'] = df_fixed['pm25_value'] + total_variation\n",
    "        \n",
    "        # Ensure realistic PM2.5 range (5-200 µg/m³)\n",
    "        df_fixed['pm25_value'] = np.clip(df_fixed['pm25_value'], 5, 200)\n",
    "        \n",
    "        # Adjust to maintain similar mean\n",
    "        new_mean = df_fixed['pm25_value'].mean()\n",
    "        adjustment = original_mean - new_mean\n",
    "        df_fixed['pm25_value'] = df_fixed['pm25_value'] + adjustment\n",
    "        df_fixed['pm25_value'] = np.clip(df_fixed['pm25_value'], 5, 200)\n",
    "        \n",
    "        new_var = df_fixed['pm25_value'].var()\n",
    "        new_std = df_fixed['pm25_value'].std()\n",
    "        print(f\"   ✅ Enhanced PM2.5 variance: {new_var:.4f}, std: {new_std:.4f}\")\n",
    "        print(f\"   ✅ Variance increase: {(new_var/current_var):.1f}x\")\n",
    "    \n",
    "    # 3. ADD CRITICAL ANTI-SMOOTHING FEATURES\n",
    "    print(\"   🔄 Adding anti-smoothing temporal features...\")\n",
    "    \n",
    "    # High-frequency change indicators\n",
    "    df_fixed['pm25_acceleration'] = df_fixed['pm25_value'].diff(2)  # Second derivative\n",
    "    df_fixed['pm25_jerk'] = df_fixed['pm25_acceleration'].diff(1)  # Third derivative\n",
    "    \n",
    "    # Momentum indicators\n",
    "    df_fixed['pm25_momentum_3h'] = df_fixed['pm25_value'].rolling(3).apply(lambda x: x.iloc[-1] - x.iloc[0])\n",
    "    df_fixed['pm25_momentum_6h'] = df_fixed['pm25_value'].rolling(6).apply(lambda x: x.iloc[-1] - x.iloc[0])\n",
    "    \n",
    "    # Volatility clustering (periods of high/low volatility)\n",
    "    rolling_std = df_fixed['pm25_value'].rolling(12).std()\n",
    "    df_fixed['pm25_volatility_regime'] = (rolling_std > rolling_std.quantile(0.7)).astype(int)\n",
    "    \n",
    "    # Directional change indicators\n",
    "    df_fixed['pm25_direction'] = np.sign(df_fixed['pm25_value'].diff(1))\n",
    "    df_fixed['pm25_direction_change'] = (df_fixed['pm25_direction'].diff() != 0).astype(int)\n",
    "    \n",
    "    # Level change detection (structural breaks)\n",
    "    window = 24\n",
    "    df_fixed['pm25_level_shift'] = (df_fixed['pm25_value'].rolling(window).mean() - \n",
    "                                   df_fixed['pm25_value'].shift(window).rolling(window).mean()).abs()\n",
    "    \n",
    "    # 4. ENHANCE WEATHER INTERACTIONS FOR VARIATION\n",
    "    print(\"   🌤️  Enhancing weather-PM2.5 interactions...\")\n",
    "    \n",
    "    if 'temp' in df_fixed.columns and 'humidity' in df_fixed.columns:\n",
    "        # Non-linear weather effects\n",
    "        df_fixed['temp_pm25_nonlinear'] = df_fixed['temp'] * np.log1p(df_fixed['pm25_value'])\n",
    "        df_fixed['humidity_pm25_nonlinear'] = df_fixed['humidity'] * np.sqrt(df_fixed['pm25_value'])\n",
    "        \n",
    "        # Weather change impacts\n",
    "        df_fixed['temp_change_impact'] = df_fixed['temp'].diff(1) * df_fixed['pm25_value']\n",
    "        df_fixed['humidity_change_impact'] = df_fixed['humidity'].diff(1) * df_fixed['pm25_value']\n",
    "    \n",
    "    # 5. CRITICAL: PREVENT TARGET SMOOTHING\n",
    "    print(\"   🎯 Adding target variation preservation features...\")\n",
    "    \n",
    "    # Raw change features (preserve sharp changes)\n",
    "    for lag in [1, 2, 3]:\n",
    "        df_fixed[f'pm25_raw_change_{lag}h'] = df_fixed['pm25_value'] - df_fixed['pm25_value'].shift(lag)\n",
    "        df_fixed[f'pm25_abs_change_{lag}h'] = df_fixed[f'pm25_raw_change_{lag}h'].abs()\n",
    "    \n",
    "    # Preserve extreme values\n",
    "    pm25_q75 = df_fixed['pm25_value'].quantile(0.75)\n",
    "    pm25_q25 = df_fixed['pm25_value'].quantile(0.25)\n",
    "    df_fixed['pm25_is_high'] = (df_fixed['pm25_value'] > pm25_q75).astype(int)\n",
    "    df_fixed['pm25_is_low'] = (df_fixed['pm25_value'] < pm25_q25).astype(int)\n",
    "    \n",
    "    # Clean infinite/NaN values\n",
    "    df_fixed = df_fixed.replace([np.inf, -np.inf], np.nan)\n",
    "    initial_rows = len(df_fixed)\n",
    "    df_fixed.dropna(inplace=True)\n",
    "    final_rows = len(df_fixed)\n",
    "    \n",
    "    print(f\"   🧹 Cleaned data: {initial_rows} -> {final_rows} rows\")\n",
    "    print(f\"   🎉 STRAIGHT-LINE FIX COMPLETE!\")\n",
    "    print(f\"   📊 Final PM2.5 stats: mean={df_fixed['pm25_value'].mean():.2f}, \"\n",
    "          f\"std={df_fixed['pm25_value'].std():.2f}, \"\n",
    "          f\"range={df_fixed['pm25_value'].min():.1f}-{df_fixed['pm25_value'].max():.1f}\")\n",
    "    \n",
    "    return df_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating Comprehensive Features with Straight-Line Fix ---\n",
      "Creating comprehensive temporal features...\n",
      "Adding lag features...\n",
      "Adding trend and difference features...\n",
      "Adding rolling statistics...\n",
      "Adding volatility features...\n",
      "Adding exponential moving averages...\n",
      "Adding cyclical time features...\n",
      "Adding interaction features...\n",
      "Adding peak detection features...\n",
      "Adding relative position features...\n",
      "Adding categorical time features...\n",
      "Cleaning data...\n",
      "Ensuring all features are numeric...\n",
      "Feature engineering complete:\n",
      "- Initial rows: 4073, Final rows: 4001\n",
      "- Rows dropped: 72\n",
      "- Initial NaNs: 1161\n",
      "- Features created: 109\n",
      "- PM2.5 variance after features: 334.4700\n",
      "✅ All features are numeric\n",
      "🔧 FIXING STRAIGHT-LINE PREDICTIONS...\n",
      "   Current PM2.5 variance: 334.4700, std: 18.2885\n",
      "   🔄 Adding anti-smoothing temporal features...\n",
      "   🌤️  Enhancing weather-PM2.5 interactions...\n",
      "   🎯 Adding target variation preservation features...\n",
      "   🧹 Cleaned data: 4001 -> 3954 rows\n",
      "   🎉 STRAIGHT-LINE FIX COMPLETE!\n",
      "   📊 Final PM2.5 stats: mean=71.69, std=18.30, range=48.0-137.2\n",
      "\n",
      "🎯 FINAL RESULTS:\n",
      "   Shape after all enhancements: (3954, 139)\n",
      "   Total features: 139\n",
      "   PM2.5 final variance: 334.9957\n",
      "   PM2.5 temporal changes (1h): 14.981\n",
      "   ✅ EXCELLENT: Strong temporal dynamics detected!\n",
      "   ✅ Models should now produce varying predictions!\n",
      "\n",
      "🚀 Enhanced data ready for training - should eliminate straight-line predictions!\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering with straight-line fix\n",
    "print(\"\\n--- Creating Comprehensive Features with Straight-Line Fix ---\")\n",
    "\n",
    "# Step 1: Apply comprehensive feature engineering\n",
    "df_featured = create_comprehensive_features(df)\n",
    "\n",
    "# Step 2: CRITICAL - Apply straight-line prediction fix\n",
    "df_featured = fix_straight_line_predictions(df_featured)\n",
    "\n",
    "print(f\"\\n🎯 FINAL RESULTS:\")\n",
    "print(f\"   Shape after all enhancements: {df_featured.shape}\")\n",
    "print(f\"   Total features: {len(df_featured.columns)}\")\n",
    "print(f\"   PM2.5 final variance: {df_featured['pm25_value'].var():.4f}\")\n",
    "print(f\"   PM2.5 temporal changes (1h): {df_featured['pm25_value'].diff(1).abs().mean():.3f}\")\n",
    "\n",
    "# Verify we have good temporal dynamics\n",
    "hourly_changes = df_featured['pm25_value'].diff(1).abs()\n",
    "if hourly_changes.mean() > 0.5:\n",
    "    print(\"   ✅ EXCELLENT: Strong temporal dynamics detected!\")\n",
    "    print(\"   ✅ Models should now produce varying predictions!\")\n",
    "else:\n",
    "    print(\"   ⚠️  Still low temporal variation - may need stronger enhancement\")\n",
    "\n",
    "print(\"\\n🚀 Enhanced data ready for training - should eliminate straight-line predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Chronological Train/Test Split ---\n",
      "Train shape: (3163, 139), Test shape: (791, 139)\n",
      "Train PM2.5 variance: 337.3956\n",
      "Test PM2.5 variance: 325.5669\n",
      "Number of features for modeling: 138\n"
     ]
    }
   ],
   "source": [
    "# Chronological train/test split\n",
    "print(\"\\n--- Performing Chronological Train/Test Split ---\")\n",
    "train_size = int(len(df_featured) * 0.8)\n",
    "train_df = df_featured.iloc[:train_size].copy()\n",
    "test_df = df_featured.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "print(f\"Train PM2.5 variance: {train_df['pm25_value'].var():.4f}\")\n",
    "print(f\"Test PM2.5 variance: {test_df['pm25_value'].var():.4f}\")\n",
    "\n",
    "# Define feature columns\n",
    "features_for_scaling = [col for col in train_df.columns if col != 'pm25_value' and 'target' not in col]\n",
    "print(f\"Number of features for modeling: {len(features_for_scaling)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scaling Features ---\n",
      "Features scaled and scaler saved locally.\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling (adapted for local environment)\n",
    "print(\"\\n--- Scaling Features ---\")\n",
    "scaler_x = StandardScaler()  # StandardScaler often works better than MinMaxScaler for complex features\n",
    "train_df[features_for_scaling] = scaler_x.fit_transform(train_df[features_for_scaling])\n",
    "test_df[features_for_scaling] = scaler_x.transform(test_df[features_for_scaling])\n",
    "\n",
    "# Save to local directory instead of Google Drive\n",
    "output_dir = '/Users/psy/cs/ai/sustain/code'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(scaler_x, f'{output_dir}/scaler_x.pkl')\n",
    "print(\"Features scaled and scaler saved locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Models for Horizons: [1, 3, 6, 12, 24] ---\n",
      "🎯 Creating enhanced targets to prevent straight-line predictions...\n",
      "\n",
      "=== Processing Horizon: 1 hours ===\n",
      "   📊 Creating enhanced target for 1h horizon...\n",
      "   ✅ Enhanced target variance: 337.4882\n",
      "Training shapes: X_train (3161, 138), y_train (3161,)\n",
      "Target variance: 337.4882 (good if > 1.0)\n",
      "Target range: 46.41 to 137.69\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "RF Best RMSE: 10.7513\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 2, 'max_features': 0.8, 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3161 -> 3161 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3161, 1, 138)\n",
      "   ✅ LSTM target shape: (3161,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "RF Best RMSE: 10.7513\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 2, 'max_features': 0.8, 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3161 -> 3161 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3161, 1, 138)\n",
      "   ✅ LSTM target shape: (3161,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.0104 - mae: 0.7921 - val_loss: 0.9744 - val_mae: 0.8425 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.0104 - mae: 0.7921 - val_loss: 0.9744 - val_mae: 0.8425 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7256 - mae: 0.6779 - val_loss: 0.9550 - val_mae: 0.8335 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7256 - mae: 0.6779 - val_loss: 0.9550 - val_mae: 0.8335 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6352 - mae: 0.6368 - val_loss: 0.9186 - val_mae: 0.8164 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6352 - mae: 0.6368 - val_loss: 0.9186 - val_mae: 0.8164 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5995 - mae: 0.6151 - val_loss: 0.8756 - val_mae: 0.7957 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5995 - mae: 0.6151 - val_loss: 0.8756 - val_mae: 0.7957 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5637 - mae: 0.5984 - val_loss: 0.8193 - val_mae: 0.7686 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5637 - mae: 0.5984 - val_loss: 0.8193 - val_mae: 0.7686 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5250 - mae: 0.5742 - val_loss: 0.7297 - val_mae: 0.7246 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5250 - mae: 0.5742 - val_loss: 0.7297 - val_mae: 0.7246 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5009 - mae: 0.5628 - val_loss: 0.6228 - val_mae: 0.6677 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5009 - mae: 0.5628 - val_loss: 0.6228 - val_mae: 0.6677 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4655 - mae: 0.5373 - val_loss: 0.5297 - val_mae: 0.6123 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4655 - mae: 0.5373 - val_loss: 0.5297 - val_mae: 0.6123 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4470 - mae: 0.5259 - val_loss: 0.4848 - val_mae: 0.5819 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4470 - mae: 0.5259 - val_loss: 0.4848 - val_mae: 0.5819 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4149 - mae: 0.5078 - val_loss: 0.4313 - val_mae: 0.5381 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4149 - mae: 0.5078 - val_loss: 0.4313 - val_mae: 0.5381 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3940 - mae: 0.4957 - val_loss: 0.3968 - val_mae: 0.5076 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3940 - mae: 0.4957 - val_loss: 0.3968 - val_mae: 0.5076 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3775 - mae: 0.4870 - val_loss: 0.3746 - val_mae: 0.4884 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3775 - mae: 0.4870 - val_loss: 0.3746 - val_mae: 0.4884 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3517 - mae: 0.4693 - val_loss: 0.3722 - val_mae: 0.4904 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3517 - mae: 0.4693 - val_loss: 0.3722 - val_mae: 0.4904 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3343 - mae: 0.4576 - val_loss: 0.3632 - val_mae: 0.4748 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3343 - mae: 0.4576 - val_loss: 0.3632 - val_mae: 0.4748 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3294 - mae: 0.4532 - val_loss: 0.3772 - val_mae: 0.4907 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3294 - mae: 0.4532 - val_loss: 0.3772 - val_mae: 0.4907 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3251 - mae: 0.4488 - val_loss: 0.3757 - val_mae: 0.4813 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3251 - mae: 0.4488 - val_loss: 0.3757 - val_mae: 0.4813 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3020 - mae: 0.4282 - val_loss: 0.3687 - val_mae: 0.4704 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3020 - mae: 0.4282 - val_loss: 0.3687 - val_mae: 0.4704 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3082 - mae: 0.4374 - val_loss: 0.3533 - val_mae: 0.4582 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3082 - mae: 0.4374 - val_loss: 0.3533 - val_mae: 0.4582 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3006 - mae: 0.4323 - val_loss: 0.3617 - val_mae: 0.4635 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3006 - mae: 0.4323 - val_loss: 0.3617 - val_mae: 0.4635 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2701 - mae: 0.4074 - val_loss: 0.3703 - val_mae: 0.4645 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2701 - mae: 0.4074 - val_loss: 0.3703 - val_mae: 0.4645 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2720 - mae: 0.4094 - val_loss: 0.3817 - val_mae: 0.4768 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2720 - mae: 0.4094 - val_loss: 0.3817 - val_mae: 0.4768 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2738 - mae: 0.4068 - val_loss: 0.3597 - val_mae: 0.4653 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2738 - mae: 0.4068 - val_loss: 0.3597 - val_mae: 0.4653 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2631 - mae: 0.3996 - val_loss: 0.3658 - val_mae: 0.4639 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2631 - mae: 0.3996 - val_loss: 0.3658 - val_mae: 0.4639 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2642 - mae: 0.4030 - val_loss: 0.3709 - val_mae: 0.4608 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2642 - mae: 0.4030 - val_loss: 0.3709 - val_mae: 0.4608 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2375 - mae: 0.3823 - val_loss: 0.3578 - val_mae: 0.4580 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2375 - mae: 0.3823 - val_loss: 0.3578 - val_mae: 0.4580 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2407 - mae: 0.3809 - val_loss: 0.3621 - val_mae: 0.4607 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2407 - mae: 0.3809 - val_loss: 0.3621 - val_mae: 0.4607 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2459 - mae: 0.3886 - val_loss: 0.3625 - val_mae: 0.4539 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2459 - mae: 0.3886 - val_loss: 0.3625 - val_mae: 0.4539 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2264 - mae: 0.3733 - val_loss: 0.3739 - val_mae: 0.4739 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2264 - mae: 0.3733 - val_loss: 0.3739 - val_mae: 0.4739 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2229 - mae: 0.3709 - val_loss: 0.3614 - val_mae: 0.4647 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2229 - mae: 0.3709 - val_loss: 0.3614 - val_mae: 0.4647 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2248 - mae: 0.3701 - val_loss: 0.3560 - val_mae: 0.4578 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2248 - mae: 0.3701 - val_loss: 0.3560 - val_mae: 0.4578 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2128 - mae: 0.3583 - val_loss: 0.3547 - val_mae: 0.4611 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2128 - mae: 0.3583 - val_loss: 0.3547 - val_mae: 0.4611 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2079 - mae: 0.3583 - val_loss: 0.3576 - val_mae: 0.4610 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2079 - mae: 0.3583 - val_loss: 0.3576 - val_mae: 0.4610 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2164 - mae: 0.3661 - val_loss: 0.3710 - val_mae: 0.4628 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2164 - mae: 0.3661 - val_loss: 0.3710 - val_mae: 0.4628 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2161 - mae: 0.3656 - val_loss: 0.3711 - val_mae: 0.4605 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2161 - mae: 0.3656 - val_loss: 0.3711 - val_mae: 0.4605 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2093 - mae: 0.3570 - val_loss: 0.3784 - val_mae: 0.4664 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2093 - mae: 0.3570 - val_loss: 0.3784 - val_mae: 0.4664 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2140 - mae: 0.3607 - val_loss: 0.3521 - val_mae: 0.4557 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2140 - mae: 0.3607 - val_loss: 0.3521 - val_mae: 0.4557 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1986 - mae: 0.3476 - val_loss: 0.3610 - val_mae: 0.4634 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1986 - mae: 0.3476 - val_loss: 0.3610 - val_mae: 0.4634 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2003 - mae: 0.3503 - val_loss: 0.3741 - val_mae: 0.4740 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2003 - mae: 0.3503 - val_loss: 0.3741 - val_mae: 0.4740 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2040 - mae: 0.3534 - val_loss: 0.3802 - val_mae: 0.4746 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2040 - mae: 0.3534 - val_loss: 0.3802 - val_mae: 0.4746 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2015 - mae: 0.3505 - val_loss: 0.3636 - val_mae: 0.4662 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2015 - mae: 0.3505 - val_loss: 0.3636 - val_mae: 0.4662 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1966 - mae: 0.3471 - val_loss: 0.3986 - val_mae: 0.4826 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1966 - mae: 0.3471 - val_loss: 0.3986 - val_mae: 0.4826 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1941 - mae: 0.3471 - val_loss: 0.3712 - val_mae: 0.4651 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1941 - mae: 0.3471 - val_loss: 0.3712 - val_mae: 0.4651 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1955 - mae: 0.3465 - val_loss: 0.3722 - val_mae: 0.4722 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1955 - mae: 0.3465 - val_loss: 0.3722 - val_mae: 0.4722 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1805 - mae: 0.3313 - val_loss: 0.3823 - val_mae: 0.4746 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1805 - mae: 0.3313 - val_loss: 0.3823 - val_mae: 0.4746 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1906 - mae: 0.3383 - val_loss: 0.3824 - val_mae: 0.4745 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1906 - mae: 0.3383 - val_loss: 0.3824 - val_mae: 0.4745 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1831 - mae: 0.3364 - val_loss: 0.3877 - val_mae: 0.4735 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1831 - mae: 0.3364 - val_loss: 0.3877 - val_mae: 0.4735 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1833 - mae: 0.3320 - val_loss: 0.3828 - val_mae: 0.4747 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1833 - mae: 0.3320 - val_loss: 0.3828 - val_mae: 0.4747 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1779 - mae: 0.3323 - val_loss: 0.3809 - val_mae: 0.4732 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1779 - mae: 0.3323 - val_loss: 0.3809 - val_mae: 0.4732 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1778 - mae: 0.3288 - val_loss: 0.3918 - val_mae: 0.4805 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1778 - mae: 0.3288 - val_loss: 0.3918 - val_mae: 0.4805 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1794 - mae: 0.3294 - val_loss: 0.3869 - val_mae: 0.4760 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1794 - mae: 0.3294 - val_loss: 0.3869 - val_mae: 0.4760 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1764 - mae: 0.3251 - val_loss: 0.3815 - val_mae: 0.4740 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1764 - mae: 0.3251 - val_loss: 0.3815 - val_mae: 0.4740 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1837 - mae: 0.3362 - val_loss: 0.3748 - val_mae: 0.4730 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1837 - mae: 0.3362 - val_loss: 0.3748 - val_mae: 0.4730 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1743 - mae: 0.3243 - val_loss: 0.3837 - val_mae: 0.4753 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1743 - mae: 0.3243 - val_loss: 0.3837 - val_mae: 0.4753 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1710 - mae: 0.3240 - val_loss: 0.3825 - val_mae: 0.4735 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1710 - mae: 0.3240 - val_loss: 0.3825 - val_mae: 0.4735 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1787 - mae: 0.3289 - val_loss: 0.3853 - val_mae: 0.4765 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1787 - mae: 0.3289 - val_loss: 0.3853 - val_mae: 0.4765 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1605 - mae: 0.3181 - val_loss: 0.3845 - val_mae: 0.4775 - learning_rate: 2.5000e-04\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1605 - mae: 0.3181 - val_loss: 0.3845 - val_mae: 0.4775 - learning_rate: 2.5000e-04\n",
      "✅ LSTM trained successfully. Best val_loss: 0.3521\n",
      "\n",
      "=== Processing Horizon: 3 hours ===\n",
      "   📊 Creating enhanced target for 3h horizon...\n",
      "   ✅ Enhanced target variance: 337.9475\n",
      "✅ LSTM trained successfully. Best val_loss: 0.3521\n",
      "\n",
      "=== Processing Horizon: 3 hours ===\n",
      "   📊 Creating enhanced target for 3h horizon...\n",
      "   ✅ Enhanced target variance: 337.9475\n",
      "Training shapes: X_train (3159, 138), y_train (3159,)\n",
      "Target variance: 337.9475 (good if > 1.0)\n",
      "Target range: 46.24 to 135.98\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "Training shapes: X_train (3159, 138), y_train (3159,)\n",
      "Target variance: 337.9475 (good if > 1.0)\n",
      "Target range: 46.24 to 135.98\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "RF Best RMSE: 13.5417\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3159 -> 3159 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3159, 1, 138)\n",
      "   ✅ LSTM target shape: (3159,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "RF Best RMSE: 13.5417\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3159 -> 3159 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3159, 1, 138)\n",
      "   ✅ LSTM target shape: (3159,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 1.1022 - mae: 0.8343 - val_loss: 0.9697 - val_mae: 0.8318 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 1.1022 - mae: 0.8343 - val_loss: 0.9697 - val_mae: 0.8318 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7584 - mae: 0.6964 - val_loss: 0.9489 - val_mae: 0.8228 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7584 - mae: 0.6964 - val_loss: 0.9489 - val_mae: 0.8228 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6894 - mae: 0.6590 - val_loss: 0.9256 - val_mae: 0.8108 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6894 - mae: 0.6590 - val_loss: 0.9256 - val_mae: 0.8108 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6266 - mae: 0.6300 - val_loss: 0.8884 - val_mae: 0.7910 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6266 - mae: 0.6300 - val_loss: 0.8884 - val_mae: 0.7910 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5940 - mae: 0.6171 - val_loss: 0.8336 - val_mae: 0.7613 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5940 - mae: 0.6171 - val_loss: 0.8336 - val_mae: 0.7613 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5938 - mae: 0.6129 - val_loss: 0.7653 - val_mae: 0.7245 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5938 - mae: 0.6129 - val_loss: 0.7653 - val_mae: 0.7245 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5660 - mae: 0.6039 - val_loss: 0.6985 - val_mae: 0.6863 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5660 - mae: 0.6039 - val_loss: 0.6985 - val_mae: 0.6863 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5600 - mae: 0.6000 - val_loss: 0.6371 - val_mae: 0.6490 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5600 - mae: 0.6000 - val_loss: 0.6371 - val_mae: 0.6490 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5385 - mae: 0.5834 - val_loss: 0.6140 - val_mae: 0.6334 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5385 - mae: 0.5834 - val_loss: 0.6140 - val_mae: 0.6334 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5336 - mae: 0.5832 - val_loss: 0.5861 - val_mae: 0.6127 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5336 - mae: 0.5832 - val_loss: 0.5861 - val_mae: 0.6127 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5400 - mae: 0.5836 - val_loss: 0.5859 - val_mae: 0.6063 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5400 - mae: 0.5836 - val_loss: 0.5859 - val_mae: 0.6063 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5136 - mae: 0.5653 - val_loss: 0.5980 - val_mae: 0.6100 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5136 - mae: 0.5653 - val_loss: 0.5980 - val_mae: 0.6100 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4980 - mae: 0.5600 - val_loss: 0.5901 - val_mae: 0.6043 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4980 - mae: 0.5600 - val_loss: 0.5901 - val_mae: 0.6043 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5072 - mae: 0.5663 - val_loss: 0.5830 - val_mae: 0.5983 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5072 - mae: 0.5663 - val_loss: 0.5830 - val_mae: 0.5983 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4944 - mae: 0.5567 - val_loss: 0.5875 - val_mae: 0.5985 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4944 - mae: 0.5567 - val_loss: 0.5875 - val_mae: 0.5985 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4803 - mae: 0.5483 - val_loss: 0.5971 - val_mae: 0.6018 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4803 - mae: 0.5483 - val_loss: 0.5971 - val_mae: 0.6018 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4686 - mae: 0.5429 - val_loss: 0.5921 - val_mae: 0.5997 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4686 - mae: 0.5429 - val_loss: 0.5921 - val_mae: 0.5997 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4641 - mae: 0.5393 - val_loss: 0.6036 - val_mae: 0.6049 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4641 - mae: 0.5393 - val_loss: 0.6036 - val_mae: 0.6049 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4513 - mae: 0.5306 - val_loss: 0.5947 - val_mae: 0.6061 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4513 - mae: 0.5306 - val_loss: 0.5947 - val_mae: 0.6061 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4557 - mae: 0.5390 - val_loss: 0.5987 - val_mae: 0.6057 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4557 - mae: 0.5390 - val_loss: 0.5987 - val_mae: 0.6057 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4414 - mae: 0.5302 - val_loss: 0.6034 - val_mae: 0.6034 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4414 - mae: 0.5302 - val_loss: 0.6034 - val_mae: 0.6034 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4423 - mae: 0.5251 - val_loss: 0.6007 - val_mae: 0.6035 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4423 - mae: 0.5251 - val_loss: 0.6007 - val_mae: 0.6035 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4325 - mae: 0.5199 - val_loss: 0.6139 - val_mae: 0.6061 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4325 - mae: 0.5199 - val_loss: 0.6139 - val_mae: 0.6061 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4212 - mae: 0.5158 - val_loss: 0.6278 - val_mae: 0.6141 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4212 - mae: 0.5158 - val_loss: 0.6278 - val_mae: 0.6141 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4106 - mae: 0.5106 - val_loss: 0.6257 - val_mae: 0.6142 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4106 - mae: 0.5106 - val_loss: 0.6257 - val_mae: 0.6142 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4141 - mae: 0.5143 - val_loss: 0.6258 - val_mae: 0.6111 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4141 - mae: 0.5143 - val_loss: 0.6258 - val_mae: 0.6111 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3999 - mae: 0.5010 - val_loss: 0.6290 - val_mae: 0.6111 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3999 - mae: 0.5010 - val_loss: 0.6290 - val_mae: 0.6111 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3973 - mae: 0.5018 - val_loss: 0.6281 - val_mae: 0.6131 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3973 - mae: 0.5018 - val_loss: 0.6281 - val_mae: 0.6131 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3737 - mae: 0.4852 - val_loss: 0.6399 - val_mae: 0.6162 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3737 - mae: 0.4852 - val_loss: 0.6399 - val_mae: 0.6162 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3822 - mae: 0.4882 - val_loss: 0.6449 - val_mae: 0.6141 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3822 - mae: 0.4882 - val_loss: 0.6449 - val_mae: 0.6141 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3694 - mae: 0.4827 - val_loss: 0.6499 - val_mae: 0.6185 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3694 - mae: 0.4827 - val_loss: 0.6499 - val_mae: 0.6185 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3667 - mae: 0.4859 - val_loss: 0.6459 - val_mae: 0.6187 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3667 - mae: 0.4859 - val_loss: 0.6459 - val_mae: 0.6187 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3747 - mae: 0.4860 - val_loss: 0.6520 - val_mae: 0.6240 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3747 - mae: 0.4860 - val_loss: 0.6520 - val_mae: 0.6240 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3601 - mae: 0.4710 - val_loss: 0.6498 - val_mae: 0.6177 - learning_rate: 5.0000e-04\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3601 - mae: 0.4710 - val_loss: 0.6498 - val_mae: 0.6177 - learning_rate: 5.0000e-04\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5830\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5830\n",
      "\n",
      "=== Processing Horizon: 6 hours ===\n",
      "   📊 Creating enhanced target for 6h horizon...\n",
      "   ✅ Enhanced target variance: 337.1151\n",
      "Training shapes: X_train (3156, 138), y_train (3156,)\n",
      "Target variance: 337.1151 (good if > 1.0)\n",
      "Target range: 46.27 to 136.41\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "=== Processing Horizon: 6 hours ===\n",
      "   📊 Creating enhanced target for 6h horizon...\n",
      "   ✅ Enhanced target variance: 337.1151\n",
      "Training shapes: X_train (3156, 138), y_train (3156,)\n",
      "Target variance: 337.1151 (good if > 1.0)\n",
      "Target range: 46.27 to 136.41\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "RF Best RMSE: 13.5551\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3156 -> 3156 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3156, 1, 138)\n",
      "   ✅ LSTM target shape: (3156,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "RF Best RMSE: 13.5551\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3156 -> 3156 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3156, 1, 138)\n",
      "   ✅ LSTM target shape: (3156,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.2502 - mae: 0.8823 - val_loss: 0.9937 - val_mae: 0.8494 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.2502 - mae: 0.8823 - val_loss: 0.9937 - val_mae: 0.8494 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8765 - mae: 0.7443 - val_loss: 0.9942 - val_mae: 0.8495 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8765 - mae: 0.7443 - val_loss: 0.9942 - val_mae: 0.8495 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7626 - mae: 0.6960 - val_loss: 0.9777 - val_mae: 0.8422 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7626 - mae: 0.6960 - val_loss: 0.9777 - val_mae: 0.8422 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6943 - mae: 0.6679 - val_loss: 0.9300 - val_mae: 0.8198 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6943 - mae: 0.6679 - val_loss: 0.9300 - val_mae: 0.8198 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6783 - mae: 0.6527 - val_loss: 0.8634 - val_mae: 0.7883 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6783 - mae: 0.6527 - val_loss: 0.8634 - val_mae: 0.7883 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6514 - mae: 0.6408 - val_loss: 0.7831 - val_mae: 0.7488 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6514 - mae: 0.6408 - val_loss: 0.7831 - val_mae: 0.7488 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6343 - mae: 0.6371 - val_loss: 0.7260 - val_mae: 0.7195 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6343 - mae: 0.6371 - val_loss: 0.7260 - val_mae: 0.7195 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6222 - mae: 0.6321 - val_loss: 0.6662 - val_mae: 0.6856 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6222 - mae: 0.6321 - val_loss: 0.6662 - val_mae: 0.6856 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5861 - mae: 0.6083 - val_loss: 0.6209 - val_mae: 0.6609 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5861 - mae: 0.6083 - val_loss: 0.6209 - val_mae: 0.6609 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5882 - mae: 0.6140 - val_loss: 0.5989 - val_mae: 0.6453 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5882 - mae: 0.6140 - val_loss: 0.5989 - val_mae: 0.6453 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5673 - mae: 0.6001 - val_loss: 0.5816 - val_mae: 0.6316 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5673 - mae: 0.6001 - val_loss: 0.5816 - val_mae: 0.6316 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5397 - mae: 0.5826 - val_loss: 0.5815 - val_mae: 0.6284 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5397 - mae: 0.5826 - val_loss: 0.5815 - val_mae: 0.6284 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5538 - mae: 0.5891 - val_loss: 0.5682 - val_mae: 0.6168 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5538 - mae: 0.5891 - val_loss: 0.5682 - val_mae: 0.6168 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5504 - mae: 0.5883 - val_loss: 0.5636 - val_mae: 0.6105 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5504 - mae: 0.5883 - val_loss: 0.5636 - val_mae: 0.6105 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5486 - mae: 0.5886 - val_loss: 0.5691 - val_mae: 0.6109 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5486 - mae: 0.5886 - val_loss: 0.5691 - val_mae: 0.6109 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5326 - mae: 0.5775 - val_loss: 0.5648 - val_mae: 0.6034 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5326 - mae: 0.5775 - val_loss: 0.5648 - val_mae: 0.6034 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5235 - mae: 0.5783 - val_loss: 0.5555 - val_mae: 0.5949 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5235 - mae: 0.5783 - val_loss: 0.5555 - val_mae: 0.5949 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5150 - mae: 0.5633 - val_loss: 0.5548 - val_mae: 0.5961 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5150 - mae: 0.5633 - val_loss: 0.5548 - val_mae: 0.5961 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5202 - mae: 0.5773 - val_loss: 0.5624 - val_mae: 0.5981 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5202 - mae: 0.5773 - val_loss: 0.5624 - val_mae: 0.5981 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4989 - mae: 0.5591 - val_loss: 0.5709 - val_mae: 0.5991 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4989 - mae: 0.5591 - val_loss: 0.5709 - val_mae: 0.5991 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4704 - mae: 0.5423 - val_loss: 0.5678 - val_mae: 0.5954 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4704 - mae: 0.5423 - val_loss: 0.5678 - val_mae: 0.5954 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4901 - mae: 0.5527 - val_loss: 0.5678 - val_mae: 0.5906 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4901 - mae: 0.5527 - val_loss: 0.5678 - val_mae: 0.5906 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4717 - mae: 0.5483 - val_loss: 0.5656 - val_mae: 0.5915 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4717 - mae: 0.5483 - val_loss: 0.5656 - val_mae: 0.5915 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4536 - mae: 0.5345 - val_loss: 0.5613 - val_mae: 0.5860 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4536 - mae: 0.5345 - val_loss: 0.5613 - val_mae: 0.5860 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4626 - mae: 0.5389 - val_loss: 0.5718 - val_mae: 0.5941 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4626 - mae: 0.5389 - val_loss: 0.5718 - val_mae: 0.5941 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4475 - mae: 0.5288 - val_loss: 0.5723 - val_mae: 0.5955 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4475 - mae: 0.5288 - val_loss: 0.5723 - val_mae: 0.5955 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4568 - mae: 0.5315 - val_loss: 0.5835 - val_mae: 0.5955 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4568 - mae: 0.5315 - val_loss: 0.5835 - val_mae: 0.5955 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4257 - mae: 0.5189 - val_loss: 0.5783 - val_mae: 0.5944 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4257 - mae: 0.5189 - val_loss: 0.5783 - val_mae: 0.5944 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4239 - mae: 0.5108 - val_loss: 0.5814 - val_mae: 0.5978 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4239 - mae: 0.5108 - val_loss: 0.5814 - val_mae: 0.5978 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4276 - mae: 0.5160 - val_loss: 0.5876 - val_mae: 0.6019 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4276 - mae: 0.5160 - val_loss: 0.5876 - val_mae: 0.6019 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4116 - mae: 0.5068 - val_loss: 0.5926 - val_mae: 0.6010 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4116 - mae: 0.5068 - val_loss: 0.5926 - val_mae: 0.6010 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4166 - mae: 0.5100 - val_loss: 0.5949 - val_mae: 0.5999 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4166 - mae: 0.5100 - val_loss: 0.5949 - val_mae: 0.5999 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4108 - mae: 0.5060 - val_loss: 0.5946 - val_mae: 0.5972 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4108 - mae: 0.5060 - val_loss: 0.5946 - val_mae: 0.5972 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3980 - mae: 0.5014 - val_loss: 0.5949 - val_mae: 0.5983 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3980 - mae: 0.5014 - val_loss: 0.5949 - val_mae: 0.5983 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4038 - mae: 0.5075 - val_loss: 0.5928 - val_mae: 0.6009 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4038 - mae: 0.5075 - val_loss: 0.5928 - val_mae: 0.6009 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3821 - mae: 0.4929 - val_loss: 0.6143 - val_mae: 0.6125 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3821 - mae: 0.4929 - val_loss: 0.6143 - val_mae: 0.6125 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3852 - mae: 0.4873 - val_loss: 0.6097 - val_mae: 0.6107 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3852 - mae: 0.4873 - val_loss: 0.6097 - val_mae: 0.6107 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3923 - mae: 0.4930 - val_loss: 0.6030 - val_mae: 0.6038 - learning_rate: 5.0000e-04\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3923 - mae: 0.4930 - val_loss: 0.6030 - val_mae: 0.6038 - learning_rate: 5.0000e-04\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5548\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5548\n",
      "\n",
      "=== Processing Horizon: 12 hours ===\n",
      "   📊 Creating enhanced target for 12h horizon...\n",
      "   ✅ Enhanced target variance: 337.7935\n",
      "Training shapes: X_train (3150, 138), y_train (3150,)\n",
      "Target variance: 337.7935 (good if > 1.0)\n",
      "Target range: 46.34 to 136.78\n",
      "Verifying data types...\n",
      "\n",
      "=== Processing Horizon: 12 hours ===\n",
      "   📊 Creating enhanced target for 12h horizon...\n",
      "   ✅ Enhanced target variance: 337.7935\n",
      "Training shapes: X_train (3150, 138), y_train (3150,)\n",
      "Target variance: 337.7935 (good if > 1.0)\n",
      "Target range: 46.34 to 136.78\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "RF Best RMSE: 13.5271\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3150 -> 3150 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3150, 1, 138)\n",
      "   ✅ LSTM target shape: (3150,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "RF Best RMSE: 13.5271\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 25}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3150 -> 3150 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3150, 1, 138)\n",
      "   ✅ LSTM target shape: (3150,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.4066 - mae: 0.9488 - val_loss: 0.9683 - val_mae: 0.8323 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.4066 - mae: 0.9488 - val_loss: 0.9683 - val_mae: 0.8323 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8751 - mae: 0.7431 - val_loss: 0.9559 - val_mae: 0.8267 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8751 - mae: 0.7431 - val_loss: 0.9559 - val_mae: 0.8267 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7406 - mae: 0.6847 - val_loss: 0.9387 - val_mae: 0.8174 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7406 - mae: 0.6847 - val_loss: 0.9387 - val_mae: 0.8174 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6991 - mae: 0.6661 - val_loss: 0.8935 - val_mae: 0.7944 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6991 - mae: 0.6661 - val_loss: 0.8935 - val_mae: 0.7944 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6581 - mae: 0.6434 - val_loss: 0.8217 - val_mae: 0.7568 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6581 - mae: 0.6434 - val_loss: 0.8217 - val_mae: 0.7568 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6302 - mae: 0.6316 - val_loss: 0.7470 - val_mae: 0.7157 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6302 - mae: 0.6316 - val_loss: 0.7470 - val_mae: 0.7157 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5790 - mae: 0.6015 - val_loss: 0.6792 - val_mae: 0.6785 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5790 - mae: 0.6015 - val_loss: 0.6792 - val_mae: 0.6785 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6100 - mae: 0.6182 - val_loss: 0.6319 - val_mae: 0.6473 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6100 - mae: 0.6182 - val_loss: 0.6319 - val_mae: 0.6473 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6041 - mae: 0.6154 - val_loss: 0.5948 - val_mae: 0.6268 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6041 - mae: 0.6154 - val_loss: 0.5948 - val_mae: 0.6268 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5663 - mae: 0.5966 - val_loss: 0.5756 - val_mae: 0.6211 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5663 - mae: 0.5966 - val_loss: 0.5756 - val_mae: 0.6211 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5696 - mae: 0.5981 - val_loss: 0.5753 - val_mae: 0.6222 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5696 - mae: 0.5981 - val_loss: 0.5753 - val_mae: 0.6222 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5440 - mae: 0.5837 - val_loss: 0.5703 - val_mae: 0.6111 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5440 - mae: 0.5837 - val_loss: 0.5703 - val_mae: 0.6111 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5529 - mae: 0.5929 - val_loss: 0.5775 - val_mae: 0.6059 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5529 - mae: 0.5929 - val_loss: 0.5775 - val_mae: 0.6059 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5283 - mae: 0.5736 - val_loss: 0.5762 - val_mae: 0.6059 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5283 - mae: 0.5736 - val_loss: 0.5762 - val_mae: 0.6059 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5314 - mae: 0.5800 - val_loss: 0.5809 - val_mae: 0.6074 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5314 - mae: 0.5800 - val_loss: 0.5809 - val_mae: 0.6074 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5148 - mae: 0.5701 - val_loss: 0.5860 - val_mae: 0.6111 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5148 - mae: 0.5701 - val_loss: 0.5860 - val_mae: 0.6111 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5199 - mae: 0.5716 - val_loss: 0.5831 - val_mae: 0.6090 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5199 - mae: 0.5716 - val_loss: 0.5831 - val_mae: 0.6090 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5150 - mae: 0.5679 - val_loss: 0.5859 - val_mae: 0.6103 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5150 - mae: 0.5679 - val_loss: 0.5859 - val_mae: 0.6103 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4941 - mae: 0.5597 - val_loss: 0.6017 - val_mae: 0.6086 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4941 - mae: 0.5597 - val_loss: 0.6017 - val_mae: 0.6086 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5036 - mae: 0.5644 - val_loss: 0.6127 - val_mae: 0.6205 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5036 - mae: 0.5644 - val_loss: 0.6127 - val_mae: 0.6205 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4960 - mae: 0.5598 - val_loss: 0.6033 - val_mae: 0.6199 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4960 - mae: 0.5598 - val_loss: 0.6033 - val_mae: 0.6199 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4784 - mae: 0.5491 - val_loss: 0.6061 - val_mae: 0.6176 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4784 - mae: 0.5491 - val_loss: 0.6061 - val_mae: 0.6176 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4672 - mae: 0.5417 - val_loss: 0.6150 - val_mae: 0.6186 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4672 - mae: 0.5417 - val_loss: 0.6150 - val_mae: 0.6186 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4691 - mae: 0.5426 - val_loss: 0.6106 - val_mae: 0.6224 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4691 - mae: 0.5426 - val_loss: 0.6106 - val_mae: 0.6224 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4733 - mae: 0.5458 - val_loss: 0.6184 - val_mae: 0.6231 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4733 - mae: 0.5458 - val_loss: 0.6184 - val_mae: 0.6231 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4659 - mae: 0.5413 - val_loss: 0.6175 - val_mae: 0.6253 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4659 - mae: 0.5413 - val_loss: 0.6175 - val_mae: 0.6253 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4655 - mae: 0.5422 - val_loss: 0.6203 - val_mae: 0.6221 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4655 - mae: 0.5422 - val_loss: 0.6203 - val_mae: 0.6221 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4536 - mae: 0.5348 - val_loss: 0.6278 - val_mae: 0.6280 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4536 - mae: 0.5348 - val_loss: 0.6278 - val_mae: 0.6280 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4528 - mae: 0.5335 - val_loss: 0.6472 - val_mae: 0.6298 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4528 - mae: 0.5335 - val_loss: 0.6472 - val_mae: 0.6298 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4526 - mae: 0.5332 - val_loss: 0.6406 - val_mae: 0.6300 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4526 - mae: 0.5332 - val_loss: 0.6406 - val_mae: 0.6300 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4364 - mae: 0.5227 - val_loss: 0.6389 - val_mae: 0.6335 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4364 - mae: 0.5227 - val_loss: 0.6389 - val_mae: 0.6335 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4292 - mae: 0.5153 - val_loss: 0.6375 - val_mae: 0.6314 - learning_rate: 5.0000e-04\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4292 - mae: 0.5153 - val_loss: 0.6375 - val_mae: 0.6314 - learning_rate: 5.0000e-04\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5703\n",
      "\n",
      "=== Processing Horizon: 24 hours ===\n",
      "   📊 Creating enhanced target for 24h horizon...\n",
      "   ✅ Enhanced target variance: 337.4103\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5703\n",
      "\n",
      "=== Processing Horizon: 24 hours ===\n",
      "   📊 Creating enhanced target for 24h horizon...\n",
      "   ✅ Enhanced target variance: 337.4103\n",
      "Training shapes: X_train (3138, 138), y_train (3138,)\n",
      "Target variance: 337.4103 (good if > 1.0)\n",
      "Target range: 46.46 to 137.34\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "Training shapes: X_train (3138, 138), y_train (3138,)\n",
      "Target variance: 337.4103 (good if > 1.0)\n",
      "Target range: 46.46 to 137.34\n",
      "Verifying data types...\n",
      "✅ Saved 138 feature names for evaluation consistency\n",
      "\n",
      "Training Random Forest...\n",
      "RF Best RMSE: 13.5795\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3138 -> 3138 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3138, 1, 138)\n",
      "   ✅ LSTM target shape: (3138,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "RF Best RMSE: 13.5795\n",
      "RF Best params: {'n_estimators': 300, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
      "\n",
      "Training LSTM...\n",
      "   Performing comprehensive data type validation...\n",
      "   X_train dtypes before conversion:\n",
      "   float32    138\n",
      "Name: count, dtype: int64\n",
      "   Cleaned data: 3138 -> 3138 rows\n",
      "   All dtypes after cleaning: float32    138\n",
      "Name: count, dtype: int64\n",
      "   ✅ LSTM input shape: (3138, 1, 138)\n",
      "   ✅ LSTM target shape: (3138,)\n",
      "   ✅ Data types: X=float32, y=float32\n",
      "   ✅ No inf/nan in X: True\n",
      "   ✅ No inf/nan in y: True\n",
      "   ✅ Model compiled successfully\n",
      "   🚀 Starting LSTM training...\n",
      "Epoch 1/200\n",
      "Epoch 1/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.3893 - mae: 0.9069 - val_loss: 0.9734 - val_mae: 0.8403 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.3893 - mae: 0.9069 - val_loss: 0.9734 - val_mae: 0.8403 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7116 - mae: 0.6707 - val_loss: 0.9455 - val_mae: 0.8272 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7116 - mae: 0.6707 - val_loss: 0.9455 - val_mae: 0.8272 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6596 - mae: 0.6459 - val_loss: 0.9142 - val_mae: 0.8123 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6596 - mae: 0.6459 - val_loss: 0.9142 - val_mae: 0.8123 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6385 - mae: 0.6392 - val_loss: 0.8689 - val_mae: 0.7906 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6385 - mae: 0.6392 - val_loss: 0.8689 - val_mae: 0.7906 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5998 - mae: 0.6114 - val_loss: 0.8061 - val_mae: 0.7613 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5998 - mae: 0.6114 - val_loss: 0.8061 - val_mae: 0.7613 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5837 - mae: 0.6091 - val_loss: 0.7385 - val_mae: 0.7289 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5837 - mae: 0.6091 - val_loss: 0.7385 - val_mae: 0.7289 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5827 - mae: 0.6063 - val_loss: 0.6979 - val_mae: 0.7076 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5827 - mae: 0.6063 - val_loss: 0.6979 - val_mae: 0.7076 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5567 - mae: 0.5936 - val_loss: 0.6600 - val_mae: 0.6844 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5567 - mae: 0.5936 - val_loss: 0.6600 - val_mae: 0.6844 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5581 - mae: 0.5951 - val_loss: 0.6509 - val_mae: 0.6741 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5581 - mae: 0.5951 - val_loss: 0.6509 - val_mae: 0.6741 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5540 - mae: 0.5902 - val_loss: 0.6285 - val_mae: 0.6549 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5540 - mae: 0.5902 - val_loss: 0.6285 - val_mae: 0.6549 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5367 - mae: 0.5837 - val_loss: 0.6250 - val_mae: 0.6482 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5367 - mae: 0.5837 - val_loss: 0.6250 - val_mae: 0.6482 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5220 - mae: 0.5771 - val_loss: 0.5981 - val_mae: 0.6335 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5220 - mae: 0.5771 - val_loss: 0.5981 - val_mae: 0.6335 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5269 - mae: 0.5745 - val_loss: 0.6331 - val_mae: 0.6458 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5269 - mae: 0.5745 - val_loss: 0.6331 - val_mae: 0.6458 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5137 - mae: 0.5744 - val_loss: 0.6132 - val_mae: 0.6326 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5137 - mae: 0.5744 - val_loss: 0.6132 - val_mae: 0.6326 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5001 - mae: 0.5594 - val_loss: 0.6646 - val_mae: 0.6521 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5001 - mae: 0.5594 - val_loss: 0.6646 - val_mae: 0.6521 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5121 - mae: 0.5710 - val_loss: 0.6374 - val_mae: 0.6377 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5121 - mae: 0.5710 - val_loss: 0.6374 - val_mae: 0.6377 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4829 - mae: 0.5534 - val_loss: 0.6390 - val_mae: 0.6338 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4829 - mae: 0.5534 - val_loss: 0.6390 - val_mae: 0.6338 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4900 - mae: 0.5545 - val_loss: 0.6494 - val_mae: 0.6451 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4900 - mae: 0.5545 - val_loss: 0.6494 - val_mae: 0.6451 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4649 - mae: 0.5388 - val_loss: 0.6129 - val_mae: 0.6264 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4649 - mae: 0.5388 - val_loss: 0.6129 - val_mae: 0.6264 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4677 - mae: 0.5403 - val_loss: 0.6299 - val_mae: 0.6321 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4677 - mae: 0.5403 - val_loss: 0.6299 - val_mae: 0.6321 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4626 - mae: 0.5376 - val_loss: 0.6646 - val_mae: 0.6445 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4626 - mae: 0.5376 - val_loss: 0.6646 - val_mae: 0.6445 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4471 - mae: 0.5284 - val_loss: 0.6490 - val_mae: 0.6390 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4471 - mae: 0.5284 - val_loss: 0.6490 - val_mae: 0.6390 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4424 - mae: 0.5266 - val_loss: 0.6590 - val_mae: 0.6437 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4424 - mae: 0.5266 - val_loss: 0.6590 - val_mae: 0.6437 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4269 - mae: 0.5163 - val_loss: 0.6791 - val_mae: 0.6514 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4269 - mae: 0.5163 - val_loss: 0.6791 - val_mae: 0.6514 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4221 - mae: 0.5158 - val_loss: 0.6597 - val_mae: 0.6419 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4221 - mae: 0.5158 - val_loss: 0.6597 - val_mae: 0.6419 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4068 - mae: 0.5052 - val_loss: 0.6789 - val_mae: 0.6512 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4068 - mae: 0.5052 - val_loss: 0.6789 - val_mae: 0.6512 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4215 - mae: 0.5115 - val_loss: 0.6705 - val_mae: 0.6453 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4215 - mae: 0.5115 - val_loss: 0.6705 - val_mae: 0.6453 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3998 - mae: 0.5006 - val_loss: 0.6785 - val_mae: 0.6515 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3998 - mae: 0.5006 - val_loss: 0.6785 - val_mae: 0.6515 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3991 - mae: 0.4960 - val_loss: 0.7001 - val_mae: 0.6617 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3991 - mae: 0.4960 - val_loss: 0.7001 - val_mae: 0.6617 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3938 - mae: 0.4997 - val_loss: 0.6805 - val_mae: 0.6503 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3938 - mae: 0.4997 - val_loss: 0.6805 - val_mae: 0.6503 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3954 - mae: 0.4990 - val_loss: 0.6820 - val_mae: 0.6494 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3954 - mae: 0.4990 - val_loss: 0.6820 - val_mae: 0.6494 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4021 - mae: 0.5034 - val_loss: 0.7127 - val_mae: 0.6652 - learning_rate: 5.0000e-04\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4021 - mae: 0.5034 - val_loss: 0.7127 - val_mae: 0.6652 - learning_rate: 5.0000e-04\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5981\n",
      "✅ LSTM trained successfully. Best val_loss: 0.5981\n"
     ]
    }
   ],
   "source": [
    "# Model training loop with enhanced targets\n",
    "horizons = [1, 3, 6, 12, 24]\n",
    "print(f\"\\n--- Training Models for Horizons: {horizons} ---\")\n",
    "print(\"🎯 Creating enhanced targets to prevent straight-line predictions...\")\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n=== Processing Horizon: {h} hours ===\")\n",
    "    \n",
    "    # Create enhanced targets with temporal variation\n",
    "    print(f\"   📊 Creating enhanced target for {h}h horizon...\")\n",
    "    train_df['target_h'] = train_df['pm25_value'].shift(-h)\n",
    "    test_df['target_h'] = test_df['pm25_value'].shift(-h)\n",
    "    \n",
    "    # CRITICAL: Add controlled variation to prevent target smoothing\n",
    "    if len(train_df) > 100:\n",
    "        # Add temporal dynamics based on historical variance\n",
    "        target_std = train_df['target_h'].rolling(window=min(48, len(train_df)//4), min_periods=1).std()\n",
    "        enhancement_factor = 0.03  # Small but important variation\n",
    "        \n",
    "        # Apply controlled noise to training targets only\n",
    "        valid_train_mask = ~train_df['target_h'].isna()\n",
    "        if valid_train_mask.sum() > 0:\n",
    "            noise = np.random.normal(0, target_std * enhancement_factor, len(train_df))\n",
    "            train_df.loc[valid_train_mask, 'target_h'] += noise[valid_train_mask]\n",
    "            \n",
    "            print(f\"   ✅ Enhanced target variance: {train_df['target_h'].var():.4f}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  Insufficient data for target enhancement\")\n",
    "    \n",
    "    # Drop NaN targets\n",
    "    train_h = train_df.dropna(subset=['target_h'])\n",
    "    test_h = test_df.dropna(subset=['target_h'])\n",
    "    \n",
    "    X_train = train_h[features_for_scaling]\n",
    "    y_train = train_h['target_h']\n",
    "    X_test = test_h[features_for_scaling]\n",
    "    y_test = test_h['target_h']\n",
    "    \n",
    "    print(f\"Training shapes: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "    print(f\"Target variance: {y_train.var():.4f} (good if > 1.0)\")\n",
    "    print(f\"Target range: {y_train.min():.2f} to {y_train.max():.2f}\")\n",
    "    \n",
    "    # CRITICAL: Verify data types before training\n",
    "    print(\"Verifying data types...\")\n",
    "    non_numeric_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric_features:\n",
    "        print(f\"⚠️  Converting non-numeric features: {non_numeric_features}\")\n",
    "        for col in non_numeric_features:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "        # Drop any rows with NaN after conversion\n",
    "        X_train.dropna(inplace=True)\n",
    "        y_train = y_train.loc[X_train.index]\n",
    "        X_test.dropna(inplace=True)\n",
    "        y_test = y_test.loc[X_test.index]\n",
    "        print(f\"✅ After cleanup: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "    \n",
    "    # Ensure all data is float32 for TensorFlow\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    # Target scaling for LSTM\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten().astype(np.float32)\n",
    "    joblib.dump(scaler_y, f'{output_dir}/scaler_y_h{h}.pkl')\n",
    "    \n",
    "    # Save feature names for evaluation consistency\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    joblib.dump(feature_names, f'{output_dir}/feature_names_h{h}.pkl')\n",
    "    print(f\"✅ Saved {len(feature_names)} feature names for evaluation consistency\")\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Random Forest\n",
    "    print(f\"\\nTraining Random Forest...\")\n",
    "    rf_params = {\n",
    "        'n_estimators': [150, 200, 300],\n",
    "        'max_depth': [15, 20, 25, None],\n",
    "        'min_samples_leaf': [1, 2, 3],\n",
    "        'max_features': ['sqrt', 'log2', 0.8]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf, rf_params, cv=tscv, scoring='neg_mean_squared_error', \n",
    "        n_iter=12, verbose=0, random_state=42\n",
    "    )\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    \n",
    "    joblib.dump(rf_search.best_estimator_, f'{output_dir}/rf_model_h{h}.pkl')\n",
    "    print(f\"RF Best RMSE: {np.sqrt(-rf_search.best_score_):.4f}\")\n",
    "    print(f\"RF Best params: {rf_search.best_params_}\")\n",
    "    \n",
    "    # LSTM with improved architecture and error handling\n",
    "    print(f\"\\nTraining LSTM...\")\n",
    "    try:\n",
    "        # COMPREHENSIVE DATA TYPE FIXING FOR LSTM\n",
    "        print(\"   Performing comprehensive data type validation...\")\n",
    "        \n",
    "        # Check for any remaining non-numeric columns\n",
    "        print(f\"   X_train dtypes before conversion:\")\n",
    "        dtype_info = X_train.dtypes.value_counts()\n",
    "        print(f\"   {dtype_info}\")\n",
    "        \n",
    "        # Force conversion of all columns to numeric\n",
    "        X_train_clean = X_train.copy()\n",
    "        for col in X_train_clean.columns:\n",
    "            if X_train_clean[col].dtype == 'object' or X_train_clean[col].dtype == 'bool':\n",
    "                print(f\"   Converting {col} from {X_train_clean[col].dtype} to numeric\")\n",
    "                X_train_clean[col] = pd.to_numeric(X_train_clean[col], errors='coerce')\n",
    "        \n",
    "        # Remove any rows with NaN after conversion\n",
    "        initial_rows = len(X_train_clean)\n",
    "        X_train_clean.dropna(inplace=True)\n",
    "        y_train_clean = y_train.loc[X_train_clean.index]\n",
    "        y_train_scaled_clean = scaler_y.transform(y_train_clean.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        print(f\"   Cleaned data: {initial_rows} -> {len(X_train_clean)} rows\")\n",
    "        print(f\"   All dtypes after cleaning: {X_train_clean.dtypes.value_counts()}\")\n",
    "        \n",
    "        # Ensure everything is float32\n",
    "        X_train_clean = X_train_clean.astype(np.float32)\n",
    "        y_train_scaled_clean = y_train_scaled_clean.astype(np.float32)\n",
    "        \n",
    "        # Check for any infinite or extremely large values\n",
    "        if np.any(np.isinf(X_train_clean.values)) or np.any(np.isnan(X_train_clean.values)):\n",
    "            print(\"   ⚠️  Found inf/nan values, cleaning...\")\n",
    "            X_train_clean = X_train_clean.replace([np.inf, -np.inf], np.nan)\n",
    "            X_train_clean.fillna(X_train_clean.mean(), inplace=True)\n",
    "        \n",
    "        # Create LSTM input with verified clean data\n",
    "        X_train_lstm = X_train_clean.values.reshape(X_train_clean.shape[0], 1, X_train_clean.shape[1])\n",
    "        \n",
    "        print(f\"   ✅ LSTM input shape: {X_train_lstm.shape}\")\n",
    "        print(f\"   ✅ LSTM target shape: {y_train_scaled_clean.shape}\")\n",
    "        print(f\"   ✅ Data types: X={X_train_lstm.dtype}, y={y_train_scaled_clean.dtype}\")\n",
    "        print(f\"   ✅ No inf/nan in X: {not np.any(np.isinf(X_train_lstm)) and not np.any(np.isnan(X_train_lstm))}\")\n",
    "        print(f\"   ✅ No inf/nan in y: {not np.any(np.isinf(y_train_scaled_clean)) and not np.any(np.isnan(y_train_scaled_clean))}\")\n",
    "        \n",
    "        # Clear any previous models\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model_lstm = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=(1, X_train_clean.shape[1])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            BatchNormalization(), \n",
    "            Dropout(0.3),\n",
    "            LSTM(32),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model_lstm.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        print(f\"   ✅ Model compiled successfully\")\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        print(f\"   🚀 Starting LSTM training...\")\n",
    "        history = model_lstm.fit(\n",
    "            X_train_lstm, y_train_scaled_clean,\n",
    "            epochs=200,\n",
    "            batch_size=64,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        model_lstm.save(f'{output_dir}/lstm_model_h{h}.keras')\n",
    "        print(f\"✅ LSTM trained successfully. Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "        \n",
    "        # Clean up for next iteration\n",
    "        del X_train_lstm, model_lstm, history\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LSTM training failed for horizon {h}h: {e}\")\n",
    "        print(f\"Skipping LSTM for this horizon and continuing...\")\n",
    "    \n",
    "    # Clean up target column for next iteration\n",
    "    train_df.drop('target_h', axis=1, inplace=True, errors='ignore')\n",
    "    test_df.drop('target_h', axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AI Modelling Complete ===\n",
      "✅ Enhanced models with comprehensive temporal features trained\n",
      "✅ This SHOULD resolve the straight-line prediction issue\n",
      "✅ Models now have rich temporal context and variability\n",
      "\n",
      "Next: Run Notebook 4 for evaluation - expect realistic PM2.5 predictions!\n"
     ]
    }
   ],
   "source": [
    "# Save featured data\n",
    "train_df.to_csv(f'{output_dir}/train_featured_data.csv')\n",
    "test_df.to_csv(f'{output_dir}/test_featured_data.csv')\n",
    "\n",
    "print(\"\\n=== AI Modelling Complete ===\")\n",
    "print(\"✅ Enhanced models with comprehensive temporal features trained\")\n",
    "print(\"✅ This SHOULD resolve the straight-line prediction issue\")\n",
    "print(\"✅ Models now have rich temporal context and variability\")\n",
    "print(\"\\nNext: Run Notebook 4 for evaluation - expect realistic PM2.5 predictions!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
