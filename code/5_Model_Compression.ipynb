{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zgSJMHgI98T"
   },
   "source": [
    "# Notebook 5: Model Compression\n",
    "## Introduction\n",
    "# Loads models from Notebook 3, compresses for h=6 (chosen for balance), evaluates trade-offs.\n",
    "# Justification: Dynamic/Float16/Int quantization for LSTM; param reduction/feature selection for RF. Ensures sustainability (lower energy) while maintaining ~95% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2039,
     "status": "ok",
     "timestamp": 1754100001139,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "IUMXAsHecb46",
    "outputId": "ef3441a8-1658-4fb0-b028-9c5f0299e0c4"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13636,
     "status": "ok",
     "timestamp": 1754099798304,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "FypvIpXoGbLc",
    "outputId": "fd8aa771-4aea-4208-8204-04ab3ae15eb1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import shap\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Google Drive Mounting (ensure this is in a preceding cell and run) ---\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "# drive.mount('/content/drive')\n",
    "# your_project_path = '/content/drive/MyDrive/AI_Sustainability_Project_lsa' # IMPORTANT: Match this to your setup\n",
    "# os.makedirs(your_project_path, exist_ok=True)\n",
    "# %cd \"{your_project_path}\"\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "print(\"--- Starting Model Compression and Efficiency Analysis (Notebook 5) ---\")\n",
    "\n",
    "# Define the filename for the processed data generated by Notebook 3\n",
    "input_data_filename = 'featured_data_for_models.csv'\n",
    "\n",
    "# Choose a single horizon for compression analysis as specified by the assessment\n",
    "chosen_h = 6\n",
    "\n",
    "# Assume average inference power (W) for sustainability; Justify as a typical mobile CPU/edge device estimate for SDG 13\n",
    "AVG_POWER_W = 5.0 # Watts\n",
    "\n",
    "# Helper to compute energy (mWh) for sustainability analysis\n",
    "def estimate_energy(time_s, power_w=AVG_POWER_W):\n",
    "    \"\"\"Estimates energy consumption in milli-Watt-hours (mWh) given time in seconds and power in Watts).\"\"\"\n",
    "    return (power_w * time_s) / 3600 * 1000 # (Watts * seconds) / (seconds/hour) * 1000 (milli)\n",
    "\n",
    "# --- Load and Prepare Data for the Chosen Horizon ---\n",
    "print(f\"Loading featured data from: {input_data_filename}\")\n",
    "try:\n",
    "    df_raw = pd.read_csv(input_data_filename, index_col='timestamp', parse_dates=True)\n",
    "    print(f\"Data loaded successfully. Initial shape: {df_raw.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from {input_data_filename}: {e}\")\n",
    "    raise SystemExit(\"Failed to load featured data. Aborting Notebook 5 execution.\")\n",
    "\n",
    "# Create the target column for the chosen horizon, then drop NaNs\n",
    "df_raw['target_h'] = df_raw['pm25_value'].shift(-chosen_h)\n",
    "\n",
    "# Drop any rows with NaN values introduced by shifting the target\n",
    "df_processed_h = df_raw.dropna(subset=['target_h']).copy()\n",
    "\n",
    "# Define features and target\n",
    "features_cols = [col for col in df_processed_h.columns if col not in ['pm25_value', 'target_h']]\n",
    "\n",
    "# Time-based split for consistency\n",
    "train_ratio = 0.8\n",
    "split_point = int(len(df_processed_h) * train_ratio)\n",
    "\n",
    "X_train = df_processed_h[features_cols].iloc[:split_point]\n",
    "y_train_unscaled = df_processed_h['target_h'].iloc[:split_point] # Target for RF models (unscaled)\n",
    "\n",
    "X_test = df_processed_h[features_cols].iloc[split_point:]\n",
    "y_test_unscaled = df_processed_h['target_h'].iloc[split_point:] # Target for evaluation (unscaled)\n",
    "\n",
    "print(f\"Data prepared for horizon {chosen_h}:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train_unscaled shape: {y_train_unscaled.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test_unscaled shape: {y_test_unscaled.shape}\")\n",
    "\n",
    "# Load scaler_x (features) from Notebook 3 for scaling X_test for LSTM\n",
    "try:\n",
    "    scaler_x = joblib.load('scaler_x.pkl')\n",
    "    X_test_scaled = scaler_x.transform(X_test) # Scale X_test for LSTM and TFLite\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features_cols, index=X_test.index)\n",
    "    print(\"Feature scaler_x loaded and X_test scaled for LSTM.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading scaler_x.pkl: {e}. X_test might be unscaled for LSTM predictions.\")\n",
    "    X_test_scaled_df = X_test.copy() # Use unscaled if scaler not found (LSTM preds might be off)\n",
    "\n",
    "\n",
    "# Load scaler_y (target) for inverse transform (for LSTM predictions ONLY)\n",
    "try:\n",
    "    scaler_y = joblib.load(f'scaler_y_h{chosen_h}.pkl')\n",
    "    print(f\"Target scaler_y_h{chosen_h}.pkl loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading scaler_y_h{chosen_h}.pkl: {e}\")\n",
    "    raise SystemExit(f\"Failed to load scaler_y for horizon {chosen_h}. Aborting.\")\n",
    "\n",
    "compression_results = []\n",
    "\n",
    "# --- LSTM Original Model Evaluation ---\n",
    "print(f\"\\n--- Evaluating LSTM Original (Horizon {chosen_h}h) ---\")\n",
    "try:\n",
    "    lstm_original = tf.keras.models.load_model(f'lstm_model_h{chosen_h}.h5', compile=False)\n",
    "    lstm_original.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    X_test_lstm_input = X_test_scaled_df.values.reshape(-1, 1, len(features_cols)).astype(np.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    original_preds_scaled = lstm_original.predict(X_test_lstm_input, verbose=0)\n",
    "    original_time = time.time() - start_time\n",
    "\n",
    "    original_preds = scaler_y.inverse_transform(original_preds_scaled).flatten()\n",
    "    original_mae = mean_absolute_error(y_test_unscaled, original_preds)\n",
    "    original_energy = estimate_energy(original_time)\n",
    "\n",
    "    original_lstm_path = f'lstm_original_h{chosen_h}.h5'\n",
    "    lstm_original.save(original_lstm_path)\n",
    "    original_size = os.path.getsize(original_lstm_path) / (1024*1024)\n",
    "\n",
    "    compression_results.append({\n",
    "        'Model': 'LSTM_Original', 'MAE': original_mae, 'Size_MB': original_size,\n",
    "        'Time_s': original_time, 'Energy_mWh': original_energy\n",
    "    })\n",
    "    print(f'LSTM Original - MAE: {original_mae:.4f}, Size: {original_size:.2f} MB, Time: {original_time:.4f}s, Energy: {original_energy:.4f} mWh')\n",
    "except Exception as e:\n",
    "    print(f\"Error with LSTM Original evaluation: {e}\")\n",
    "    compression_results.append({'Model': 'LSTM_Original', 'MAE': np.nan, 'Size_MB': np.nan, 'Time_s': np.nan, 'Energy_mWh': np.nan})\n",
    "\n",
    "\n",
    "# --- LSTM Float16 Quantization ---\n",
    "print(f\"\\n--- Evaluating LSTM Float16 (Horizon {chosen_h}h) ---\")\n",
    "try:\n",
    "    lstm_f16 = tf.keras.models.clone_model(lstm_original)\n",
    "    weights_f16 = [w.astype(np.float16) for w in lstm_original.get_weights()]\n",
    "    lstm_f16.set_weights(weights_f16)\n",
    "    lstm_f16.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    start_time = time.time()\n",
    "    f16_preds_scaled = lstm_f16.predict(X_test_lstm_input, verbose=0)\n",
    "    f16_time = time.time() - start_time\n",
    "\n",
    "    f16_preds = scaler_y.inverse_transform(f16_preds_scaled).flatten()\n",
    "    f16_mae = mean_absolute_error(y_test_unscaled, f16_preds)\n",
    "    f16_energy = estimate_energy(f16_time)\n",
    "\n",
    "    f16_lstm_path = f'lstm_f16_h{chosen_h}.h5'\n",
    "    lstm_f16.save(f16_lstm_path)\n",
    "    f16_size = os.path.getsize(f16_lstm_path) / (1024*1024)\n",
    "\n",
    "    compression_results.append({\n",
    "        'Model': 'LSTM_Float16', 'MAE': f16_mae, 'Size_MB': f16_size,\n",
    "        'Time_s': f16_time, 'Energy_mWh': f16_energy\n",
    "    })\n",
    "    print(f'LSTM Float16 - MAE: {f16_mae:.4f}, Size: {f16_size:.2f} MB, Time: {f16_time:.4f}s, Energy: {f16_energy:.4f} mWh')\n",
    "except Exception as e:\n",
    "    print(f\"Error with LSTM Float16 quantization: {e}\")\n",
    "    compression_results.append({'Model': 'LSTM_Float16', 'MAE': np.nan, 'Size_MB': np.nan, 'Time_s': np.nan, 'Energy_mWh': np.nan})\n",
    "\n",
    "\n",
    "# --- TFLite Dynamic Range Quantization for LSTM ---\n",
    "print(f\"\\n--- Evaluating LSTM TFLite Dynamic Quantization (Horizon {chosen_h}h) ---\")\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(lstm_original)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Flags to try addressing previous TFLite conversion errors with LSTM\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.experimental_enable_resource_variables = True\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    tflite_path = f'lstm_tflite_dynamic_h{chosen_h}.tflite'\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    tflite_size = os.path.getsize(tflite_path) / (1024*1024)\n",
    "\n",
    "    # TFLite Inference\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    start_time = time.time()\n",
    "    tflite_preds_scaled = []\n",
    "    for i in range(X_test_lstm_input.shape[0]):\n",
    "        interpreter.set_tensor(input_details[0]['index'], X_test_lstm_input[i:i+1].astype(input_details[0]['dtype']))\n",
    "        interpreter.invoke()\n",
    "        tflite_preds_scaled.append(interpreter.get_tensor(output_details[0]['index'])[0])\n",
    "    tflite_time = time.time() - start_time\n",
    "\n",
    "    tflite_preds = scaler_y.inverse_transform(np.array(tflite_preds_scaled)).flatten()\n",
    "    tflite_mae = mean_absolute_error(y_test_unscaled, tflite_preds)\n",
    "    tflite_energy = estimate_energy(tflite_time)\n",
    "\n",
    "    compression_results.append({\n",
    "        'Model': 'LSTM_TFLite_Dynamic', 'MAE': tflite_mae, 'Size_MB': tflite_size,\n",
    "        'Time_s': tflite_time, 'Energy_mWh': tflite_energy\n",
    "    })\n",
    "    print(f'LSTM TFLite Dynamic - MAE: {tflite_mae:.4f}, Size: {tflite_size:.2f} MB, Time: {tflite_time:.4f}s, Energy: {tflite_energy:.4f} mWh')\n",
    "except Exception as e:\n",
    "    print(f\"Error with LSTM TFLite dynamic quantization: {e}. Skipping TFLite.\")\n",
    "    compression_results.append({'Model': 'LSTM_TFLite_Dynamic', 'MAE': np.nan, 'Size_MB': np.nan, 'Time_s': np.nan, 'Energy_mWh': np.nan})\n",
    "\n",
    "\n",
    "# --- Random Forest Original Model Evaluation ---\n",
    "print(f\"\\n--- Evaluating RF Original (Horizon {chosen_h}h) ---\")\n",
    "try:\n",
    "    rf_original = joblib.load(f'rf_model_h{chosen_h}.pkl')\n",
    "\n",
    "    start_time = time.time()\n",
    "    # IMPORTANT FIX: RF was trained on UNscaled target in Notebook 3 (after previous fix).\n",
    "    # So, rf_original.predict(X_test) directly gives UNscaled predictions.\n",
    "    original_rf_preds = rf_original.predict(X_test)\n",
    "    original_rf_time = time.time() - start_time\n",
    "\n",
    "    original_rf_mae = mean_absolute_error(y_test_unscaled, original_rf_preds)\n",
    "    original_rf_energy = estimate_energy(original_rf_time)\n",
    "\n",
    "    original_rf_path = f'rf_original_h{chosen_h}.pkl'\n",
    "    joblib.dump(rf_original, original_rf_path)\n",
    "    original_rf_size = os.path.getsize(original_rf_path) / (1024*1024)\n",
    "\n",
    "    compression_results.append({\n",
    "        'Model': 'RF_Original', 'MAE': original_rf_mae, 'Size_MB': original_rf_size,\n",
    "        'Time_s': original_rf_time, 'Energy_mWh': original_rf_energy\n",
    "    })\n",
    "    print(f'RF Original - MAE: {original_rf_mae:.4f}, Size: {original_rf_size:.2f} MB, Time: {original_rf_time:.4f}s, Energy: {original_rf_energy:.4f} mWh')\n",
    "except Exception as e:\n",
    "    print(f\"Error with RF Original evaluation: {e}\")\n",
    "    compression_results.append({'Model': 'RF_Original', 'MAE': np.nan, 'Size_MB': np.nan, 'Time_s': np.nan, 'Energy_mWh': np.nan})\n",
    "\n",
    "\n",
    "# --- Random Forest with Feature Selection ---\n",
    "print(f\"\\n--- Evaluating RF Feature Selection (Horizon {chosen_h}h) ---\")\n",
    "try:\n",
    "    rf_base_model_for_selection = joblib.load(f'rf_model_h{chosen_h}.pkl')\n",
    "    selector = SelectFromModel(rf_base_model_for_selection, prefit=True, threshold='median')\n",
    "\n",
    "    # Transform X_train and X_test to numpy arrays to avoid 'feature names' warning\n",
    "    X_train_sel = selector.transform(X_train.values)\n",
    "    X_test_sel = selector.transform(X_test.values)\n",
    "\n",
    "    rf_sel = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_sel.fit(X_train_sel, y_train_unscaled) # Fit on unscaled y_train\n",
    "\n",
    "    rf_sel_path = f'rf_feature_selected_h{chosen_h}.pkl'\n",
    "    joblib.dump(rf_sel, rf_sel_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # RF predicts directly unscaled target\n",
    "    rf_sel_preds = rf_sel.predict(X_test_sel)\n",
    "    rf_sel_time = time.time() - start_time\n",
    "\n",
    "    rf_sel_mae = mean_absolute_error(y_test_unscaled, rf_sel_preds)\n",
    "    rf_sel_energy = estimate_energy(rf_sel_time)\n",
    "    rf_sel_size = os.path.getsize(rf_sel_path) / (1024*1024)\n",
    "\n",
    "    compression_results.append({\n",
    "        'Model': 'RF_FeatureSelection', 'MAE': rf_sel_mae, 'Size_MB': rf_sel_size,\n",
    "        'Time_s': rf_sel_time, 'Energy_mWh': rf_sel_energy\n",
    "    })\n",
    "    print(f'RF Feature Selection - MAE: {rf_sel_mae:.4f}, Size: {rf_sel_size:.2f} MB, Time: {rf_sel_time:.4f}s, Energy: {rf_sel_energy:.4f} mWh')\n",
    "except Exception as e:\n",
    "    print(f\"Error with RF Feature Selection: {e}. {e}\") # Print the actual error\n",
    "    compression_results.append({'Model': 'RF_FeatureSelection', 'MAE': np.nan, 'Size_MB': np.nan, 'Time_s': np.nan, 'Energy_mWh': np.nan})\n",
    "\n",
    "\n",
    "# --- Random Forest with Parameter Reduction ---\n",
    "print(f\"\\n--- Evaluating RF Parameter Reduction (Horizon {chosen_h}h) ---\")\n",
    "try:\n",
    "    rf_red = RandomForestRegressor(n_estimators=50, max_depth=8, min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
    "    rf_red.fit(X_train, y_train_unscaled) # Fit on unscaled y_train\n",
    "\n",
    "    rf_red_path = f'rf_param_reduced_h{chosen_h}.pkl'\n",
    "    joblib.dump(rf_red, rf_red_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # RF predicts directly unscaled target\n",
    "    rf_red_preds = rf_red.predict(X_test)\n",
    "    rf_red_time = time.time() - start_time\n",
    "\n",
    "    rf_red_mae = mean_absolute_error(y_test_unscaled, rf_red_preds)\n",
    "    rf_red_energy = estimate_energy(rf_red_time)\n",
    "    rf_red_size = os.path.getsize(rf_red_path) / (1024*1024)\n",
    "\n",
    "    compression_results.append({\n",
    "        'Model': 'RF_ParamReduction', 'MAE': rf_red_mae, 'Size_MB': rf_red_size,\n",
    "        'Time_s': rf_red_time, 'Energy_mWh': rf_red_energy\n",
    "    })\n",
    "    print(f'RF Param Reduction - MAE: {rf_red_mae:.4f}, Size: {rf_red_size:.2f} MB, Time: {rf_red_time:.4f}s, Energy: {rf_red_energy:.4f} mWh')\n",
    "except Exception as e:\n",
    "    print(f\"Error with RF Parameter Reduction: {e}\")\n",
    "    compression_results.append({'Model': 'RF_ParamReduction', 'MAE': np.nan, 'Size_MB': np.nan, 'Time_s': np.nan, 'Energy_mWh': np.nan})\n",
    "\n",
    "\n",
    "# --- Add Comprehensive Metrics (Compression Ratio, Energy Savings %, MAE Increase %) ---\n",
    "df_results = pd.DataFrame(compression_results)\n",
    "\n",
    "print(\"\\n--- Calculating Compression Metrics ---\")\n",
    "for i, row in df_results.iterrows():\n",
    "    if 'Original' not in row['Model']:\n",
    "        base_model_name = 'LSTM_Original' if 'LSTM' in row['Model'] else 'RF_Original'\n",
    "        base_model_row = df_results[df_results['Model'] == base_model_name]\n",
    "\n",
    "        if not base_model_row.empty:\n",
    "            base_idx = base_model_row.index[0]\n",
    "\n",
    "            original_size_mb = df_results.at[base_idx, 'Size_MB']\n",
    "            compressed_size_mb = df_results.at[i, 'Size_MB']\n",
    "\n",
    "            original_energy_mwh = df_results.at[base_idx, 'Energy_mWh']\n",
    "            compressed_energy_mwh = df_results.at[i, 'Energy_mWh']\n",
    "\n",
    "            original_mae = df_results.at[base_idx, 'MAE']\n",
    "            compressed_mae = df_results.at[i, 'MAE']\n",
    "\n",
    "            # Compression Ratio: Original Size / Compressed Size\n",
    "            df_results.at[i, 'Compression Ratio (Size)'] = original_size_mb / compressed_size_mb if compressed_size_mb > 0 else np.inf\n",
    "\n",
    "            # Energy Savings %: (Original Energy - Compressed Energy) / Original Energy * 100\n",
    "            df_results.at[i, 'Energy Savings %'] = (original_energy_mwh - compressed_energy_mwh) / original_energy_mwh * 100 if original_energy_mwh > 0 else 0\n",
    "\n",
    "            # MAE Increase %: (Compressed MAE - Original MAE) / Original MAE * 100\n",
    "            df_results.at[i, 'MAE Increase %'] = (compressed_mae - original_mae) / original_mae * 100 if original_mae > 0 else 0\n",
    "\n",
    "            print(f\"Metrics for {row['Model']}: Size Ratio={df_results.at[i, 'Compression Ratio (Size)']:.2f}x, Energy Savings={df_results.at[i, 'Energy Savings %']:.2f}%, MAE Increase={df_results.at[i, 'MAE Increase %']:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Base model {base_model_name} not found in results for {row['Model']}. Cannot calculate ratios.\")\n",
    "\n",
    "# Sustainability Analysis: Check SDG criteria (<5% MAE loss, >2x compression)\n",
    "print(\"\\n--- Sustainability Analysis (Meets SDG Criteria: MAE Increase <5%, Compression Ratio (Size) >2x) ---\")\n",
    "for i, row in df_results.iterrows():\n",
    "    if 'Original' not in row['Model']:\n",
    "        mae_increase_percentage = row.get('MAE Increase %', np.inf)\n",
    "        compression_ratio = row.get('Compression Ratio (Size)', 0)\n",
    "\n",
    "        meets_criteria = (mae_increase_percentage < 5) and (compression_ratio > 2)\n",
    "        print(f\"{row['Model']}: {'YES' if meets_criteria else 'NO'} (MAE Inc: {mae_increase_percentage:.2f}%, Comp Ratio: {compression_ratio:.2f}x)\")\n",
    "\n",
    "# --- XAI: SHAP on the most compressed RF model (RF_ParamReduction) ---\n",
    "print(f\"\\n--- Performing Explainable AI (SHAP) on Compressed RF Model ---\")\n",
    "try:\n",
    "    rf_compressed_model = joblib.load(f'rf_param_reduced_h{chosen_h}.pkl')\n",
    "\n",
    "    shap_sample_rf_compressed = X_test.sample(n=min(len(X_test), 100), random_state=42)\n",
    "\n",
    "    explainer_rf_comp = shap.TreeExplainer(rf_compressed_model)\n",
    "    shap_values_rf_comp = explainer_rf_comp.shap_values(shap_sample_rf_compressed)\n",
    "\n",
    "    if isinstance(shap_values_rf_comp, list) and len(shap_values_rf_comp) == 1:\n",
    "        shap_values_rf_comp = shap_values_rf_comp[0]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_rf_comp, shap_sample_rf_compressed, plot_type=\"bar\", show=False)\n",
    "    plt.title(f'SHAP Feature Importance (Compressed RF, Horizon {chosen_h}h)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_compressed_rf_h{chosen_h}_bar.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_rf_comp, shap_sample_rf_compressed, show=False)\n",
    "    plt.title(f'SHAP Feature Effects (Compressed RF, Horizon {chosen_h}h)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_compressed_rf_h{chosen_h}_scatter.png')\n",
    "    plt.show()\n",
    "    print(f\"SHAP analysis for compressed RF completed and plots saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error performing SHAP for compressed RF: {e}\")\n",
    "\n",
    "\n",
    "# --- Save enhanced results to CSV and HTML for report ---\n",
    "df_results_final_output_csv = 'compression_results_enhanced.csv'\n",
    "df_results_final_output_html = 'compression_summary.html'\n",
    "\n",
    "df_results.to_csv(df_results_final_output_csv, index=False)\n",
    "df_results.to_html(df_results_final_output_html, index=False)\n",
    "print(f\"\\nFinal compression results saved to {df_results_final_output_csv} and {df_results_final_output_html}.\")\n",
    "print(\"--- Model Compression and Efficiency Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure a best model exists and measure timing on a small batch\n",
    "import os, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "CANDIDATES = ['best_lstm.keras', 'lstm_model_h24.keras']\n",
    "model_path = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "if model_path is None:\n",
    "    raise FileNotFoundError('No model file found among: ' + ', '.join(CANDIDATES))\n",
    "\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Create a small random batch with expected input shape\n",
    "# If your model expects (batch, time, features) with time=1 or 24, adjust here accordingly\n",
    "try:\n",
    "    time_steps = model.input_shape[1] or 1\n",
    "    features = model.input_shape[2]\n",
    "except Exception:\n",
    "    time_steps, features = 1, 5\n",
    "\n",
    "X_dummy = np.random.rand(128, time_steps, features).astype(np.float32)\n",
    "start = time.time()\n",
    "_ = model.predict(X_dummy, verbose=0)\n",
    "elapsed_s = time.time() - start\n",
    "print(f'Inference time (128 samples): {elapsed_s*1000:.2f} ms')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
