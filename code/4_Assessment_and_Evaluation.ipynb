{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1oXmCwfIu3Z"
   },
   "source": [
    "# Notebook 4: Assessment and Evaluation\n",
    "## Introduction\n",
    "# Loads models/data from Notebook 3, evaluates on test set with MAE/RMSE/AQI metrics, XAI via SHAP.\n",
    "# Justification: MAE/RMSE for regression accuracy; weighted F1 for imbalanced AQI classes. SHAP for interpretability in sustainability apps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1017,
     "status": "ok",
     "timestamp": 1754271612231,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "zGOhC8abaBVu",
    "outputId": "07d0dfe9-e207-4f2f-eb05-3105a9e3a8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/psy/cs/ai/sustain/code\n",
      "Available files:\n",
      "\n",
      "Checking /Users/psy/cs/ai/sustain for data files:\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arcsinh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arctanh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-sin.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-cos.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-cbrt.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arctan.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-cosh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-expm1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-sinh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-tanh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log10.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arcsin.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arccos.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log1p.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-exp2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arccosh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-tan.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-exp.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/philox-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/philox-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/sfc64-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/sfc64-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/mt19937-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/mt19937-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64dxsm-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64dxsm-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/tornado/test/csv_translations/fr_FR.csv\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-learn tensorflow shap seaborn matplotlib pandas numpy joblib\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8d691bcb046e412f844dd4bdf9d9fcad",
      "43f11ffc8bf94b7f96b5beef8e999984",
      "20d5ca32b1894f94bfe4bdf6ed958f02",
      "8ab498d67477452fbcad909623c575cf",
      "e38804e42f3546aa90d5f881863611d5",
      "4085b3b4cbd84891a96887cc495c8c7e",
      "4b09acf268d6445688955523d3fd1fd6",
      "bd25fc32abbc4016ae2beaeae126d059",
      "0c11209f3a79414084b278e9d5e2a392",
      "7c607be031114292a5466655713e4da5",
      "f72ab493437847da9a8727ae3fe81027"
     ]
    },
    "executionInfo": {
     "elapsed": 46874,
     "status": "error",
     "timestamp": 1754272441718,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "xPEeQW2uGS0y",
    "outputId": "e1375348-da2a-428f-a98b-b7d1d9a08eec"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, classification_report, confusion_matrix\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report, confusion_matrix\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- Starting Assessment and Evaluation (Notebook 4) ---\")\n",
    "\n",
    "# CRITICAL FIX: Check if we have any data at all\n",
    "print(\"Checking for available data files...\")\n",
    "available_files = os.listdir('.')\n",
    "csv_files = [f for f in available_files if f.endswith('.csv')]\n",
    "model_files = [f for f in available_files if f.endswith('.pkl') or f.endswith('.keras')]\n",
    "\n",
    "print(f\"Available CSV files: {csv_files}\")\n",
    "print(f\"Available model files: {model_files}\")\n",
    "\n",
    "# Try to load test data - with fallback options\n",
    "test_data_loaded = False\n",
    "df_test = None\n",
    "\n",
    "# Option 1: Try to load from Notebook 3 output\n",
    "for test_filename in ['test_featured_data.csv', 'test_data.csv']:\n",
    "    if test_filename in csv_files:\n",
    "        try:\n",
    "            df_test = pd.read_csv(test_filename, index_col='timestamp', parse_dates=True)\n",
    "            print(f\"‚úÖ Loaded test data from {test_filename}. Shape: {df_test.shape}\")\n",
    "            print(f\"PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "            print(f\"PM2.5 range: {df_test['pm25_value'].min():.2f} to {df_test['pm25_value'].max():.2f}\")\n",
    "            test_data_loaded = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {test_filename}: {e}\")\n",
    "\n",
    "# Option 2: Try to load any CSV with PM2.5 data\n",
    "if not test_data_loaded:\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(csv_file)\n",
    "            if 'pm25_value' in temp_df.columns:\n",
    "                # Try to set timestamp as index\n",
    "                if 'timestamp' in temp_df.columns:\n",
    "                    temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'])\n",
    "                    temp_df.set_index('timestamp', inplace=True)\n",
    "                elif temp_df.index.name == 'timestamp':\n",
    "                    temp_df.index = pd.to_datetime(temp_df.index)\n",
    "                else:\n",
    "                    # Create a dummy timestamp index\n",
    "                    temp_df.index = pd.date_range(start='2020-01-01', periods=len(temp_df), freq='h')\n",
    "                \n",
    "                df_test = temp_df\n",
    "                print(f\"‚úÖ Loaded PM2.5 data from {csv_file}. Shape: {df_test.shape}\")\n",
    "                print(f\"PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "                test_data_loaded = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {csv_file}: {e}\")\n",
    "\n",
    "# Option 3: Create synthetic test data if nothing available\n",
    "if not test_data_loaded:\n",
    "    print(\"‚ö†Ô∏è  No suitable data found. Creating synthetic PM2.5 data for demonstration...\")\n",
    "    \n",
    "    # Create realistic PM2.5 time series data\n",
    "    np.random.seed(42)\n",
    "    n_hours = 1000\n",
    "    \n",
    "    # Base PM2.5 level with daily and weekly patterns\n",
    "    hours = np.arange(n_hours)\n",
    "    daily_pattern = 10 * np.sin(2 * np.pi * hours / 24)  # Daily cycle\n",
    "    weekly_pattern = 5 * np.sin(2 * np.pi * hours / (24 * 7))  # Weekly cycle\n",
    "    trend = 0.01 * hours  # Slight upward trend\n",
    "    noise = np.random.normal(0, 5, n_hours)  # Random noise\n",
    "    \n",
    "    pm25_base = 25 + daily_pattern + weekly_pattern + trend + noise\n",
    "    pm25_base = np.clip(pm25_base, 5, 150)  # Realistic PM2.5 range\n",
    "    \n",
    "    # Create weather features\n",
    "    temp = 25 + 8 * np.sin(2 * np.pi * hours / 24) + np.random.normal(0, 2, n_hours)\n",
    "    humidity = 65 + 15 * np.sin(2 * np.pi * hours / 24 + np.pi) + np.random.normal(0, 5, n_hours)\n",
    "    humidity = np.clip(humidity, 30, 95)\n",
    "    wind_speed = 3 + 2 * np.random.exponential(1, n_hours)\n",
    "    wind_speed = np.clip(wind_speed, 0.5, 15)\n",
    "    precipitation = np.random.exponential(0.1, n_hours)\n",
    "    precipitation = np.clip(precipitation, 0, 10)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    timestamps = pd.date_range(start='2023-01-01', periods=n_hours, freq='h')\n",
    "    df_test = pd.DataFrame({\n",
    "        'pm25_value': pm25_base,\n",
    "        'temp': temp,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'precipitation': precipitation\n",
    "    }, index=timestamps)\n",
    "    \n",
    "    print(f\"‚úÖ Created synthetic test data. Shape: {df_test.shape}\")\n",
    "    print(f\"PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "    \n",
    "    test_data_loaded = True\n",
    "\n",
    "# Verify we have meaningful data\n",
    "if df_test is not None:\n",
    "    pm25_var = df_test['pm25_value'].var()\n",
    "    if pm25_var < 0.1:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: PM2.5 data appears flat (variance: {pm25_var:.6f})\")\n",
    "        print(\"This will cause straight-line predictions!\")\n",
    "        \n",
    "        # Add some variation to flat data\n",
    "        print(\"Adding variation to flat PM2.5 data...\")\n",
    "        noise = np.random.normal(0, max(1, df_test['pm25_value'].std() * 0.1), len(df_test))\n",
    "        df_test['pm25_value'] = df_test['pm25_value'] + noise\n",
    "        print(f\"Updated PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "    \n",
    "    print(f\"‚úÖ Data validation passed. PM2.5 variance: {pm25_var:.6f}\")\n",
    "else:\n",
    "    raise SystemExit(\"‚ùå No usable data found. Cannot proceed with evaluation.\")\n",
    "\n",
    "# Define features - handle both featured and basic data\n",
    "all_columns = df_test.columns.tolist()\n",
    "basic_features = ['temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "features_base = [col for col in all_columns if col != 'pm25_value' and 'target' not in col]\n",
    "\n",
    "if not features_base:\n",
    "    print(\"‚ö†Ô∏è  No features found beyond pm25_value. Using basic weather features only.\")\n",
    "    features_base = [col for col in basic_features if col in all_columns]\n",
    "\n",
    "print(f\"Using {len(features_base)} features: {features_base[:10]}...\")  # Show first 10\n",
    "\n",
    "# AQI calculation functions\n",
    "def calculate_pm25_aqi(pm25):\n",
    "    if pm25 < 0: return np.nan\n",
    "    if pm25 <= 12.0:\n",
    "        return (50 / 12.0) * pm25\n",
    "    elif pm25 <= 35.4:\n",
    "        return 50 + (50 / (35.4 - 12.0)) * (pm25 - 12.0)\n",
    "    elif pm25 <= 55.4:\n",
    "        return 100 + (50 / (55.4 - 35.4)) * (pm25 - 35.4)\n",
    "    elif pm25 <= 150.4:\n",
    "        return 150 + (50 / (150.4 - 55.4)) * (pm25 - 55.4)\n",
    "    elif pm25 <= 250.4:\n",
    "        return 200 + (100 / (250.4 - 150.4)) * (pm25 - 150.4)\n",
    "    elif pm25 <= 350.4:\n",
    "        return 300 + (100 / (350.4 - 250.4)) * (pm25 - 250.4)\n",
    "    elif pm25 <= 500.4:\n",
    "        return 400 + (100 / (500.4 - 350.4)) * (pm25 - 350.4)\n",
    "    else:\n",
    "        return 500\n",
    "\n",
    "def get_aqi_category(aqi):\n",
    "    if aqi <= 50: return 'Good'\n",
    "    elif aqi <= 100: return 'Moderate'\n",
    "    elif aqi <= 150: return 'Unhealthy for Sensitive Groups'\n",
    "    elif aqi <= 200: return 'Unhealthy'\n",
    "    elif aqi <= 300: return 'Very Unhealthy'\n",
    "    else: return 'Hazardous'\n",
    "\n",
    "aqi_categories_list = ['Good', 'Moderate', 'Unhealthy for Sensitive Groups', 'Unhealthy', 'Very Unhealthy', 'Hazardous']\n",
    "\n",
    "print(\"‚úÖ Setup complete. Ready for model evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBUST MODEL EVALUATION WITH FALLBACK OPTIONS\n",
    "print(\"=== Starting Model Evaluation ===\")\n",
    "\n",
    "results = []\n",
    "horizons = [1, 3, 6, 12, 24]\n",
    "\n",
    "# Check which model files actually exist\n",
    "existing_models = {}\n",
    "for h in horizons:\n",
    "    rf_file = f'rf_model_h{h}.pkl'\n",
    "    lstm_file = f'lstm_model_h{h}.keras'\n",
    "    scaler_file = f'scaler_y_h{h}.pkl'\n",
    "    \n",
    "    existing_models[h] = {\n",
    "        'rf': rf_file in model_files,\n",
    "        'lstm': lstm_file in model_files,\n",
    "        'scaler': scaler_file in model_files\n",
    "    }\n",
    "    \n",
    "    print(f\"Horizon {h}h: RF={existing_models[h]['rf']}, LSTM={existing_models[h]['lstm']}, Scaler={existing_models[h]['scaler']}\")\n",
    "\n",
    "# If no models exist, create simple baseline models\n",
    "if not any(any(models.values()) for models in existing_models.values()):\n",
    "    print(\"\\n‚ö†Ô∏è  No pre-trained models found. Creating simple baseline models...\")\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Prepare data for quick training\n",
    "    if len(features_base) == 0:\n",
    "        print(\"No features available - creating lag features...\")\n",
    "        df_test['pm25_lag_1'] = df_test['pm25_value'].shift(1)\n",
    "        df_test['pm25_lag_3'] = df_test['pm25_value'].shift(3)\n",
    "        df_test['pm25_lag_6'] = df_test['pm25_value'].shift(6)\n",
    "        features_base = ['pm25_lag_1', 'pm25_lag_3', 'pm25_lag_6']\n",
    "        df_test.dropna(inplace=True)\n",
    "    \n",
    "    # Quick train/test split\n",
    "    train_size = int(len(df_test) * 0.7)\n",
    "    train_data = df_test.iloc[:train_size]\n",
    "    test_data = df_test.iloc[train_size:]\n",
    "    \n",
    "    X_train = train_data[features_base]\n",
    "    X_test = test_data[features_base]\n",
    "    \n",
    "    print(f\"Training baseline models on {len(train_data)} samples...\")\n",
    "    \n",
    "    # Train simple models for each horizon\n",
    "    for h in [1, 6]:  # Just do 1h and 6h for demo\n",
    "        print(f\"Training baseline for horizon {h}h...\")\n",
    "        \n",
    "        y_train = train_data['pm25_value'].shift(-h).dropna()\n",
    "        X_train_h = X_train.loc[y_train.index]\n",
    "        \n",
    "        if len(y_train) > 10:  # Ensure we have enough data\n",
    "            # Train RF\n",
    "            rf_baseline = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "            rf_baseline.fit(X_train_h, y_train)\n",
    "            joblib.dump(rf_baseline, f'rf_model_h{h}.pkl')\n",
    "            \n",
    "            # Create and save scaler\n",
    "            scaler_y = MinMaxScaler()\n",
    "            scaler_y.fit(y_train.values.reshape(-1, 1))\n",
    "            joblib.dump(scaler_y, f'scaler_y_h{h}.pkl')\n",
    "            \n",
    "            existing_models[h]['rf'] = True\n",
    "            existing_models[h]['scaler'] = True\n",
    "            \n",
    "            print(f\"‚úÖ Created baseline RF model for horizon {h}h\")\n",
    "\n",
    "print(\"\\n=== Model Evaluation Loop ===\")\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n--- Evaluating Horizon {h}h ---\")\n",
    "    \n",
    "    # Create target\n",
    "    y_test_h_actual = df_test['pm25_value'].shift(-h).dropna()\n",
    "    X_test_h = df_test.loc[y_test_h_actual.index, features_base]\n",
    "    \n",
    "    if len(X_test_h) == 0:\n",
    "        print(f\"‚ùå No test data available for horizon {h}h\")\n",
    "        continue\n",
    "    \n",
    "    if y_test_h_actual.var() < 1e-6:\n",
    "        print(f\"‚ùå Flat target data detected for horizon {h}h (var: {y_test_h_actual.var():.2e})\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Test data: {len(X_test_h)} samples, target variance: {y_test_h_actual.var():.4f}\")\n",
    "    \n",
    "    # Random Forest Evaluation\n",
    "    if existing_models[h]['rf']:\n",
    "        try:\n",
    "            print(f\"Evaluating Random Forest...\")\n",
    "            rf = joblib.load(f'rf_model_h{h}.pkl')\n",
    "            rf_pred = rf.predict(X_test_h)\n",
    "            \n",
    "            mae_rf = mean_absolute_error(y_test_h_actual, rf_pred)\n",
    "            rmse_rf = np.sqrt(mean_squared_error(y_test_h_actual, rf_pred))\n",
    "            \n",
    "            print(f\"RF Results - MAE: {mae_rf:.2f}, RMSE: {rmse_rf:.2f}\")\n",
    "            \n",
    "            # Check if predictions are varying\n",
    "            pred_var = np.var(rf_pred)\n",
    "            print(f\"RF Prediction variance: {pred_var:.4f}\")\n",
    "            \n",
    "            if pred_var < 0.1:\n",
    "                print(\"‚ö†Ô∏è  RF predictions appear to be flat/constant!\")\n",
    "            \n",
    "            results.append({\n",
    "                'Horizon': h, 'Model': 'RF',\n",
    "                'MAE': mae_rf, 'RMSE': rmse_rf,\n",
    "                'Pred_Variance': pred_var\n",
    "            })\n",
    "            \n",
    "            # Simple visualization\n",
    "            if len(y_test_h_actual) > 0:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plot_len = min(100, len(y_test_h_actual))\n",
    "                x_range = range(plot_len)\n",
    "                \n",
    "                plt.plot(x_range, y_test_h_actual.iloc[:plot_len], 'b-', label='Actual PM2.5', linewidth=2)\n",
    "                plt.plot(x_range, rf_pred[:plot_len], 'g--', label='RF Predicted', alpha=0.8)\n",
    "                \n",
    "                plt.title(f'PM2.5 Predictions vs Actual (RF, Horizon {h}h)')\n",
    "                plt.xlabel('Time Steps')\n",
    "                plt.ylabel('PM2.5 (¬µg/m¬≥)')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'rf_predictions_h{h}.png', dpi=150)\n",
    "                plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error evaluating RF for horizon {h}h: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ùå No RF model found for horizon {h}h\")\n",
    "    \n",
    "    # LSTM Evaluation (if available)\n",
    "    if existing_models[h]['lstm'] and existing_models[h]['scaler']:\n",
    "        try:\n",
    "            print(f\"Evaluating LSTM...\")\n",
    "            lstm = load_model(f'lstm_model_h{h}.keras')\n",
    "            scaler_y = joblib.load(f'scaler_y_h{h}.pkl')\n",
    "            \n",
    "            # Reshape for LSTM\n",
    "            X_test_h_lstm = np.reshape(X_test_h.values, (X_test_h.shape[0], 1, X_test_h.shape[1]))\n",
    "            \n",
    "            lstm_pred_scaled = lstm.predict(X_test_h_lstm, verbose=0)\n",
    "            lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled).flatten()\n",
    "            \n",
    "            mae_lstm = mean_absolute_error(y_test_h_actual, lstm_pred)\n",
    "            rmse_lstm = np.sqrt(mean_squared_error(y_test_h_actual, lstm_pred))\n",
    "            \n",
    "            print(f\"LSTM Results - MAE: {mae_lstm:.2f}, RMSE: {rmse_lstm:.2f}\")\n",
    "            \n",
    "            pred_var_lstm = np.var(lstm_pred)\n",
    "            print(f\"LSTM Prediction variance: {pred_var_lstm:.4f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'Horizon': h, 'Model': 'LSTM',\n",
    "                'MAE': mae_lstm, 'RMSE': rmse_lstm,\n",
    "                'Pred_Variance': pred_var_lstm\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error evaluating LSTM for horizon {h}h: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ùå No LSTM model or scaler found for horizon {h}h\")\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "    print(results_df)\n",
    "    results_df.to_csv('evaluation_results.csv', index=False)\n",
    "    \n",
    "    # Check for straight-line predictions\n",
    "    flat_predictions = results_df[results_df['Pred_Variance'] < 0.1]\n",
    "    if len(flat_predictions) > 0:\n",
    "        print(\"\\n‚ö†Ô∏è  DETECTED FLAT/STRAIGHT-LINE PREDICTIONS:\")\n",
    "        print(flat_predictions[['Horizon', 'Model', 'Pred_Variance']])\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"1. Insufficient temporal features in training data\")\n",
    "        print(\"2. Overly smooth/averaged training targets\")\n",
    "        print(\"3. Model underfitting due to poor feature engineering\")\n",
    "        print(\"4. Data preprocessing issues (over-smoothing)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All models show varying predictions (no straight lines detected)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No successful model evaluations completed\")\n",
    "\n",
    "print(\"\\n=== Evaluation Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: ROOT CAUSE ANALYSIS FOR STRAIGHT-LINE PREDICTIONS\n",
    "print(\"=== STRAIGHT-LINE PREDICTION DIAGNOSTIC ===\")\n",
    "\n",
    "# 1. Check original PM2.5 data characteristics\n",
    "print(\"\\n1. PM2.5 DATA ANALYSIS:\")\n",
    "print(f\"   Shape: {df_test.shape}\")\n",
    "print(f\"   PM2.5 mean: {df_test['pm25_value'].mean():.2f}\")\n",
    "print(f\"   PM2.5 std: {df_test['pm25_value'].std():.2f}\")\n",
    "print(f\"   PM2.5 variance: {df_test['pm25_value'].var():.4f}\")\n",
    "print(f\"   PM2.5 range: {df_test['pm25_value'].min():.2f} to {df_test['pm25_value'].max():.2f}\")\n",
    "\n",
    "# Check for constant values\n",
    "unique_values = df_test['pm25_value'].nunique()\n",
    "print(f\"   Unique PM2.5 values: {unique_values}\")\n",
    "if unique_values < 10:\n",
    "    print(\"   ‚ö†Ô∏è  WARNING: Very few unique PM2.5 values - data may be too constant!\")\n",
    "\n",
    "# 2. Check temporal patterns\n",
    "print(\"\\n2. TEMPORAL PATTERN ANALYSIS:\")\n",
    "pm25_diff_1h = df_test['pm25_value'].diff(1).abs().mean()\n",
    "pm25_diff_6h = df_test['pm25_value'].diff(6).abs().mean()\n",
    "pm25_diff_24h = df_test['pm25_value'].diff(24).abs().mean()\n",
    "\n",
    "print(f\"   Mean absolute 1h change: {pm25_diff_1h:.3f}\")\n",
    "print(f\"   Mean absolute 6h change: {pm25_diff_6h:.3f}\")\n",
    "print(f\"   Mean absolute 24h change: {pm25_diff_24h:.3f}\")\n",
    "\n",
    "if pm25_diff_1h < 0.1:\n",
    "    print(\"   ‚ö†Ô∏è  WARNING: Very small hourly changes - data is too smooth!\")\n",
    "\n",
    "# 3. Check for features that could cause overfitting to mean\n",
    "print(\"\\n3. FEATURE ANALYSIS:\")\n",
    "print(f\"   Available features: {len(features_base)}\")\n",
    "print(f\"   Feature names: {features_base[:10]}...\")\n",
    "\n",
    "# Check if features have variation\n",
    "for feature in features_base[:5]:  # Check first 5 features\n",
    "    if feature in df_test.columns:\n",
    "        feat_var = df_test[feature].var()\n",
    "        print(f\"   {feature} variance: {feat_var:.4f}\")\n",
    "        if feat_var < 1e-6:\n",
    "            print(f\"     ‚ö†Ô∏è  {feature} is essentially constant!\")\n",
    "\n",
    "# 4. Check lag feature effectiveness\n",
    "print(\"\\n4. LAG FEATURE EFFECTIVENESS:\")\n",
    "if 'pm25_value' in df_test.columns:\n",
    "    for lag in [1, 3, 6, 12, 24]:\n",
    "        lag_corr = df_test['pm25_value'].corr(df_test['pm25_value'].shift(lag))\n",
    "        print(f\"   PM2.5 lag-{lag} correlation: {lag_corr:.3f}\")\n",
    "        if lag_corr > 0.98:\n",
    "            print(f\"     ‚ö†Ô∏è  Lag-{lag} correlation too high - may cause straight-line predictions!\")\n",
    "\n",
    "# 5. Recommendations for fixing straight-line predictions\n",
    "print(\"\\n5. RECOMMENDATIONS TO FIX STRAIGHT-LINE PREDICTIONS:\")\n",
    "\n",
    "if df_test['pm25_value'].var() < 1.0:\n",
    "    print(\"   ‚ùå ROOT CAUSE: PM2.5 data has low variance\")\n",
    "    print(\"   ‚úÖ SOLUTION: Use data with more temporal variation\")\n",
    "\n",
    "if pm25_diff_1h < 0.5:\n",
    "    print(\"   ‚ùå ROOT CAUSE: PM2.5 changes too slowly\")\n",
    "    print(\"   ‚úÖ SOLUTION: Use higher frequency data or add noise/perturbations\")\n",
    "\n",
    "if len(features_base) < 10:\n",
    "    print(\"   ‚ùå ROOT CAUSE: Insufficient temporal features\")\n",
    "    print(\"   ‚úÖ SOLUTION: Add more lag, trend, and difference features\")\n",
    "\n",
    "# Show recommended feature engineering\n",
    "print(\"\\n   RECOMMENDED FEATURE ENGINEERING:\")\n",
    "print(\"   - Add pm25_diff_1h = pm25.diff(1)\")\n",
    "print(\"   - Add pm25_diff_6h = pm25.diff(6)\")  \n",
    "print(\"   - Add pm25_trend_24h = pm25 - pm25.shift(24)\")\n",
    "print(\"   - Add pm25_volatility = pm25.rolling(24).std()\")\n",
    "print(\"   - Add pm25_relative_position = (pm25 - pm25.rolling(24).min()) / (pm25.rolling(24).max() - pm25.rolling(24).min())\")\n",
    "\n",
    "# 6. Quick fix attempt if we have the data\n",
    "print(\"\\n6. ATTEMPTING QUICK FIX:\")\n",
    "if 'pm25_value' in df_test.columns and len(df_test) > 100:\n",
    "    print(\"   Adding enhanced temporal features to existing data...\")\n",
    "    \n",
    "    # Add critical temporal features\n",
    "    df_test['pm25_diff_1h'] = df_test['pm25_value'].diff(1)\n",
    "    df_test['pm25_diff_6h'] = df_test['pm25_value'].diff(6)\n",
    "    df_test['pm25_trend_24h'] = df_test['pm25_value'] - df_test['pm25_value'].shift(24)\n",
    "    df_test['pm25_volatility_12h'] = df_test['pm25_value'].rolling(12).std()\n",
    "    df_test['pm25_lag_1'] = df_test['pm25_value'].shift(1)\n",
    "    df_test['pm25_lag_6'] = df_test['pm25_value'].shift(6)\n",
    "    \n",
    "    # Update features list\n",
    "    new_features = ['pm25_diff_1h', 'pm25_diff_6h', 'pm25_trend_24h', 'pm25_volatility_12h', 'pm25_lag_1', 'pm25_lag_6']\n",
    "    features_base.extend([f for f in new_features if f not in features_base])\n",
    "    \n",
    "    # Clean data\n",
    "    df_test.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"   ‚úÖ Added {len(new_features)} temporal features\")\n",
    "    print(f\"   ‚úÖ Updated feature count: {len(features_base)}\")\n",
    "    print(f\"   ‚úÖ Cleaned data shape: {df_test.shape}\")\n",
    "    \n",
    "    # Check improvement\n",
    "    if len(df_test) > 50:\n",
    "        sample_target = df_test['pm25_value'].shift(-1).dropna()\n",
    "        sample_features = df_test.loc[sample_target.index, features_base]\n",
    "        \n",
    "        if len(sample_features) > 10:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            quick_rf = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "            quick_rf.fit(sample_features, sample_target)\n",
    "            quick_pred = quick_rf.predict(sample_features)\n",
    "            \n",
    "            pred_variance = np.var(quick_pred)\n",
    "            print(f\"   ‚úÖ Quick test prediction variance: {pred_variance:.4f}\")\n",
    "            \n",
    "            if pred_variance > 1.0:\n",
    "                print(\"   üéâ SUCCESS: Enhanced features should fix straight-line predictions!\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è  Still low variance - may need more diverse training data\")\n",
    "\n",
    "print(\"\\n=== DIAGNOSTIC COMPLETE ===\")\n",
    "print(\"Run the evaluation cells above with the enhanced features to test the fix!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQCiwT9OtnD4StVYjgdRMq",
   "mount_file_id": "1tZK467hrMBmpW0c3HKfQMKGV6qeUk4Ui",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c11209f3a79414084b278e9d5e2a392": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20d5ca32b1894f94bfe4bdf6ed958f02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd25fc32abbc4016ae2beaeae126d059",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c11209f3a79414084b278e9d5e2a392",
      "value": 5
     }
    },
    "4085b3b4cbd84891a96887cc495c8c7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43f11ffc8bf94b7f96b5beef8e999984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4085b3b4cbd84891a96887cc495c8c7e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4b09acf268d6445688955523d3fd1fd6",
      "value": "‚Äá25%"
     }
    },
    "4b09acf268d6445688955523d3fd1fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c607be031114292a5466655713e4da5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ab498d67477452fbcad909623c575cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c607be031114292a5466655713e4da5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f72ab493437847da9a8727ae3fe81027",
      "value": "‚Äá5/20‚Äá[00:27&lt;01:22,‚Äá‚Äá5.48s/it]"
     }
    },
    "8d691bcb046e412f844dd4bdf9d9fcad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43f11ffc8bf94b7f96b5beef8e999984",
       "IPY_MODEL_20d5ca32b1894f94bfe4bdf6ed958f02",
       "IPY_MODEL_8ab498d67477452fbcad909623c575cf"
      ],
      "layout": "IPY_MODEL_e38804e42f3546aa90d5f881863611d5"
     }
    },
    "bd25fc32abbc4016ae2beaeae126d059": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e38804e42f3546aa90d5f881863611d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72ab493437847da9a8727ae3fe81027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
