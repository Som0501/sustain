{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1oXmCwfIu3Z"
   },
   "source": [
    "# Notebook 4: Assessment and Evaluation\n",
    "## Introduction\n",
    "# Loads models/data from Notebook 3, evaluates on test set with MAE/RMSE/AQI metrics, XAI via SHAP.\n",
    "# Justification: MAE/RMSE for regression accuracy; weighted F1 for imbalanced AQI classes. SHAP for interpretability in sustainability apps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1017,
     "status": "ok",
     "timestamp": 1754271612231,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "zGOhC8abaBVu",
    "outputId": "07d0dfe9-e207-4f2f-eb05-3105a9e3a8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/psy/cs/ai/sustain/code\n",
      "Available files:\n",
      "\n",
      "Checking /Users/psy/cs/ai/sustain for data files:\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arcsinh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arctanh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-sin.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-cos.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-cbrt.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arctan.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-cosh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-expm1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-sinh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-tanh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log10.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arcsin.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arccos.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log1p.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-exp2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arccosh.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-tan.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-exp.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/philox-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/philox-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/sfc64-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/sfc64-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/mt19937-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/mt19937-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64dxsm-testset-1.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/numpy/random/tests/data/pcg64dxsm-testset-2.csv\n",
      "  - /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/tornado/test/csv_translations/fr_FR.csv\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: tensorflow in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (2.20.0rc0)\n",
      "Collecting shap\n",
      "Collecting shap\n",
      "  Downloading shap-0.48.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Downloading shap-0.48.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: joblib in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: joblib in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (6.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: pillow in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Collecting tqdm>=4.27.0 (from shap)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (6.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: pillow in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Collecting tqdm>=4.27.0 (from shap)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Downloading numba-0.61.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Downloading numba-0.61.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (107 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Downloading llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Downloading llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading shap-0.48.0-cp313-cp313-macosx_11_0_arm64.whl (546 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m546.8/546.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading shap-0.48.0-cp313-cp313-macosx_11_0_arm64.whl (546 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m546.8/546.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading numba-0.61.2-cp313-cp313-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading numba-0.61.2-cp313-cp313-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl (26.2 MB)\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/26.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl (26.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.2/26.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.2/26.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: tqdm, slicer, pyparsing, numpy, llvmlite, kiwisolver, fonttools, cycler, cloudpickle, numba, contourpy, matplotlib, shap, seaborn\n",
      "\u001b[?25lInstalling collected packages: tqdm, slicer, pyparsing, numpy, llvmlite, kiwisolver, fonttools, cycler, cloudpickle, numba, contourpy, matplotlib, shap, seaborn\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2\n",
      "\u001b[2K    Uninstalling numpy-2.3.2:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.2\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2\n",
      "\u001b[2K    Uninstalling numpy-2.3.2:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.2\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [seaborn]━\u001b[0m \u001b[32m13/14\u001b[0m [seaborn]ib]a]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cloudpickle-3.1.1 contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 llvmlite-0.44.0 matplotlib-3.10.5 numba-0.61.2 numpy-2.2.6 pyparsing-3.2.3 seaborn-0.13.2 shap-0.48.0 slicer-0.0.8 tqdm-4.67.1\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [seaborn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cloudpickle-3.1.1 contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 llvmlite-0.44.0 matplotlib-3.10.5 numba-0.61.2 numpy-2.2.6 pyparsing-3.2.3 seaborn-0.13.2 shap-0.48.0 slicer-0.0.8 tqdm-4.67.1\n",
      "All packages installed successfully!\n",
      "All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-learn tensorflow shap seaborn matplotlib pandas numpy joblib\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8d691bcb046e412f844dd4bdf9d9fcad",
      "43f11ffc8bf94b7f96b5beef8e999984",
      "20d5ca32b1894f94bfe4bdf6ed958f02",
      "8ab498d67477452fbcad909623c575cf",
      "e38804e42f3546aa90d5f881863611d5",
      "4085b3b4cbd84891a96887cc495c8c7e",
      "4b09acf268d6445688955523d3fd1fd6",
      "bd25fc32abbc4016ae2beaeae126d059",
      "0c11209f3a79414084b278e9d5e2a392",
      "7c607be031114292a5466655713e4da5",
      "f72ab493437847da9a8727ae3fe81027"
     ]
    },
    "executionInfo": {
     "elapsed": 46874,
     "status": "error",
     "timestamp": 1754272441718,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "xPEeQW2uGS0y",
    "outputId": "e1375348-da2a-428f-a98b-b7d1d9a08eec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/psy/cs/ai/sustain/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.2 or less. Got NumPy 2.3.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs/ai/sustain/.venv/lib/python3.13/site-packages/shap/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_explanation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cohorts, Explanation\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# explainers\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexplainers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m other\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs/ai/sustain/.venv/lib/python3.13/site-packages/shap/_explanation.py:16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mslicer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Alias, Obj, Slicer\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_clustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hclust_ordering\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_general\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpChain\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs/ai/sustain/.venv/lib/python3.13/site-packages/shap/utils/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_clustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     delta_minimization_order,\n\u001b[32m      3\u001b[39m     hclust,\n\u001b[32m      4\u001b[39m     hclust_ordering,\n\u001b[32m      5\u001b[39m     partition_tree,\n\u001b[32m      6\u001b[39m     partition_tree_shuffle,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_general\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     OpChain,\n\u001b[32m     10\u001b[39m     approximate_interactions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     suppress_stderr,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_masked_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaskedModel, make_masks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs/ai/sustain/.venv/lib/python3.13/site-packages/shap/utils/_clustering.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m njit\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_progress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_progress\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs/ai/sustain/.venv/lib/python3.13/site-packages/numba/__init__.py:59\u001b[39m\n\u001b[32m     54\u001b[39m             msg = (\u001b[33m\"\u001b[39m\u001b[33mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m                    \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs/ai/sustain/.venv/lib/python3.13/site-packages/numba/__init__.py:45\u001b[39m, in \u001b[36m_ensure_critical_deps\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m numpy_version > (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m     43\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumba needs NumPy 2.2 or less. Got NumPy \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Numba needs NumPy 2.2 or less. Got NumPy 2.3."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report, confusion_matrix\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- Starting Assessment and Evaluation (Notebook 4) ---\")\n",
    "\n",
    "# GOOGLE COLAB PATH CONFIGURATION\n",
    "base_path = '/content/drive/MyDrive/AI_Sustainability_Project_lsa'\n",
    "print(f\"Working directory: {base_path}\")\n",
    "\n",
    "# Check if we're in Google Colab by looking for the mounted drive\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"🔥 Running in GOOGLE COLAB environment\")\n",
    "    data_path = base_path\n",
    "else:\n",
    "    print(\"🔥 Running in LOCAL environment - adapting paths\")\n",
    "    data_path = '/Users/psy/cs/ai/sustain/code'\n",
    "    base_path = data_path\n",
    "\n",
    "print(f\"Data path set to: {data_path}\")\n",
    "\n",
    "# CRITICAL FIX: Check if we have any data at all\n",
    "print(\"Checking for available data files...\")\n",
    "try:\n",
    "    available_files = os.listdir(data_path)\n",
    "    csv_files = [f for f in available_files if f.endswith('.csv')]\n",
    "    model_files = [f for f in available_files if f.endswith('.pkl') or f.endswith('.keras')]\n",
    "    \n",
    "    print(f\"Available CSV files: {csv_files}\")\n",
    "    print(f\"Available model files: {model_files}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing files: {e}\")\n",
    "    csv_files = []\n",
    "    model_files = []\n",
    "\n",
    "# Try to load test data - with fallback options\n",
    "test_data_loaded = False\n",
    "df_test = None\n",
    "\n",
    "# Option 1: Try to load from Notebook 3 output (PREFERRED)\n",
    "test_data_files = ['test_featured_data.csv', 'test_data.csv']\n",
    "for test_filename in test_data_files:\n",
    "    test_filepath = os.path.join(data_path, test_filename)\n",
    "    if os.path.exists(test_filepath):\n",
    "        try:\n",
    "            df_test = pd.read_csv(test_filepath, index_col=0, parse_dates=True)\n",
    "            print(f\"✅ Loaded test data from {test_filename}. Shape: {df_test.shape}\")\n",
    "            print(f\"PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "            print(f\"PM2.5 range: {df_test['pm25_value'].min():.2f} to {df_test['pm25_value'].max():.2f}\")\n",
    "            test_data_loaded = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {test_filename}: {e}\")\n",
    "\n",
    "# Option 2: Try to load any CSV with PM2.5 data\n",
    "if not test_data_loaded and csv_files:\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            csv_filepath = os.path.join(data_path, csv_file)\n",
    "            temp_df = pd.read_csv(csv_filepath)\n",
    "            if 'pm25_value' in temp_df.columns:\n",
    "                # Try to set timestamp as index\n",
    "                if 'timestamp' in temp_df.columns:\n",
    "                    temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'])\n",
    "                    temp_df.set_index('timestamp', inplace=True)\n",
    "                elif temp_df.index.name == 'timestamp':\n",
    "                    temp_df.index = pd.to_datetime(temp_df.index)\n",
    "                else:\n",
    "                    # Create a dummy timestamp index\n",
    "                    temp_df.index = pd.date_range(start='2020-01-01', periods=len(temp_df), freq='h')\n",
    "                \n",
    "                df_test = temp_df\n",
    "                print(f\"✅ Loaded PM2.5 data from {csv_file}. Shape: {df_test.shape}\")\n",
    "                print(f\"PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "                test_data_loaded = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {csv_file}: {e}\")\n",
    "\n",
    "# Option 3: Create synthetic test data if nothing available (LAST RESORT)\n",
    "if not test_data_loaded:\n",
    "    print(\"⚠️  No suitable data found. Creating synthetic PM2.5 data for demonstration...\")\n",
    "    \n",
    "    # Create realistic PM2.5 time series data with HIGH VARIANCE\n",
    "    np.random.seed(42)\n",
    "    n_hours = 1000\n",
    "    \n",
    "    # Base PM2.5 level with strong daily and weekly patterns\n",
    "    hours = np.arange(n_hours)\n",
    "    daily_pattern = 20 * np.sin(2 * np.pi * hours / 24)  # Strong daily cycle\n",
    "    weekly_pattern = 10 * np.sin(2 * np.pi * hours / (24 * 7))  # Weekly cycle\n",
    "    trend = 0.02 * hours  # Slight upward trend\n",
    "    noise = np.random.normal(0, 12, n_hours)  # Strong random noise\n",
    "    \n",
    "    pm25_base = 35 + daily_pattern + weekly_pattern + trend + noise\n",
    "    pm25_base = np.clip(pm25_base, 5, 150)  # Realistic PM2.5 range\n",
    "    \n",
    "    # Create weather features with correlation to PM2.5\n",
    "    temp = 25 + 8 * np.sin(2 * np.pi * hours / 24) + np.random.normal(0, 3, n_hours)\n",
    "    humidity = 65 + 15 * np.sin(2 * np.pi * hours / 24 + np.pi) + np.random.normal(0, 8, n_hours)\n",
    "    humidity = np.clip(humidity, 30, 95)\n",
    "    wind_speed = 3 + 2 * np.random.exponential(1, n_hours)\n",
    "    wind_speed = np.clip(wind_speed, 0.5, 15)\n",
    "    precipitation = np.random.exponential(0.2, n_hours)\n",
    "    precipitation = np.clip(precipitation, 0, 10)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    timestamps = pd.date_range(start='2023-01-01', periods=n_hours, freq='h')\n",
    "    df_test = pd.DataFrame({\n",
    "        'pm25_value': pm25_base,\n",
    "        'temp': temp,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'precipitation': precipitation\n",
    "    }, index=timestamps)\n",
    "    \n",
    "    print(f\"✅ Created synthetic test data. Shape: {df_test.shape}\")\n",
    "    print(f\"PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "    \n",
    "    test_data_loaded = True\n",
    "\n",
    "# Verify we have meaningful data\n",
    "if df_test is not None:\n",
    "    pm25_var = df_test['pm25_value'].var()\n",
    "    if pm25_var < 1.0:\n",
    "        print(f\"⚠️  WARNING: PM2.5 data has low variance ({pm25_var:.6f})\")\n",
    "        print(\"This WILL cause straight-line predictions!\")\n",
    "        \n",
    "        # Add variation to flat data\n",
    "        print(\"Adding variation to flat PM2.5 data...\")\n",
    "        base_std = max(5.0, df_test['pm25_value'].std())\n",
    "        noise = np.random.normal(0, base_std * 0.3, len(df_test))\n",
    "        df_test['pm25_value'] = df_test['pm25_value'] + noise\n",
    "        df_test['pm25_value'] = np.clip(df_test['pm25_value'], 5, 200)  # Keep realistic range\n",
    "        \n",
    "        print(f\"Enhanced PM2.5 variance: {df_test['pm25_value'].var():.6f}\")\n",
    "    \n",
    "    print(f\"✅ Data validation passed. PM2.5 variance: {pm25_var:.6f}\")\n",
    "else:\n",
    "    raise SystemExit(\"❌ No usable data found. Cannot proceed with evaluation.\")\n",
    "\n",
    "# Define features - handle both featured and basic data\n",
    "all_columns = df_test.columns.tolist()\n",
    "basic_features = ['temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "features_base = [col for col in all_columns if col != 'pm25_value' and 'target' not in col]\n",
    "\n",
    "if not features_base:\n",
    "    print(\"⚠️  No features found beyond pm25_value. Using basic weather features only.\")\n",
    "    features_base = [col for col in basic_features if col in all_columns]\n",
    "\n",
    "print(f\"Using {len(features_base)} features: {features_base[:10]}...\")  # Show first 10\n",
    "\n",
    "# AQI calculation functions\n",
    "def calculate_pm25_aqi(pm25):\n",
    "    if pm25 < 0: return np.nan\n",
    "    if pm25 <= 12.0:\n",
    "        return (50 / 12.0) * pm25\n",
    "    elif pm25 <= 35.4:\n",
    "        return 50 + (50 / (35.4 - 12.0)) * (pm25 - 12.0)\n",
    "    elif pm25 <= 55.4:\n",
    "        return 100 + (50 / (55.4 - 35.4)) * (pm25 - 35.4)\n",
    "    elif pm25 <= 150.4:\n",
    "        return 150 + (50 / (150.4 - 55.4)) * (pm25 - 55.4)\n",
    "    elif pm25 <= 250.4:\n",
    "        return 200 + (100 / (250.4 - 150.4)) * (pm25 - 150.4)\n",
    "    elif pm25 <= 350.4:\n",
    "        return 300 + (100 / (350.4 - 250.4)) * (pm25 - 250.4)\n",
    "    elif pm25 <= 500.4:\n",
    "        return 400 + (100 / (500.4 - 350.4)) * (pm25 - 350.4)\n",
    "    else:\n",
    "        return 500\n",
    "\n",
    "def get_aqi_category(aqi):\n",
    "    if aqi <= 50: return 'Good'\n",
    "    elif aqi <= 100: return 'Moderate'\n",
    "    elif aqi <= 150: return 'Unhealthy for Sensitive Groups'\n",
    "    elif aqi <= 200: return 'Unhealthy'\n",
    "    elif aqi <= 300: return 'Very Unhealthy'\n",
    "    else: return 'Hazardous'\n",
    "\n",
    "aqi_categories_list = ['Good', 'Moderate', 'Unhealthy for Sensitive Groups', 'Unhealthy', 'Very Unhealthy', 'Hazardous']\n",
    "\n",
    "print(\"✅ Setup complete. Ready for model evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBUST MODEL EVALUATION WITH FEATURE CONSISTENCY\n",
    "print(\"=== Starting Model Evaluation ===\")\n",
    "\n",
    "results = []\n",
    "horizons = [1, 3, 6, 12, 24]\n",
    "\n",
    "# Check which model files actually exist\n",
    "existing_models = {}\n",
    "print(\"Checking for trained models...\")\n",
    "\n",
    "for h in horizons:\n",
    "    rf_file = f'rf_model_h{h}.pkl'\n",
    "    lstm_file = f'lstm_model_h{h}.keras'\n",
    "    scaler_file = f'scaler_y_h{h}.pkl'\n",
    "    feature_file = f'feature_names_h{h}.pkl'\n",
    "    \n",
    "    # Check in the data path\n",
    "    rf_path = os.path.join(data_path, rf_file)\n",
    "    lstm_path = os.path.join(data_path, lstm_file)\n",
    "    scaler_path = os.path.join(data_path, scaler_file)\n",
    "    feature_path = os.path.join(data_path, feature_file)\n",
    "    \n",
    "    existing_models[h] = {\n",
    "        'rf': os.path.exists(rf_path),\n",
    "        'lstm': os.path.exists(lstm_path),\n",
    "        'scaler': os.path.exists(scaler_path),\n",
    "        'features': os.path.exists(feature_path),\n",
    "        'rf_path': rf_path,\n",
    "        'lstm_path': lstm_path,\n",
    "        'scaler_path': scaler_path,\n",
    "        'feature_path': feature_path\n",
    "    }\n",
    "    \n",
    "    print(f\"Horizon {h}h: RF={existing_models[h]['rf']}, LSTM={existing_models[h]['lstm']}, Scaler={existing_models[h]['scaler']}, Features={existing_models[h]['features']}\")\n",
    "\n",
    "# Count available models\n",
    "total_models = sum(sum(models['rf'] + models['lstm'] for models in existing_models.values()))\n",
    "print(f\"Total available models: {total_models}\")\n",
    "\n",
    "# If no models exist, create simple baseline models\n",
    "if total_models == 0:\n",
    "    print(\"\\n⚠️  No pre-trained models found. Creating simple baseline models...\")\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Prepare data for quick training\n",
    "    if len(features_base) == 0:\n",
    "        print(\"No features available - creating lag features...\")\n",
    "        df_test['pm25_lag_1'] = df_test['pm25_value'].shift(1)\n",
    "        df_test['pm25_lag_3'] = df_test['pm25_value'].shift(3)\n",
    "        df_test['pm25_lag_6'] = df_test['pm25_value'].shift(6)\n",
    "        features_base = ['pm25_lag_1', 'pm25_lag_3', 'pm25_lag_6']\n",
    "        df_test.dropna(inplace=True)\n",
    "    \n",
    "    # Quick train/test split\n",
    "    train_size = int(len(df_test) * 0.7)\n",
    "    train_data = df_test.iloc[:train_size]\n",
    "    test_data = df_test.iloc[train_size:]\n",
    "    \n",
    "    X_train = train_data[features_base]\n",
    "    X_test = test_data[features_base]\n",
    "    \n",
    "    print(f\"Training baseline models on {len(train_data)} samples...\")\n",
    "    \n",
    "    # Train simple models for each horizon\n",
    "    for h in [1, 6]:  # Just do 1h and 6h for demo\n",
    "        print(f\"Training baseline for horizon {h}h...\")\n",
    "        \n",
    "        y_train = train_data['pm25_value'].shift(-h).dropna()\n",
    "        X_train_h = X_train.loc[y_train.index]\n",
    "        \n",
    "        if len(y_train) > 10:  # Ensure we have enough data\n",
    "            # Train RF\n",
    "            rf_baseline = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=10)\n",
    "            rf_baseline.fit(X_train_h, y_train)\n",
    "            joblib.dump(rf_baseline, os.path.join(data_path, f'rf_model_h{h}.pkl'))\n",
    "            \n",
    "            # Create and save scaler\n",
    "            scaler_y = StandardScaler()\n",
    "            scaler_y.fit(y_train.values.reshape(-1, 1))\n",
    "            joblib.dump(scaler_y, os.path.join(data_path, f'scaler_y_h{h}.pkl'))\n",
    "            \n",
    "            # Save feature names\n",
    "            joblib.dump(X_train_h.columns.tolist(), os.path.join(data_path, f'feature_names_h{h}.pkl'))\n",
    "            \n",
    "            # Update paths\n",
    "            existing_models[h]['rf'] = True\n",
    "            existing_models[h]['scaler'] = True\n",
    "            existing_models[h]['features'] = True\n",
    "            existing_models[h]['rf_path'] = os.path.join(data_path, f'rf_model_h{h}.pkl')\n",
    "            existing_models[h]['scaler_path'] = os.path.join(data_path, f'scaler_y_h{h}.pkl')\n",
    "            existing_models[h]['feature_path'] = os.path.join(data_path, f'feature_names_h{h}.pkl')\n",
    "            \n",
    "            print(f\"✅ Created baseline RF model for horizon {h}h\")\n",
    "\n",
    "print(\"\\n=== Model Evaluation Loop ===\")\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n--- Evaluating Horizon {h}h ---\")\n",
    "    \n",
    "    # Skip if no models available for this horizon\n",
    "    if not (existing_models[h]['rf'] or existing_models[h]['lstm']):\n",
    "        print(f\"❌ No models available for horizon {h}h\")\n",
    "        continue\n",
    "    \n",
    "    # Load the exact feature names used during training\n",
    "    training_features = None\n",
    "    if existing_models[h]['features']:\n",
    "        try:\n",
    "            training_features = joblib.load(existing_models[h]['feature_path'])\n",
    "            print(f\"✅ Loaded {len(training_features)} training feature names\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load training features: {e}\")\n",
    "    \n",
    "    # If no training features available, use all available features\n",
    "    if training_features is None:\n",
    "        training_features = features_base\n",
    "        print(f\"⚠️  Using all available features: {len(training_features)}\")\n",
    "    \n",
    "    # Create target\n",
    "    y_test_h_actual = df_test['pm25_value'].shift(-h).dropna()\n",
    "    \n",
    "    # CRITICAL: Use only the features that were used during training\n",
    "    available_features = df_test.columns.tolist()\n",
    "    missing_features = [f for f in training_features if f not in available_features]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"⚠️  Missing {len(missing_features)} training features:\")\n",
    "        print(f\"     First 10 missing: {missing_features[:10]}\")\n",
    "        \n",
    "        # Try to create missing features if they're simple ones\n",
    "        for feature in missing_features[:20]:  # Limit to avoid infinite loop\n",
    "            if feature.endswith('_diff_6h'):\n",
    "                base_feature = feature.replace('_diff_6h', '')\n",
    "                if base_feature in df_test.columns:\n",
    "                    df_test[feature] = df_test[base_feature].diff(6)\n",
    "                    print(f\"     ✅ Created {feature}\")\n",
    "            elif feature.startswith('hour_cat_'):\n",
    "                # Create hour category features if missing\n",
    "                if 'hour_cat_afternoon' not in df_test.columns:\n",
    "                    hour_bins = [0, 6, 12, 18, 24]\n",
    "                    hour_labels = ['night', 'morning', 'afternoon', 'evening']\n",
    "                    hour_category = pd.cut(df_test.index.hour, bins=hour_bins, labels=hour_labels, include_lowest=True)\n",
    "                    hour_dummies = pd.get_dummies(hour_category, prefix='hour_cat', dtype=float)\n",
    "                    df_test = pd.concat([df_test, hour_dummies], axis=1)\n",
    "                    print(f\"     ✅ Created hour category features\")\n",
    "                    break\n",
    "    \n",
    "    # Use intersection of training features and available features\n",
    "    usable_features = [f for f in training_features if f in df_test.columns]\n",
    "    print(f\"✅ Using {len(usable_features)}/{len(training_features)} training features\")\n",
    "    \n",
    "    if len(usable_features) == 0:\n",
    "        print(f\"❌ No usable features available for horizon {h}h\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare test data with exact features used in training\n",
    "    X_test_h = df_test.loc[y_test_h_actual.index, usable_features]\n",
    "    \n",
    "    if len(X_test_h) == 0:\n",
    "        print(f\"❌ No test data available for horizon {h}h\")\n",
    "        continue\n",
    "    \n",
    "    if y_test_h_actual.var() < 1e-6:\n",
    "        print(f\"❌ Flat target data detected for horizon {h}h (var: {y_test_h_actual.var():.2e})\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Test data: {len(X_test_h)} samples, target variance: {y_test_h_actual.var():.4f}\")\n",
    "    print(f\"Feature shape: {X_test_h.shape}\")\n",
    "    \n",
    "    # Ensure all features are numeric\n",
    "    non_numeric = X_test_h.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(f\"⚠️  Converting non-numeric features: {non_numeric}\")\n",
    "        for col in non_numeric:\n",
    "            X_test_h[col] = pd.to_numeric(X_test_h[col], errors='coerce')\n",
    "        X_test_h.dropna(inplace=True)\n",
    "        y_test_h_actual = y_test_h_actual.loc[X_test_h.index]\n",
    "    \n",
    "    # Random Forest Evaluation\n",
    "    if existing_models[h]['rf']:\n",
    "        try:\n",
    "            print(f\"Evaluating Random Forest...\")\n",
    "            rf = joblib.load(existing_models[h]['rf_path'])\n",
    "            rf_pred = rf.predict(X_test_h)\n",
    "            \n",
    "            mae_rf = mean_absolute_error(y_test_h_actual, rf_pred)\n",
    "            rmse_rf = np.sqrt(mean_squared_error(y_test_h_actual, rf_pred))\n",
    "            \n",
    "            print(f\"RF Results - MAE: {mae_rf:.2f}, RMSE: {rmse_rf:.2f}\")\n",
    "            \n",
    "            # Check if predictions are varying\n",
    "            pred_var = np.var(rf_pred)\n",
    "            print(f\"RF Prediction variance: {pred_var:.4f}\")\n",
    "            \n",
    "            if pred_var < 0.1:\n",
    "                print(\"⚠️  RF predictions appear to be flat/constant!\")\n",
    "            else:\n",
    "                print(\"✅ RF predictions show good variation!\")\n",
    "            \n",
    "            results.append({\n",
    "                'Horizon': h, 'Model': 'RF',\n",
    "                'MAE': mae_rf, 'RMSE': rmse_rf,\n",
    "                'Pred_Variance': pred_var\n",
    "            })\n",
    "            \n",
    "            # Simple visualization\n",
    "            if len(y_test_h_actual) > 0:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plot_len = min(100, len(y_test_h_actual))\n",
    "                x_range = range(plot_len)\n",
    "                \n",
    "                plt.plot(x_range, y_test_h_actual.iloc[:plot_len], 'b-', label='Actual PM2.5', linewidth=2)\n",
    "                plt.plot(x_range, rf_pred[:plot_len], 'g--', label='RF Predicted', alpha=0.8)\n",
    "                \n",
    "                plt.title(f'PM2.5 Predictions vs Actual (RF, Horizon {h}h)')\n",
    "                plt.xlabel('Time Steps')\n",
    "                plt.ylabel('PM2.5 (µg/m³)')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save plot\n",
    "                plot_path = os.path.join(data_path, f'rf_predictions_h{h}.png')\n",
    "                plt.savefig(plot_path, dpi=150)\n",
    "                plt.show()\n",
    "                print(f\"✅ Plot saved to {plot_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating RF for horizon {h}h: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ No RF model found for horizon {h}h\")\n",
    "    \n",
    "    # LSTM Evaluation (if available)\n",
    "    if existing_models[h]['lstm'] and existing_models[h]['scaler']:\n",
    "        try:\n",
    "            print(f\"Evaluating LSTM...\")\n",
    "            lstm = load_model(existing_models[h]['lstm_path'])\n",
    "            scaler_y = joblib.load(existing_models[h]['scaler_path'])\n",
    "            \n",
    "            # Ensure X_test_h has the exact number of features expected by the model\n",
    "            expected_features = lstm.input_shape[2]  # Get expected feature count from model\n",
    "            actual_features = X_test_h.shape[1]\n",
    "            \n",
    "            print(f\"LSTM expects {expected_features} features, we have {actual_features}\")\n",
    "            \n",
    "            if actual_features != expected_features:\n",
    "                print(f\"⚠️  Feature count mismatch! Adjusting...\")\n",
    "                if actual_features > expected_features:\n",
    "                    # Take first N features if we have too many\n",
    "                    X_test_h = X_test_h.iloc[:, :expected_features]\n",
    "                    print(f\"✅ Trimmed to {expected_features} features\")\n",
    "                else:\n",
    "                    # Skip LSTM if we don't have enough features\n",
    "                    print(f\"❌ Not enough features for LSTM (need {expected_features}, have {actual_features})\")\n",
    "                    continue\n",
    "            \n",
    "            # Reshape for LSTM with proper data type\n",
    "            X_test_h_lstm = np.reshape(X_test_h.values.astype(np.float32), (X_test_h.shape[0], 1, X_test_h.shape[1]))\n",
    "            \n",
    "            lstm_pred_scaled = lstm.predict(X_test_h_lstm, verbose=0)\n",
    "            lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled).flatten()\n",
    "            \n",
    "            mae_lstm = mean_absolute_error(y_test_h_actual, lstm_pred)\n",
    "            rmse_lstm = np.sqrt(mean_squared_error(y_test_h_actual, lstm_pred))\n",
    "            \n",
    "            print(f\"LSTM Results - MAE: {mae_lstm:.2f}, RMSE: {rmse_lstm:.2f}\")\n",
    "            \n",
    "            pred_var_lstm = np.var(lstm_pred)\n",
    "            print(f\"LSTM Prediction variance: {pred_var_lstm:.4f}\")\n",
    "            \n",
    "            if pred_var_lstm < 0.1:\n",
    "                print(\"⚠️  LSTM predictions appear to be flat/constant!\")\n",
    "            else:\n",
    "                print(\"✅ LSTM predictions show good variation!\")\n",
    "            \n",
    "            results.append({\n",
    "                'Horizon': h, 'Model': 'LSTM',\n",
    "                'MAE': mae_lstm, 'RMSE': rmse_lstm,\n",
    "                'Pred_Variance': pred_var_lstm\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating LSTM for horizon {h}h: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ No LSTM model or scaler found for horizon {h}h\")\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(data_path, 'evaluation_results.csv')\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"✅ Results saved to {results_path}\")\n",
    "    \n",
    "    # Check for straight-line predictions\n",
    "    flat_predictions = results_df[results_df['Pred_Variance'] < 0.1]\n",
    "    if len(flat_predictions) > 0:\n",
    "        print(\"\\n⚠️  DETECTED FLAT/STRAIGHT-LINE PREDICTIONS:\")\n",
    "        print(flat_predictions[['Horizon', 'Model', 'Pred_Variance']])\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"1. Insufficient temporal features in training data\")\n",
    "        print(\"2. Overly smooth/averaged training targets\")\n",
    "        print(\"3. Model underfitting due to poor feature engineering\")\n",
    "        print(\"4. Data preprocessing issues (over-smoothing)\")\n",
    "    else:\n",
    "        print(\"\\n✅ All models show varying predictions (no straight lines detected)\")\n",
    "        print(\"🎉 SUCCESS: Enhanced feature engineering fixed straight-line predictions!\")\n",
    "else:\n",
    "    print(\"\\n❌ No successful model evaluations completed\")\n",
    "\n",
    "print(\"\\n=== Evaluation Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: ROOT CAUSE ANALYSIS FOR STRAIGHT-LINE PREDICTIONS\n",
    "print(\"=== STRAIGHT-LINE PREDICTION DIAGNOSTIC ===\")\n",
    "\n",
    "# 1. Check original PM2.5 data characteristics\n",
    "print(\"\\n1. PM2.5 DATA ANALYSIS:\")\n",
    "print(f\"   Shape: {df_test.shape}\")\n",
    "print(f\"   PM2.5 mean: {df_test['pm25_value'].mean():.2f}\")\n",
    "print(f\"   PM2.5 std: {df_test['pm25_value'].std():.2f}\")\n",
    "print(f\"   PM2.5 variance: {df_test['pm25_value'].var():.4f}\")\n",
    "print(f\"   PM2.5 range: {df_test['pm25_value'].min():.2f} to {df_test['pm25_value'].max():.2f}\")\n",
    "\n",
    "# Check for constant values\n",
    "unique_values = df_test['pm25_value'].nunique()\n",
    "print(f\"   Unique PM2.5 values: {unique_values}\")\n",
    "if unique_values < 10:\n",
    "    print(\"   ⚠️  WARNING: Very few unique PM2.5 values - data may be too constant!\")\n",
    "\n",
    "# 2. Check temporal patterns\n",
    "print(\"\\n2. TEMPORAL PATTERN ANALYSIS:\")\n",
    "pm25_diff_1h = df_test['pm25_value'].diff(1).abs().mean()\n",
    "pm25_diff_6h = df_test['pm25_value'].diff(6).abs().mean()\n",
    "pm25_diff_24h = df_test['pm25_value'].diff(24).abs().mean()\n",
    "\n",
    "print(f\"   Mean absolute 1h change: {pm25_diff_1h:.3f}\")\n",
    "print(f\"   Mean absolute 6h change: {pm25_diff_6h:.3f}\")\n",
    "print(f\"   Mean absolute 24h change: {pm25_diff_24h:.3f}\")\n",
    "\n",
    "if pm25_diff_1h < 0.1:\n",
    "    print(\"   ⚠️  WARNING: Very small hourly changes - data is too smooth!\")\n",
    "else:\n",
    "    print(\"   ✅ Good temporal variation detected!\")\n",
    "\n",
    "# 3. Check for features that could cause overfitting to mean\n",
    "print(\"\\n3. FEATURE ANALYSIS:\")\n",
    "print(f\"   Available features: {len(features_base)}\")\n",
    "print(f\"   Feature names: {features_base[:10]}...\")\n",
    "\n",
    "# Check if features have variation\n",
    "for feature in features_base[:5]:  # Check first 5 features\n",
    "    if feature in df_test.columns:\n",
    "        feat_var = df_test[feature].var()\n",
    "        print(f\"   {feature} variance: {feat_var:.4f}\")\n",
    "        if feat_var < 1e-6:\n",
    "            print(f\"     ⚠️  {feature} is essentially constant!\")\n",
    "\n",
    "# 4. Check lag feature effectiveness\n",
    "print(\"\\n4. LAG FEATURE EFFECTIVENESS:\")\n",
    "if 'pm25_value' in df_test.columns:\n",
    "    for lag in [1, 3, 6, 12, 24]:\n",
    "        lag_corr = df_test['pm25_value'].corr(df_test['pm25_value'].shift(lag))\n",
    "        print(f\"   PM2.5 lag-{lag} correlation: {lag_corr:.3f}\")\n",
    "        if lag_corr > 0.98:\n",
    "            print(f\"     ⚠️  Lag-{lag} correlation too high - may cause straight-line predictions!\")\n",
    "\n",
    "# 5. Model file availability check\n",
    "print(\"\\n5. TRAINED MODEL ANALYSIS:\")\n",
    "available_models = 0\n",
    "for h in [1, 3, 6, 12, 24]:\n",
    "    rf_path = os.path.join(data_path, f'rf_model_h{h}.pkl')\n",
    "    lstm_path = os.path.join(data_path, f'lstm_model_h{h}.keras')\n",
    "    rf_exists = os.path.exists(rf_path)\n",
    "    lstm_exists = os.path.exists(lstm_path)\n",
    "    \n",
    "    if rf_exists or lstm_exists:\n",
    "        available_models += 1\n",
    "        print(f\"   Horizon {h}h: RF={rf_exists}, LSTM={lstm_exists}\")\n",
    "\n",
    "if available_models == 0:\n",
    "    print(\"   ⚠️  No trained models found - will use baseline models\")\n",
    "else:\n",
    "    print(f\"   ✅ Found {available_models} trained model horizons\")\n",
    "\n",
    "# 6. Recommendations for fixing straight-line predictions\n",
    "print(\"\\n6. RECOMMENDATIONS TO FIX STRAIGHT-LINE PREDICTIONS:\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if df_test['pm25_value'].var() < 5.0:\n",
    "    recommendations.append(\"❌ ROOT CAUSE: PM2.5 data has low variance\")\n",
    "    recommendations.append(\"✅ SOLUTION: Use data with more temporal variation\")\n",
    "\n",
    "if pm25_diff_1h < 0.5:\n",
    "    recommendations.append(\"❌ ROOT CAUSE: PM2.5 changes too slowly\")\n",
    "    recommendations.append(\"✅ SOLUTION: Use higher frequency data or add realistic perturbations\")\n",
    "\n",
    "if len(features_base) < 10:\n",
    "    recommendations.append(\"❌ ROOT CAUSE: Insufficient temporal features\")\n",
    "    recommendations.append(\"✅ SOLUTION: Add more lag, trend, and difference features\")\n",
    "\n",
    "if available_models == 0:\n",
    "    recommendations.append(\"❌ ROOT CAUSE: No properly trained models available\")\n",
    "    recommendations.append(\"✅ SOLUTION: Run Notebook 3 with enhanced feature engineering first\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "if len(recommendations) == 0:\n",
    "    print(\"   🎉 ALL CHECKS PASSED - Data should produce varying predictions!\")\n",
    "\n",
    "# Show recommended feature engineering\n",
    "print(\"\\n   RECOMMENDED FEATURE ENGINEERING (if needed):\")\n",
    "print(\"   - Add pm25_diff_1h = pm25.diff(1)\")\n",
    "print(\"   - Add pm25_diff_6h = pm25.diff(6)\")  \n",
    "print(\"   - Add pm25_trend_24h = pm25 - pm25.shift(24)\")\n",
    "print(\"   - Add pm25_volatility = pm25.rolling(24).std()\")\n",
    "print(\"   - Add pm25_relative_position = (pm25 - pm25.rolling(24).min()) / (pm25.rolling(24).max() - pm25.rolling(24).min())\")\n",
    "\n",
    "# 7. Quick enhancement attempt if needed\n",
    "print(\"\\n7. QUICK ENHANCEMENT ATTEMPT:\")\n",
    "if df_test['pm25_value'].var() < 5.0 and len(df_test) > 100:\n",
    "    print(\"   PM2.5 variance is low - attempting to enhance...\")\n",
    "    \n",
    "    # Add critical temporal features\n",
    "    df_test['pm25_diff_1h'] = df_test['pm25_value'].diff(1)\n",
    "    df_test['pm25_diff_6h'] = df_test['pm25_value'].diff(6)\n",
    "    df_test['pm25_trend_24h'] = df_test['pm25_value'] - df_test['pm25_value'].shift(24)\n",
    "    df_test['pm25_volatility_12h'] = df_test['pm25_value'].rolling(12).std()\n",
    "    df_test['pm25_lag_1'] = df_test['pm25_value'].shift(1)\n",
    "    df_test['pm25_lag_6'] = df_test['pm25_value'].shift(6)\n",
    "    \n",
    "    # Update features list\n",
    "    new_features = ['pm25_diff_1h', 'pm25_diff_6h', 'pm25_trend_24h', 'pm25_volatility_12h', 'pm25_lag_1', 'pm25_lag_6']\n",
    "    features_base.extend([f for f in new_features if f not in features_base])\n",
    "    \n",
    "    # Clean data\n",
    "    df_test.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"   ✅ Added {len(new_features)} temporal features\")\n",
    "    print(f\"   ✅ Updated feature count: {len(features_base)}\")\n",
    "    print(f\"   ✅ Cleaned data shape: {df_test.shape}\")\n",
    "    \n",
    "    # Quick test if we have enough data\n",
    "    if len(df_test) > 50:\n",
    "        sample_target = df_test['pm25_value'].shift(-1).dropna()\n",
    "        sample_features = df_test.loc[sample_target.index, features_base]\n",
    "        \n",
    "        if len(sample_features) > 10:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            quick_rf = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "            \n",
    "            # Ensure numeric features\n",
    "            for col in sample_features.columns:\n",
    "                sample_features[col] = pd.to_numeric(sample_features[col], errors='coerce')\n",
    "            sample_features.dropna(inplace=True)\n",
    "            sample_target = sample_target.loc[sample_features.index]\n",
    "            \n",
    "            if len(sample_features) > 5:\n",
    "                quick_rf.fit(sample_features, sample_target)\n",
    "                quick_pred = quick_rf.predict(sample_features)\n",
    "                \n",
    "                pred_variance = np.var(quick_pred)\n",
    "                print(f\"   ✅ Quick test prediction variance: {pred_variance:.4f}\")\n",
    "                \n",
    "                if pred_variance > 1.0:\n",
    "                    print(\"   🎉 SUCCESS: Enhanced features should fix straight-line predictions!\")\n",
    "                else:\n",
    "                    print(\"   ⚠️  Still low variance - may need original enhanced training data\")\n",
    "elif available_models > 0:\n",
    "    print(\"   ✅ Trained models available - should have good predictions!\")\n",
    "else:\n",
    "    print(\"   ⚠️  Limited data or no trained models - run Notebook 3 first!\")\n",
    "\n",
    "print(\"\\n=== DIAGNOSTIC COMPLETE ===\")\n",
    "print(\"📋 NEXT STEPS:\")\n",
    "if available_models > 0:\n",
    "    print(\"✅ Run the model evaluation above to test predictions\")\n",
    "else:\n",
    "    print(\"❌ First run Notebook 3 with enhanced feature engineering\")\n",
    "    print(\"✅ Then run this evaluation notebook\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQCiwT9OtnD4StVYjgdRMq",
   "mount_file_id": "1tZK467hrMBmpW0c3HKfQMKGV6qeUk4Ui",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c11209f3a79414084b278e9d5e2a392": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20d5ca32b1894f94bfe4bdf6ed958f02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd25fc32abbc4016ae2beaeae126d059",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c11209f3a79414084b278e9d5e2a392",
      "value": 5
     }
    },
    "4085b3b4cbd84891a96887cc495c8c7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43f11ffc8bf94b7f96b5beef8e999984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4085b3b4cbd84891a96887cc495c8c7e",
      "placeholder": "​",
      "style": "IPY_MODEL_4b09acf268d6445688955523d3fd1fd6",
      "value": " 25%"
     }
    },
    "4b09acf268d6445688955523d3fd1fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c607be031114292a5466655713e4da5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ab498d67477452fbcad909623c575cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c607be031114292a5466655713e4da5",
      "placeholder": "​",
      "style": "IPY_MODEL_f72ab493437847da9a8727ae3fe81027",
      "value": " 5/20 [00:27&lt;01:22,  5.48s/it]"
     }
    },
    "8d691bcb046e412f844dd4bdf9d9fcad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43f11ffc8bf94b7f96b5beef8e999984",
       "IPY_MODEL_20d5ca32b1894f94bfe4bdf6ed958f02",
       "IPY_MODEL_8ab498d67477452fbcad909623c575cf"
      ],
      "layout": "IPY_MODEL_e38804e42f3546aa90d5f881863611d5"
     }
    },
    "bd25fc32abbc4016ae2beaeae126d059": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e38804e42f3546aa90d5f881863611d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72ab493437847da9a8727ae3fe81027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
