{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnunl16wIhcM"
   },
   "source": [
    "# Notebook 3: AI Modelling\n",
    "## Introduction\n",
    "# Loads data from Notebook 1, adds features, trains RF and LSTM for horizons 1,3,6,12,24h.\n",
    "# Justification: RF for non-linear feature importance; LSTM for temporal sequences. Horizons align with real-time forecasting needs. TimeSeriesSplit prevents data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1754271587557,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "5E5GpOToYGTW",
    "outputId": "0fc4c91b-afab-4f9f-ce0b-273ebb70512d"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define your project folder in Google Drive\n",
    "your_project_path = '/content/drive/My Drive/AI_Sustainability_Project_lsa'\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(your_project_path, exist_ok=True)\n",
    "print(f\"Project path set to: {your_project_path}\")\n",
    "\n",
    "# Change current working directory to your project path\n",
    "%cd \"{your_project_path}\"\n",
    "\n",
    "# Verify current working directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "error",
     "timestamp": 1754272234814,
     "user": {
      "displayName": "SOM KAPOOR",
      "userId": "07316503827781082392"
     },
     "user_tz": -60
    },
    "id": "gbG5Cj0lGHQt",
    "outputId": "184a6246-bb42-4af5-9844-7a4b16cb60c2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress SettingWithCopyWarning for cleaner output\n",
    "\n",
    "# Mount Google Drive (essential for Colab to access saved files)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the full path for the processed data generated by Notebook 1\n",
    "input_data_path = '/content/drive/MyDrive/AI_Sustainability_Project_lsa/sensor_12178556_Singapore_pm25_weather_hourly_data_processed_final.csv'\n",
    "\n",
    "print(f\"--- Starting AI Modelling (Notebook 3) ---\")\n",
    "print(f\"Loading pre-processed data from: {input_data_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the processed data from Google Drive\n",
    "    df = pd.read_csv(input_data_path, index_col='timestamp', parse_dates=True)\n",
    "    print(f\"Data loaded successfully. Initial shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from {input_data_path}: {e}\")\n",
    "    print(\"Please ensure Notebook 1 has run successfully and the file exists at the specified path in Google Drive.\")\n",
    "    raise SystemExit(\"Failed to load pre-processed data. Aborting Notebook 3 execution.\")\n",
    "\n",
    "\n",
    "# --- FIX: STEP 1: Feature Engineering on the entire DataFrame first ---\n",
    "def add_features(data_df):\n",
    "    \"\"\"This function now runs on the whole dataset to ensure continuity of features.\"\"\"\n",
    "    df_featured = data_df.copy() # Work on a copy\n",
    "    lags = [1, 3, 6, 12, 24, 48]\n",
    "    features_to_lag = ['pm25_value', 'temp', 'humidity', 'wind_speed', 'precipitation']\n",
    "    for feature in features_to_lag:\n",
    "        for lag in lags:\n",
    "            df_featured[f'{feature}_lag_{lag}'] = df_featured[feature].shift(lag)\n",
    "\n",
    "    df_featured['pm25_rolling_24'] = df_featured['pm25_value'].rolling(window=24, min_periods=1).mean()\n",
    "    df_featured['temp_rolling_24'] = df_featured['temp'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "    df_featured['hour_sin'] = np.sin(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['hour_cos'] = np.cos(2 * np.pi * df_featured.index.hour / 24)\n",
    "    df_featured['day_of_week_sin'] = np.sin(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "    df_featured['day_of_week_cos'] = np.cos(2 * np.pi * df_featured.index.dayofweek / 7)\n",
    "\n",
    "    df_featured['wind_humidity_interaction'] = df_featured['wind_speed'] * df_featured['humidity']\n",
    "\n",
    "    # Drop rows with NaNs that were created by the lag/roll operations at the beginning\n",
    "    df_featured.dropna(inplace=True)\n",
    "    return df_featured\n",
    "\n",
    "print(\"\\n--- Adding Features to the entire dataset --\")\n",
    "df_featured = add_features(df)\n",
    "print(f\"Shape after features and cleaning: {df_featured.shape}\")\n",
    "\n",
    "\n",
    "# --- FIX: STEP 2: Chronological Train/Test Split on the *featured* DataFrame ---\n",
    "print(\"\\n--- Performing Chronological Train/Test Split --\")\n",
    "train_size = int(len(df_featured) * 0.8)\n",
    "train_df = df_featured.iloc[:train_size].copy()\n",
    "test_df = df_featured.iloc[train_size:].copy()\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "\n",
    "# Define features list (it now includes the engineered features)\n",
    "features_for_scaling = [col for col in train_df.columns if col != 'pm25_value' and 'target' not in col]\n",
    "\n",
    "\n",
    "# --- FIX: STEP 3: Scaling (Fit on Train, Transform Both) - This part was already correct ---\n",
    "print(\"\\n--- Scaling Features (MinMaxScaler on Train) ---\")\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df[features_for_scaling] = scaler_x.fit_transform(train_df[features_for_scaling])\n",
    "test_df[features_for_scaling] = scaler_x.transform(test_df[features_for_scaling])\n",
    "joblib.dump(scaler_x, '/content/drive/MyDrive/AI_Sustainability_Project_lsa/scaler_x.pkl')\n",
    "print(f\"Features scaled. Scaler saved to '/content/drive/MyDrive/AI_Sustainability_Project_lsa/scaler_x.pkl'.\")\n",
    "\n",
    "\n",
    "# --- Model Training Loop for Multiple Horizons (Logic remains the same) ---\n",
    "horizons = [1, 3, 6, 12, 24]\n",
    "print(f\"\\n--- Training Models for Horizons: {horizons} ---\")\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n--- Processing Horizon: {h} hours ---\")\n",
    "\n",
    "    # Create target variable by shifting the original pm25_value\n",
    "    train_df['target_h'] = train_df['pm25_value'].shift(-h)\n",
    "    test_df['target_h'] = test_df['pm25_value'].shift(-h)\n",
    "\n",
    "    # Drop rows where the shifted target is now NaN (at the end of each dataframe)\n",
    "    train_h = train_df.dropna(subset=['target_h'])\n",
    "    test_h = test_df.dropna(subset=['target_h'])\n",
    "\n",
    "    X_train = train_h[features_for_scaling]\n",
    "    y_train = train_h['target_h']\n",
    "    X_test = test_h[features_for_scaling]\n",
    "    y_test = test_h['target_h']\n",
    "\n",
    "    # Scale target for LSTM only\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    joblib.dump(scaler_y, f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/scaler_y_h{h}.pkl')\n",
    "    print(f\"Target scaled for LSTM (horizon {h}). Scaler saved.\")\n",
    "\n",
    "    # Time Series Split for Cross-Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Random Forest Regressor\n",
    "    print(f\"Training RandomForestRegressor for horizon {h}...\")\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    param_dist_rf = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    rf_search = RandomizedSearchCV(rf, param_dist_rf, cv=tscv, scoring='neg_mean_squared_error', n_iter=5, verbose=0, random_state=42)\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    joblib.dump(rf_search.best_estimator_, f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/rf_model_h{h}.pkl')\n",
    "    print(f\"RF trained and saved. Best params: {rf_search.best_params_}\")\n",
    "\n",
    "    # LSTM\n",
    "    print(f\"Training LSTM for horizon {h}...\")\n",
    "    X_train_lstm = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(100, activation='relu', input_shape=(1, X_train.shape[1]), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model_lstm.compile(optimizer='adam', loss='mse')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model_lstm.fit(\n",
    "        X_train_lstm, y_train_scaled,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2, # Uses the last 20% of training data for validation\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    model_lstm.save(f'/content/drive/MyDrive/AI_Sustainability_Project_lsa/lstm_model_h{h}.keras')\n",
    "    print(f\"LSTM trained and saved. Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "# Save the final featured dataframes for evaluation in the next notebook\n",
    "train_df.to_csv('/content/drive/MyDrive/AI_Sustainability_Project_lsa/train_featured_data.csv')\n",
    "test_df.to_csv('/content/drive/MyDrive/AI_Sustainability_Project_lsa/test_featured_data.csv')\n",
    "print(\"\\nTrain and Test featured data saved for evaluation/compression in later notebooks.\")\n",
    "\n",
    "print(\"\\n--- AI Modelling Complete ---\")\n",
    "print(\"Trained models and scalers saved. Proceed to Notebook 4 for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save scalers and models; add GRU training\n",
    "from sklearn.externals import joblib if False else None\n",
    "import joblib\n",
    "\n",
    "# Target scaler (fit on training pm25_value only)\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(train[['pm25_value']].values)\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(scaler_y, 'target_scaler.pkl')\n",
    "\n",
    "# Train GRU\n",
    "gru = models.Sequential([\n",
    "    layers.Input(shape=(lookback, Xtr.shape[-1])),\n",
    "    layers.GRU(64, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "gru.compile(optimizer='adam', loss='mse')\n",
    "h_gru = gru.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=10, batch_size=64, verbose=0)\n",
    "\n",
    "# Save horizon-specific models (assuming 1-step ahead with lookback window)\n",
    "lstm.save('lstm_model_h24.keras')\n",
    "gru.save('gru_model_h24.keras')\n",
    "\n",
    "# Pick best based on validation loss\n",
    "val_lstm = (h.history['val_loss'][-1] if 'val_loss' in h.history else 1e9)\n",
    "val_gru = (h_gru.history['val_loss'][-1] if 'val_loss' in h_gru.history else 1e9)\n",
    "(best_lstm if val_lstm <= val_gru else gru).save('best_lstm.keras')\n",
    "print('Saved: feature_scaler.pkl, target_scaler.pkl, lstm_model_h24.keras, gru_model_h24.keras, best_lstm.keras')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
